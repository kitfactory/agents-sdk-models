{"config":{"lang":["ja"],"separator":"[\\s\\-\u3000\u3001\u3002\uff0c\uff0e]+","pipeline":["stemmer"]},"docs":[{"location":"","title":"Agents SDK Models \u3078\u3088\u3046\u3053\u305d","text":"<p>OpenAI Agents SDK \u3092\u62e1\u5f35\u3057\u3001\u8907\u6570\u306eLLM\u30d7\u30ed\u30d0\u30a4\u30c0\u30fc\u3092\u7d71\u4e00\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30fc\u30b9\u3067\u6271\u3048\u308b\u30e2\u30c7\u30eb\u30a2\u30c0\u30d7\u30bf\u30fc\uff06\u30ef\u30fc\u30af\u30d5\u30ed\u30fc\u62e1\u5f35\u96c6\u3067\u3059\u3002</p>"},{"location":"#_1","title":"\u4e3b\u306a\u7279\u5fb4","text":"<ul> <li>OpenAI, Gemini, Claude, Ollama \u306a\u3069\u4e3b\u8981LLM\u3092\u7c21\u5358\u5207\u66ff</li> <li>\u751f\u6210\u30fb\u8a55\u4fa1\u30fb\u30c4\u30fc\u30eb\u30fb\u30ac\u30fc\u30c9\u30ec\u30fc\u30eb\u30921\u3064\u306e\u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u3067\u7d71\u5408</li> <li>\u30e2\u30c7\u30eb\u540d\u3068\u30d7\u30ed\u30f3\u30d7\u30c8\u3060\u3051\u3067\u81ea\u5df1\u6539\u5584\u30b5\u30a4\u30af\u30eb\u3082\u5b9f\u73fe</li> <li>Pydantic\u306b\u3088\u308b\u69cb\u9020\u5316\u51fa\u529b\u5bfe\u5fdc</li> <li>Python 3.9+ / Windows, Linux, MacOS\u5bfe\u5fdc</li> </ul>"},{"location":"#_2","title":"\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb","text":""},{"location":"#pypi","title":"PyPI \u304b\u3089","text":"<pre><code>pip install agents-sdk-models\n</code></pre>"},{"location":"#uv","title":"uv \u3092\u4f7f\u3046\u5834\u5408","text":"<pre><code>uv pip install agents-sdk-models\n</code></pre>"},{"location":"#_3","title":"\u958b\u767a\u7528\uff08\u63a8\u5968\uff09","text":"<pre><code>git clone https://github.com/kitfactory/agents-sdk-models.git\ncd agents-sdk-models\npython -m venv .venv\n.venv\\Scripts\\activate  # Windows\nsource .venv/bin/activate  # Linux/Mac\nuv pip install -e .[dev]\n</code></pre>"},{"location":"#_4","title":"\u30b5\u30dd\u30fc\u30c8\u74b0\u5883","text":"<ul> <li>Python 3.9+</li> <li>OpenAI Agents SDK 0.0.9+</li> <li>Windows, Linux, MacOS </li> </ul>"},{"location":"#_5","title":"\u30c8\u30ec\u30fc\u30b7\u30f3\u30b0","text":"<p>\u672c\u30e9\u30a4\u30d6\u30e9\u30ea\u3067\u306f OpenAI Agents SDK \u306e\u30c8\u30ec\u30fc\u30b7\u30f3\u30b0\u6a5f\u80fd\u3092\u30b5\u30dd\u30fc\u30c8\u3057\u3066\u3044\u307e\u3059\u3002\u8a73\u7d30\u306f \u30c8\u30ec\u30fc\u30b7\u30f3\u30b0 \u3092\u53c2\u7167\u3057\u3066\u304f\u3060\u3055\u3044\u3002 </p>"},{"location":"README_ja/","title":"agents-sdk-models \u30c9\u30ad\u30e5\u30e1\u30f3\u30c8","text":""},{"location":"README_ja/#_1","title":"\ud83c\udf1f \u306f\u3058\u3081\u306b","text":"<p>\u3053\u306e\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306f\u3001OpenAI Agents SDK\u3092\u6d3b\u7528\u3057\u305f\u30a8\u30fc\u30b8\u30a7\u30f3\u30c8\u30fb\u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u306e\u69cb\u7bc9\u3092\u652f\u63f4\u3059\u308bPython\u30e9\u30a4\u30d6\u30e9\u30ea\u3067\u3059\u3002 \u751f\u6210\u30fb\u8a55\u4fa1\u30fb\u30c4\u30fc\u30eb\u9023\u643a\u30fb\u30ac\u30fc\u30c9\u30ec\u30fc\u30eb\u306a\u3069\u3001\u5b9f\u8df5\u7684\u306aAI\u30ef\u30fc\u30af\u30d5\u30ed\u30fc\u3092\u6700\u5c0f\u9650\u306e\u8a18\u8ff0\u3067\u5b9f\u73fe\u3067\u304d\u307e\u3059\u3002</p>"},{"location":"README_ja/#_2","title":"\ud83d\ude80 \u7279\u5fb4\u30fb\u30e1\u30ea\u30c3\u30c8","text":"<ul> <li>\ud83e\udde9 \u751f\u6210\u30fb\u8a55\u4fa1\u30fb\u30c4\u30fc\u30eb\u30fb\u30ac\u30fc\u30c9\u30ec\u30fc\u30eb\u3092\u67d4\u8edf\u306b\u7d44\u307f\u5408\u308f\u305b\u305f\u30ef\u30fc\u30af\u30d5\u30ed\u30fc\u3092\u7c21\u5358\u306b\u69cb\u7bc9</li> <li>\ud83d\udee0\ufe0f Python\u95a2\u6570\u3092\u305d\u306e\u307e\u307e\u30c4\u30fc\u30eb\u3068\u3057\u3066\u5229\u7528\u53ef\u80fd</li> <li>\ud83d\udee1\ufe0f \u30ac\u30fc\u30c9\u30ec\u30fc\u30eb\u3067\u5b89\u5168\u30fb\u5805\u7262\u306a\u5165\u529b/\u51fa\u529b\u5236\u5fa1</li> <li>\ud83d\udce6 \u8c4a\u5bcc\u306a\u30b5\u30f3\u30d7\u30eb\uff08<code>examples/</code>\uff09\u3067\u3059\u3050\u306b\u8a66\u305b\u308b</li> <li>\ud83d\ude80 \u6700\u5c0f\u9650\u306e\u8a18\u8ff0\u3067\u7d20\u65e9\u304f\u30d7\u30ed\u30c8\u30bf\u30a4\u30d4\u30f3\u30b0</li> </ul>"},{"location":"README_ja/#_3","title":"\u26a1 \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb","text":"<p><pre><code>pip install agents-sdk-models\n</code></pre> - OpenAI Agents SDK, pydantic 2.x \u306a\u3069\u304c\u5fc5\u8981\u3067\u3059\u3002\u8a73\u7d30\u306f\u516c\u5f0f\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u3082\u53c2\u7167\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p>"},{"location":"README_ja/#agentpipeline","title":"\ud83c\udfd7\ufe0f AgentPipeline\u30af\u30e9\u30b9\u306e\u4f7f\u3044\u65b9","text":"<p><code>AgentPipeline</code> \u30af\u30e9\u30b9\u306f\u3001\u751f\u6210\u6307\u793a\u30fb\u8a55\u4fa1\u6307\u793a\u30fb\u30c4\u30fc\u30eb\u30fb\u30ac\u30fc\u30c9\u30ec\u30fc\u30eb\u306a\u3069\u3092\u67d4\u8edf\u306b\u7d44\u307f\u5408\u308f\u305b\u3066\u3001LLM\u30a8\u30fc\u30b8\u30a7\u30f3\u30c8\u306e\u30ef\u30fc\u30af\u30d5\u30ed\u30fc\u3092\u7c21\u5358\u306b\u69cb\u7bc9\u3067\u304d\u307e\u3059\u3002</p>"},{"location":"README_ja/#_4","title":"\u57fa\u672c\u69cb\u6210","text":"<pre><code>from agents_sdk_models import AgentPipeline\n\npipeline = AgentPipeline(\n    name=\"my_pipeline\",\n    generation_instructions=\"...\",  # \u751f\u6210\u6307\u793a\n    evaluation_instructions=None,    # \u8a55\u4fa1\u4e0d\u8981\u306a\u3089None\n    model=\"gpt-3.5-turbo\"\n)\nresult = pipeline.run(\"\u30e6\u30fc\u30b6\u30fc\u5165\u529b\")\n</code></pre>"},{"location":"README_ja/#_5","title":"\u751f\u6210\u7269\u306e\u81ea\u52d5\u8a55\u4fa1","text":"<pre><code>pipeline = AgentPipeline(\n    name=\"evaluated_generator\",\n    generation_instructions=\"...\",\n    evaluation_instructions=\"...\",  # \u8a55\u4fa1\u6307\u793a\n    model=\"gpt-3.5-turbo\",\n    threshold=70\n)\nresult = pipeline.run(\"\u8a55\u4fa1\u5bfe\u8c61\u306e\u5165\u529b\")\n</code></pre>"},{"location":"README_ja/#_6","title":"\u30c4\u30fc\u30eb\u9023\u643a","text":"<pre><code>from agents import function_tool\n\n@function_tool\ndef search_web(query: str) -&gt; str:\n    ...\n\npipeline = AgentPipeline(\n    name=\"tooled_generator\",\n    generation_instructions=\"...\",\n    evaluation_instructions=None,\n    model=\"gpt-3.5-turbo\",\n    generation_tools=[search_web]\n)\n</code></pre>"},{"location":"README_ja/#_7","title":"\u30ac\u30fc\u30c9\u30ec\u30fc\u30eb\uff08\u5165\u529b\u5236\u5fa1\uff09","text":"<pre><code>from agents import input_guardrail, GuardrailFunctionOutput, InputGuardrailTripwireTriggered\n\n@input_guardrail\nasync def math_guardrail(ctx, agent, input):\n    ...\n\npipeline = AgentPipeline(\n    name=\"guardrail_pipeline\",\n    generation_instructions=\"...\",\n    evaluation_instructions=None,\n    model=\"gpt-4o\",\n    input_guardrails=[math_guardrail]\n)\n\ntry:\n    result = pipeline.run(\"Can you help me solve for x: 2x + 3 = 11?\")\nexcept InputGuardrailTripwireTriggered:\n    print(\"[Guardrail Triggered] Math homework detected. Request blocked.\")\n</code></pre>"},{"location":"README_ja/#_8","title":"\u30ea\u30c8\u30e9\u30a4\u6642\u306e\u30b3\u30e1\u30f3\u30c8\u30d5\u30a3\u30fc\u30c9\u30d0\u30c3\u30af","text":"<p><pre><code>from agents_sdk_models import AgentPipeline\n\npipeline = AgentPipeline(\n    name=\"comment_retry\",\n    generation_instructions=\"\u751f\u6210\u30d7\u30ed\u30f3\u30d7\u30c8\",\n    evaluation_instructions=\"\u8a55\u4fa1\u30d7\u30ed\u30f3\u30d7\u30c8\",\n    model=\"gpt-4o-mini\",\n    threshold=80,\n    retries=2,\n    retry_comment_importance=[\"serious\", \"normal\"]\n)\nresult = pipeline.run(\"\u5165\u529b\u30c6\u30ad\u30b9\u30c8\")\nprint(result)\n</code></pre> \u30ea\u30c8\u30e9\u30a4\u6642\u306b\u524d\u56de\u306e\u8a55\u4fa1\u30b3\u30e1\u30f3\u30c8\uff08\u6307\u5b9a\u3057\u305f\u91cd\u5927\u5ea6\u306e\u307f\uff09\u304c\u751f\u6210\u30d7\u30ed\u30f3\u30d7\u30c8\u306b\u81ea\u52d5\u3067\u4ed8\u4e0e\u3055\u308c\u3001\u6539\u5584\u3092\u4fc3\u3057\u307e\u3059\u3002</p>"},{"location":"api_reference/","title":"API\u30ea\u30d5\u30a1\u30ec\u30f3\u30b9","text":"<p>\u3053\u306e\u30da\u30fc\u30b8\u3067\u306f\u3001mkdocstrings\u30d7\u30e9\u30b0\u30a4\u30f3\u3092\u4f7f\u7528\u3057\u3066<code>agents_sdk_models</code>\u30d1\u30c3\u30b1\u30fc\u30b8\u306eAPI\u30ea\u30d5\u30a1\u30ec\u30f3\u30b9\u3092\u81ea\u52d5\u751f\u6210\u3057\u307e\u3059\u3002</p> <p>Agents SDK Models \u30a8\u30fc\u30b8\u30a7\u30f3\u30c8SDK\u30e2\u30c7\u30eb</p>"},{"location":"api_reference/#agents_sdk_models.AgentPipeline","title":"<code>AgentPipeline</code>","text":"<p>AgentPipeline class for managing the generation and evaluation of content using OpenAI Agents SDK OpenAI Agents SDK\u3092\u4f7f\u7528\u3057\u3066\u30b3\u30f3\u30c6\u30f3\u30c4\u306e\u751f\u6210\u3068\u8a55\u4fa1\u3092\u7ba1\u7406\u3059\u308b\u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u30af\u30e9\u30b9</p> <p>This class handles: \u3053\u306e\u30af\u30e9\u30b9\u306f\u4ee5\u4e0b\u3092\u51e6\u7406\u3057\u307e\u3059\uff1a - Content generation using instructions / instructions\u3092\u4f7f\u7528\u3057\u305f\u30b3\u30f3\u30c6\u30f3\u30c4\u751f\u6210 - Content evaluation with scoring / \u30b9\u30b3\u30a2\u30ea\u30f3\u30b0\u306b\u3088\u308b\u30b3\u30f3\u30c6\u30f3\u30c4\u8a55\u4fa1 - Session history management / \u30bb\u30c3\u30b7\u30e7\u30f3\u5c65\u6b74\u306e\u7ba1\u7406 - Output formatting and routing / \u51fa\u529b\u306e\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u3068\u30eb\u30fc\u30c6\u30a3\u30f3\u30b0</p> Source code in <code>src\\agents_sdk_models\\pipeline.py</code> <pre><code>class AgentPipeline:\n    \"\"\"\n    AgentPipeline class for managing the generation and evaluation of content using OpenAI Agents SDK\n    OpenAI Agents SDK\u3092\u4f7f\u7528\u3057\u3066\u30b3\u30f3\u30c6\u30f3\u30c4\u306e\u751f\u6210\u3068\u8a55\u4fa1\u3092\u7ba1\u7406\u3059\u308b\u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u30af\u30e9\u30b9\n\n    This class handles:\n    \u3053\u306e\u30af\u30e9\u30b9\u306f\u4ee5\u4e0b\u3092\u51e6\u7406\u3057\u307e\u3059\uff1a\n    - Content generation using instructions / instructions\u3092\u4f7f\u7528\u3057\u305f\u30b3\u30f3\u30c6\u30f3\u30c4\u751f\u6210\n    - Content evaluation with scoring / \u30b9\u30b3\u30a2\u30ea\u30f3\u30b0\u306b\u3088\u308b\u30b3\u30f3\u30c6\u30f3\u30c4\u8a55\u4fa1\n    - Session history management / \u30bb\u30c3\u30b7\u30e7\u30f3\u5c65\u6b74\u306e\u7ba1\u7406\n    - Output formatting and routing / \u51fa\u529b\u306e\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u3068\u30eb\u30fc\u30c6\u30a3\u30f3\u30b0\n    \"\"\"\n\n    def __init__(\n        self,\n        name: str,\n        generation_instructions: str,\n        evaluation_instructions: Optional[str],\n        *,\n        input_guardrails: Optional[list] = None,\n        output_guardrails: Optional[list] = None,\n        output_model: Optional[Type[Any]] = None,\n        model: str | None = None,\n        evaluation_model: str | None = None,\n        generation_tools: Optional[list] = None,\n        evaluation_tools: Optional[list] = None,\n        routing_func: Optional[Callable[[Any], Any]] = None,\n        session_history: Optional[list] = None,\n        history_size: int = 10,\n        threshold: int = 85,\n        retries: int = 3,\n        improvement_callback: Optional[Callable[[Any, EvaluationResult], None]] = None,\n        dynamic_prompt: Optional[Callable[[str], str]] = None,\n        retry_comment_importance: Optional[list[str]] = None,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the Pipeline with configuration parameters\n        \u8a2d\u5b9a\u30d1\u30e9\u30e1\u30fc\u30bf\u3067\u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u3092\u521d\u671f\u5316\u3059\u308b\n\n        Args:\n            name: Pipeline name / \u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u540d\n            generation_instructions: System prompt for generation / \u751f\u6210\u7528\u30b7\u30b9\u30c6\u30e0\u30d7\u30ed\u30f3\u30d7\u30c8\n            evaluation_instructions: System prompt for evaluation / \u8a55\u4fa1\u7528\u30b7\u30b9\u30c6\u30e0\u30d7\u30ed\u30f3\u30d7\u30c8\n            input_guardrails: Guardrails for generation / \u751f\u6210\u7528\u30ac\u30fc\u30c9\u30ec\u30fc\u30eb\n            output_guardrails: Guardrails for evaluation / \u8a55\u4fa1\u7528\u30ac\u30fc\u30c9\u30ec\u30fc\u30eb\n            output_model: Model for output formatting / \u51fa\u529b\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u7528\u30e2\u30c7\u30eb\n            model: LLM model name / LLM\u30e2\u30c7\u30eb\u540d\n            evaluation_model: Optional LLM model name for evaluation; if None, uses model. \u65e5\u672c\u8a9e: \u8a55\u4fa1\u7528\u306eLLM\u30e2\u30c7\u30eb\u540d\uff08None\u306e\u5834\u5408\u306fmodel\u3092\u4f7f\u7528\uff09\n            generation_tools: Tools for generation / \u751f\u6210\u7528\u30c4\u30fc\u30eb\n            evaluation_tools: Tools for evaluation / \u8a55\u4fa1\u7528\u30c4\u30fc\u30eb\n            routing_func: Function for output routing / \u51fa\u529b\u30eb\u30fc\u30c6\u30a3\u30f3\u30b0\u7528\u95a2\u6570\n            session_history: Session history / \u30bb\u30c3\u30b7\u30e7\u30f3\u5c65\u6b74\n            history_size: Size of history to keep / \u4fdd\u6301\u3059\u308b\u5c65\u6b74\u30b5\u30a4\u30ba\n            threshold: Evaluation score threshold / \u8a55\u4fa1\u30b9\u30b3\u30a2\u306e\u95be\u5024\n            retries: Number of retry attempts / \u30ea\u30c8\u30e9\u30a4\u8a66\u884c\u56de\u6570\n            improvement_callback: Callback for improvement suggestions / \u6539\u5584\u63d0\u6848\u7528\u30b3\u30fc\u30eb\u30d0\u30c3\u30af\n            dynamic_prompt: Optional function to dynamically build prompt / \u52d5\u7684\u30d7\u30ed\u30f3\u30d7\u30c8\u751f\u6210\u95a2\u6570\uff08\u4efb\u610f\uff09\n            retry_comment_importance: Importance levels of comments to include on retry / \u30ea\u30c8\u30e9\u30a4\u6642\u306b\u30d7\u30ed\u30f3\u30d7\u30c8\u306b\u542b\u3081\u308b\u30b3\u30e1\u30f3\u30c8\u306e\u91cd\u5927\u5ea6\u30ec\u30d9\u30eb\uff08\u4efb\u610f\uff09\n        \"\"\"\n        self.name = name\n        self.generation_instructions = generation_instructions.strip()\n        self.evaluation_instructions = evaluation_instructions.strip() if evaluation_instructions else None\n        self.output_model = output_model\n\n        self.model = model\n        self.evaluation_model = evaluation_model\n        self.generation_tools = generation_tools or []\n        self.evaluation_tools = evaluation_tools or []\n        self.input_guardrails = input_guardrails or []\n        self.output_guardrails = output_guardrails or []\n        self.routing_func = routing_func\n        self.session_history = session_history if session_history is not None else []\n        self.history_size = history_size\n        self.threshold = threshold\n        self.retries = retries\n        self.improvement_callback = improvement_callback\n        self.dynamic_prompt = dynamic_prompt\n        self.retry_comment_importance = retry_comment_importance or []\n\n        # English: Get generation LLM instance; default tracing setting applied in get_llm\n        # \u65e5\u672c\u8a9e: \u751f\u6210\u7528LLM\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u3092\u53d6\u5f97\u3057\u307e\u3059\u3002tracing\u8a2d\u5b9a\u306fget_llm\u5074\u3067\u30c7\u30d5\u30a9\u30eb\u30c8\u5024\u3092\u4f7f\u7528\n        llm = get_llm(model) if model else None\n        # English: Determine evaluation LLM instance, fallback to generation model if evaluation_model is None\n        # \u65e5\u672c\u8a9e: \u8a55\u4fa1\u7528LLM\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u3092\u6c7a\u5b9a\u3002evaluation_model\u304cNone\u306e\u5834\u5408\u306f\u751f\u6210\u30e2\u30c7\u30eb\u3092\u4f7f\u7528\n        eval_source = evaluation_model if evaluation_model else model\n        llm_eval = get_llm(eval_source) if eval_source else None\n\n        # Agents ---------------------------------------------------------\n        self.gen_agent = Agent(\n            name=f\"{name}_generator\",\n            model=llm,\n            tools=self.generation_tools,\n            instructions=self.generation_instructions,\n            input_guardrails=self.input_guardrails,\n        )\n\n        json_instr =\"\"\"\n        ----\n        \u51fa\u529b\u30d5\u30a9\u30fc\u30de\u30c3\u30c8:\n        JSON \u3067\u5fc5\u305a\u6b21\u306e\u5f62\u5f0f\u306b\u3057\u3066\u304f\u3060\u3055\u3044:\n        {\n            \"score\": int(0\uff5e100),\n            \"comment\": [str]\n        }\"\n        \"\"\"\n        self.eval_agent = (\n            Agent(\n                name=f\"{name}_evaluator\",\n                model=llm_eval,\n                tools=self.evaluation_tools,\n                instructions=self.evaluation_instructions + json_instr,\n                output_guardrails=self.output_guardrails,\n            )\n            if self.evaluation_instructions\n            else None\n        )\n\n        self._runner = Runner()\n        self._pipeline_history: List[Dict[str, str]] = []\n\n    # ------------------------------------------------------------------\n    # helpers\n    # ------------------------------------------------------------------\n\n    def _build_generation_prompt(self, user_input: str) -&gt; str:\n        \"\"\"\n        Build the prompt for content generation\n        \u30b3\u30f3\u30c6\u30f3\u30c4\u751f\u6210\u7528\u306e\u30d7\u30ed\u30f3\u30d7\u30c8\u3092\u69cb\u7bc9\u3059\u308b\n\n        Args:\n            user_input: User input text / \u30e6\u30fc\u30b6\u30fc\u5165\u529b\u30c6\u30ad\u30b9\u30c8\n\n        Returns:\n            str: Formatted prompt for generation / \u751f\u6210\u7528\u306e\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u6e08\u307f\u30d7\u30ed\u30f3\u30d7\u30c8\n        \"\"\"\n        recent = \"\\n\".join(f\"User: {h['input']}\\nAI: {h['output']}\"\n                          for h in self._pipeline_history[-self.history_size:])\n        session = \"\\n\".join(self.session_history)\n        return \"\\n\".join(filter(None, [session, recent, f\"UserInput: {user_input}\"]))\n\n    def _build_evaluation_prompt(self, user_input: str, generated_output: str) -&gt; str:\n        \"\"\"\n        Build the prompt for content evaluation\n        \u30b3\u30f3\u30c6\u30f3\u30c4\u8a55\u4fa1\u7528\u306e\u30d7\u30ed\u30f3\u30d7\u30c8\u3092\u69cb\u7bc9\u3059\u308b\n\n        Args:\n            user_input: Original user input / \u5143\u306e\u30e6\u30fc\u30b6\u30fc\u5165\u529b\n            generated_output: Generated content to evaluate / \u8a55\u4fa1\u5bfe\u8c61\u306e\u751f\u6210\u30b3\u30f3\u30c6\u30f3\u30c4\n\n        Returns:\n            str: Formatted prompt for evaluation / \u8a55\u4fa1\u7528\u306e\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u6e08\u307f\u30d7\u30ed\u30f3\u30d7\u30c8\n        \"\"\"\n        parts = [\n            \"----\",\n            f\"\u30e6\u30fc\u30b6\u30fc\u5165\u529b:\\n{user_input}\",\n            \"----\",\n            f\"\u751f\u6210\u7d50\u679c:\\n{generated_output}\",\n        ]\n        return \"\\n\".join(filter(None, parts)).strip()\n\n    @staticmethod\n    def _extract_json(text: str) -&gt; Dict[str, Any]:\n        \"\"\"\n        Extract JSON from text\n        \u30c6\u30ad\u30b9\u30c8\u304b\u3089JSON\u3092\u62bd\u51fa\u3059\u308b\n\n        Args:\n            text: Text containing JSON / JSON\u3092\u542b\u3080\u30c6\u30ad\u30b9\u30c8\n\n        Returns:\n            Dict[str, Any]: Extracted JSON data / \u62bd\u51fa\u3055\u308c\u305fJSON\u30c7\u30fc\u30bf\n\n        Raises:\n            ValueError: If JSON is not found in text / \u30c6\u30ad\u30b9\u30c8\u5185\u306bJSON\u304c\u898b\u3064\u304b\u3089\u306a\u3044\u5834\u5408\n        \"\"\"\n        match = re.search(r\"\\{.*\\}\", text, re.S)\n        if not match:\n            raise ValueError(\"JSON not found in evaluation output\")\n        return json.loads(match.group(0))\n\n    def _coerce_output(self, text: str):\n        \"\"\"\n        Convert output to specified model format\n        \u51fa\u529b\u3092\u6307\u5b9a\u3055\u308c\u305f\u30e2\u30c7\u30eb\u5f62\u5f0f\u306b\u5909\u63db\u3059\u308b\n\n        Args:\n            text: Output text to convert / \u5909\u63db\u5bfe\u8c61\u306e\u51fa\u529b\u30c6\u30ad\u30b9\u30c8\n\n        Returns:\n            Any: Converted output in specified format / \u6307\u5b9a\u3055\u308c\u305f\u5f62\u5f0f\u306e\u5909\u63db\u6e08\u307f\u51fa\u529b\n        \"\"\"\n        if self.output_model is None:\n            return text\n        try:\n            data = json.loads(text)\n        except json.JSONDecodeError:\n            return text\n        try:\n            if isinstance(self.output_model, type) and issubclass(self.output_model, BaseModel):\n                return self.output_model.model_validate(data)\n            if is_dataclass(self.output_model):\n                return self.output_model(**data)\n            return self.output_model(**data)\n        except Exception:\n            return text\n\n    def _append_to_session(self, user_input: str, raw_output: str):\n        \"\"\"\n        Append interaction to session history\n        \u30bb\u30c3\u30b7\u30e7\u30f3\u5c65\u6b74\u306b\u30a4\u30f3\u30bf\u30e9\u30af\u30b7\u30e7\u30f3\u3092\u8ffd\u52a0\u3059\u308b\n\n        Args:\n            user_input: User input text / \u30e6\u30fc\u30b6\u30fc\u5165\u529b\u30c6\u30ad\u30b9\u30c8\n            raw_output: Generated output text / \u751f\u6210\u3055\u308c\u305f\u51fa\u529b\u30c6\u30ad\u30b9\u30c8\n        \"\"\"\n        if self.session_history is None:\n            return\n        self.session_history.append(f\"User: {user_input}\\nAI: {raw_output}\")\n\n    def _route(self, parsed_output):\n        \"\"\"\n        Route the parsed output through routing function if specified\n        \u6307\u5b9a\u3055\u308c\u3066\u3044\u308b\u5834\u5408\u3001\u30d1\u30fc\u30b9\u6e08\u307f\u51fa\u529b\u3092\u30eb\u30fc\u30c6\u30a3\u30f3\u30b0\u95a2\u6570\u3067\u51e6\u7406\u3059\u308b\n\n        Args:\n            parsed_output: Parsed output to route / \u30eb\u30fc\u30c6\u30a3\u30f3\u30b0\u5bfe\u8c61\u306e\u30d1\u30fc\u30b9\u6e08\u307f\u51fa\u529b\n\n        Returns:\n            Any: Routed output / \u30eb\u30fc\u30c6\u30a3\u30f3\u30b0\u6e08\u307f\u51fa\u529b\n        \"\"\"\n        return self.routing_func(parsed_output) if self.routing_func else parsed_output\n\n    # ------------------------------------------------------------------\n    # public\n    # ------------------------------------------------------------------\n\n    def run(self, user_input: str):\n        \"\"\"\n        Run the pipeline with user input\n        \u30e6\u30fc\u30b6\u30fc\u5165\u529b\u3067\u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u3092\u5b9f\u884c\u3059\u308b\n\n        Args:\n            user_input: User input text / \u30e6\u30fc\u30b6\u30fc\u5165\u529b\u30c6\u30ad\u30b9\u30c8\n\n        Returns:\n            Any: Processed output or None if evaluation fails / \u51e6\u7406\u6e08\u307f\u51fa\u529b\u3001\u307e\u305f\u306f\u8a55\u4fa1\u5931\u6557\u6642\u306fNone\n        \"\"\"\n        attempt = 0\n        last_eval_result: Optional[EvaluationResult] = None  # Store last evaluation result for retry\n        while attempt &lt;= self.retries:\n            # ---------------- Generation ----------------\n            # On retry, include prior evaluation comments if configured\n            if attempt &gt; 0 and last_eval_result and self.retry_comment_importance:\n                # Filter comments by importance\n                try:\n                    comments = [c for c in last_eval_result.comment if c.get(\"importance\") in self.retry_comment_importance]\n                except Exception:\n                    comments = []\n                # Format comments\n                comment_lines = \"\\n\".join(f\"- ({c.get('importance')}) {c.get('content')}\" for c in comments)\n            else:\n                comment_lines = \"\"\n            # Build base prompt\n            if attempt &gt; 0 and comment_lines:\n                base = self.dynamic_prompt(user_input) if self.dynamic_prompt else self._build_generation_prompt(user_input)\n                gen_prompt = \"\\n\".join([comment_lines, base])\n            else:\n                if self.dynamic_prompt:\n                    gen_prompt = self.dynamic_prompt(user_input)\n                else:\n                    gen_prompt = self._build_generation_prompt(user_input)\n\n            gen_result = self._runner.run_sync(self.gen_agent, gen_prompt)\n            raw_output_text = getattr(gen_result, \"final_output\", str(gen_result))\n            if hasattr(gen_result, \"tool_calls\") and gen_result.tool_calls:\n                raw_output_text = str(gen_result.tool_calls[0].call())\n\n            parsed_output = self._coerce_output(raw_output_text)\n            self._pipeline_history.append({\"input\": user_input, \"output\": raw_output_text})\n\n            # ---------------- Evaluation ----------------\n            if not self.eval_agent:\n                return self._route(parsed_output)\n\n            eval_prompt = self._build_evaluation_prompt(user_input, raw_output_text)\n\n            eval_raw = self._runner.run_sync(self.eval_agent, eval_prompt)\n            eval_text = getattr(eval_raw, \"final_output\", str(eval_raw))\n            try:\n                eval_dict = self._extract_json(eval_text)\n                eval_result = EvaluationResult(**eval_dict)\n            except Exception:\n                eval_result = EvaluationResult(score=0, comment=[\"\u8a55\u4fa1 JSON \u306e\u89e3\u6790\u306b\u5931\u6557\"])\n\n            if eval_result.score &gt;= self.threshold:\n                self._append_to_session(user_input, raw_output_text)\n                return self._route(parsed_output)\n\n            # Store for next retry\n            last_eval_result = eval_result\n            attempt += 1\n\n        if self.improvement_callback:\n            self.improvement_callback(parsed_output, eval_result)\n        return None\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.AgentPipeline.__init__","title":"<code>__init__(name, generation_instructions, evaluation_instructions, *, input_guardrails=None, output_guardrails=None, output_model=None, model=None, evaluation_model=None, generation_tools=None, evaluation_tools=None, routing_func=None, session_history=None, history_size=10, threshold=85, retries=3, improvement_callback=None, dynamic_prompt=None, retry_comment_importance=None)</code>","text":"<p>Initialize the Pipeline with configuration parameters \u8a2d\u5b9a\u30d1\u30e9\u30e1\u30fc\u30bf\u3067\u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u3092\u521d\u671f\u5316\u3059\u308b</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Pipeline name / \u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u540d</p> required <code>generation_instructions</code> <code>str</code> <p>System prompt for generation / \u751f\u6210\u7528\u30b7\u30b9\u30c6\u30e0\u30d7\u30ed\u30f3\u30d7\u30c8</p> required <code>evaluation_instructions</code> <code>Optional[str]</code> <p>System prompt for evaluation / \u8a55\u4fa1\u7528\u30b7\u30b9\u30c6\u30e0\u30d7\u30ed\u30f3\u30d7\u30c8</p> required <code>input_guardrails</code> <code>Optional[list]</code> <p>Guardrails for generation / \u751f\u6210\u7528\u30ac\u30fc\u30c9\u30ec\u30fc\u30eb</p> <code>None</code> <code>output_guardrails</code> <code>Optional[list]</code> <p>Guardrails for evaluation / \u8a55\u4fa1\u7528\u30ac\u30fc\u30c9\u30ec\u30fc\u30eb</p> <code>None</code> <code>output_model</code> <code>Optional[Type[Any]]</code> <p>Model for output formatting / \u51fa\u529b\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u7528\u30e2\u30c7\u30eb</p> <code>None</code> <code>model</code> <code>str | None</code> <p>LLM model name / LLM\u30e2\u30c7\u30eb\u540d</p> <code>None</code> <code>evaluation_model</code> <code>str | None</code> <p>Optional LLM model name for evaluation; if None, uses model. \u65e5\u672c\u8a9e: \u8a55\u4fa1\u7528\u306eLLM\u30e2\u30c7\u30eb\u540d\uff08None\u306e\u5834\u5408\u306fmodel\u3092\u4f7f\u7528\uff09</p> <code>None</code> <code>generation_tools</code> <code>Optional[list]</code> <p>Tools for generation / \u751f\u6210\u7528\u30c4\u30fc\u30eb</p> <code>None</code> <code>evaluation_tools</code> <code>Optional[list]</code> <p>Tools for evaluation / \u8a55\u4fa1\u7528\u30c4\u30fc\u30eb</p> <code>None</code> <code>routing_func</code> <code>Optional[Callable[[Any], Any]]</code> <p>Function for output routing / \u51fa\u529b\u30eb\u30fc\u30c6\u30a3\u30f3\u30b0\u7528\u95a2\u6570</p> <code>None</code> <code>session_history</code> <code>Optional[list]</code> <p>Session history / \u30bb\u30c3\u30b7\u30e7\u30f3\u5c65\u6b74</p> <code>None</code> <code>history_size</code> <code>int</code> <p>Size of history to keep / \u4fdd\u6301\u3059\u308b\u5c65\u6b74\u30b5\u30a4\u30ba</p> <code>10</code> <code>threshold</code> <code>int</code> <p>Evaluation score threshold / \u8a55\u4fa1\u30b9\u30b3\u30a2\u306e\u95be\u5024</p> <code>85</code> <code>retries</code> <code>int</code> <p>Number of retry attempts / \u30ea\u30c8\u30e9\u30a4\u8a66\u884c\u56de\u6570</p> <code>3</code> <code>improvement_callback</code> <code>Optional[Callable[[Any, EvaluationResult], None]]</code> <p>Callback for improvement suggestions / \u6539\u5584\u63d0\u6848\u7528\u30b3\u30fc\u30eb\u30d0\u30c3\u30af</p> <code>None</code> <code>dynamic_prompt</code> <code>Optional[Callable[[str], str]]</code> <p>Optional function to dynamically build prompt / \u52d5\u7684\u30d7\u30ed\u30f3\u30d7\u30c8\u751f\u6210\u95a2\u6570\uff08\u4efb\u610f\uff09</p> <code>None</code> <code>retry_comment_importance</code> <code>Optional[list[str]]</code> <p>Importance levels of comments to include on retry / \u30ea\u30c8\u30e9\u30a4\u6642\u306b\u30d7\u30ed\u30f3\u30d7\u30c8\u306b\u542b\u3081\u308b\u30b3\u30e1\u30f3\u30c8\u306e\u91cd\u5927\u5ea6\u30ec\u30d9\u30eb\uff08\u4efb\u610f\uff09</p> <code>None</code> Source code in <code>src\\agents_sdk_models\\pipeline.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    generation_instructions: str,\n    evaluation_instructions: Optional[str],\n    *,\n    input_guardrails: Optional[list] = None,\n    output_guardrails: Optional[list] = None,\n    output_model: Optional[Type[Any]] = None,\n    model: str | None = None,\n    evaluation_model: str | None = None,\n    generation_tools: Optional[list] = None,\n    evaluation_tools: Optional[list] = None,\n    routing_func: Optional[Callable[[Any], Any]] = None,\n    session_history: Optional[list] = None,\n    history_size: int = 10,\n    threshold: int = 85,\n    retries: int = 3,\n    improvement_callback: Optional[Callable[[Any, EvaluationResult], None]] = None,\n    dynamic_prompt: Optional[Callable[[str], str]] = None,\n    retry_comment_importance: Optional[list[str]] = None,\n) -&gt; None:\n    \"\"\"\n    Initialize the Pipeline with configuration parameters\n    \u8a2d\u5b9a\u30d1\u30e9\u30e1\u30fc\u30bf\u3067\u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u3092\u521d\u671f\u5316\u3059\u308b\n\n    Args:\n        name: Pipeline name / \u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u540d\n        generation_instructions: System prompt for generation / \u751f\u6210\u7528\u30b7\u30b9\u30c6\u30e0\u30d7\u30ed\u30f3\u30d7\u30c8\n        evaluation_instructions: System prompt for evaluation / \u8a55\u4fa1\u7528\u30b7\u30b9\u30c6\u30e0\u30d7\u30ed\u30f3\u30d7\u30c8\n        input_guardrails: Guardrails for generation / \u751f\u6210\u7528\u30ac\u30fc\u30c9\u30ec\u30fc\u30eb\n        output_guardrails: Guardrails for evaluation / \u8a55\u4fa1\u7528\u30ac\u30fc\u30c9\u30ec\u30fc\u30eb\n        output_model: Model for output formatting / \u51fa\u529b\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u7528\u30e2\u30c7\u30eb\n        model: LLM model name / LLM\u30e2\u30c7\u30eb\u540d\n        evaluation_model: Optional LLM model name for evaluation; if None, uses model. \u65e5\u672c\u8a9e: \u8a55\u4fa1\u7528\u306eLLM\u30e2\u30c7\u30eb\u540d\uff08None\u306e\u5834\u5408\u306fmodel\u3092\u4f7f\u7528\uff09\n        generation_tools: Tools for generation / \u751f\u6210\u7528\u30c4\u30fc\u30eb\n        evaluation_tools: Tools for evaluation / \u8a55\u4fa1\u7528\u30c4\u30fc\u30eb\n        routing_func: Function for output routing / \u51fa\u529b\u30eb\u30fc\u30c6\u30a3\u30f3\u30b0\u7528\u95a2\u6570\n        session_history: Session history / \u30bb\u30c3\u30b7\u30e7\u30f3\u5c65\u6b74\n        history_size: Size of history to keep / \u4fdd\u6301\u3059\u308b\u5c65\u6b74\u30b5\u30a4\u30ba\n        threshold: Evaluation score threshold / \u8a55\u4fa1\u30b9\u30b3\u30a2\u306e\u95be\u5024\n        retries: Number of retry attempts / \u30ea\u30c8\u30e9\u30a4\u8a66\u884c\u56de\u6570\n        improvement_callback: Callback for improvement suggestions / \u6539\u5584\u63d0\u6848\u7528\u30b3\u30fc\u30eb\u30d0\u30c3\u30af\n        dynamic_prompt: Optional function to dynamically build prompt / \u52d5\u7684\u30d7\u30ed\u30f3\u30d7\u30c8\u751f\u6210\u95a2\u6570\uff08\u4efb\u610f\uff09\n        retry_comment_importance: Importance levels of comments to include on retry / \u30ea\u30c8\u30e9\u30a4\u6642\u306b\u30d7\u30ed\u30f3\u30d7\u30c8\u306b\u542b\u3081\u308b\u30b3\u30e1\u30f3\u30c8\u306e\u91cd\u5927\u5ea6\u30ec\u30d9\u30eb\uff08\u4efb\u610f\uff09\n    \"\"\"\n    self.name = name\n    self.generation_instructions = generation_instructions.strip()\n    self.evaluation_instructions = evaluation_instructions.strip() if evaluation_instructions else None\n    self.output_model = output_model\n\n    self.model = model\n    self.evaluation_model = evaluation_model\n    self.generation_tools = generation_tools or []\n    self.evaluation_tools = evaluation_tools or []\n    self.input_guardrails = input_guardrails or []\n    self.output_guardrails = output_guardrails or []\n    self.routing_func = routing_func\n    self.session_history = session_history if session_history is not None else []\n    self.history_size = history_size\n    self.threshold = threshold\n    self.retries = retries\n    self.improvement_callback = improvement_callback\n    self.dynamic_prompt = dynamic_prompt\n    self.retry_comment_importance = retry_comment_importance or []\n\n    # English: Get generation LLM instance; default tracing setting applied in get_llm\n    # \u65e5\u672c\u8a9e: \u751f\u6210\u7528LLM\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u3092\u53d6\u5f97\u3057\u307e\u3059\u3002tracing\u8a2d\u5b9a\u306fget_llm\u5074\u3067\u30c7\u30d5\u30a9\u30eb\u30c8\u5024\u3092\u4f7f\u7528\n    llm = get_llm(model) if model else None\n    # English: Determine evaluation LLM instance, fallback to generation model if evaluation_model is None\n    # \u65e5\u672c\u8a9e: \u8a55\u4fa1\u7528LLM\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u3092\u6c7a\u5b9a\u3002evaluation_model\u304cNone\u306e\u5834\u5408\u306f\u751f\u6210\u30e2\u30c7\u30eb\u3092\u4f7f\u7528\n    eval_source = evaluation_model if evaluation_model else model\n    llm_eval = get_llm(eval_source) if eval_source else None\n\n    # Agents ---------------------------------------------------------\n    self.gen_agent = Agent(\n        name=f\"{name}_generator\",\n        model=llm,\n        tools=self.generation_tools,\n        instructions=self.generation_instructions,\n        input_guardrails=self.input_guardrails,\n    )\n\n    json_instr =\"\"\"\n    ----\n    \u51fa\u529b\u30d5\u30a9\u30fc\u30de\u30c3\u30c8:\n    JSON \u3067\u5fc5\u305a\u6b21\u306e\u5f62\u5f0f\u306b\u3057\u3066\u304f\u3060\u3055\u3044:\n    {\n        \"score\": int(0\uff5e100),\n        \"comment\": [str]\n    }\"\n    \"\"\"\n    self.eval_agent = (\n        Agent(\n            name=f\"{name}_evaluator\",\n            model=llm_eval,\n            tools=self.evaluation_tools,\n            instructions=self.evaluation_instructions + json_instr,\n            output_guardrails=self.output_guardrails,\n        )\n        if self.evaluation_instructions\n        else None\n    )\n\n    self._runner = Runner()\n    self._pipeline_history: List[Dict[str, str]] = []\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.AgentPipeline.run","title":"<code>run(user_input)</code>","text":"<p>Run the pipeline with user input \u30e6\u30fc\u30b6\u30fc\u5165\u529b\u3067\u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u3092\u5b9f\u884c\u3059\u308b</p> <p>Parameters:</p> Name Type Description Default <code>user_input</code> <code>str</code> <p>User input text / \u30e6\u30fc\u30b6\u30fc\u5165\u529b\u30c6\u30ad\u30b9\u30c8</p> required <p>Returns:</p> Name Type Description <code>Any</code> <p>Processed output or None if evaluation fails / \u51e6\u7406\u6e08\u307f\u51fa\u529b\u3001\u307e\u305f\u306f\u8a55\u4fa1\u5931\u6557\u6642\u306fNone</p> Source code in <code>src\\agents_sdk_models\\pipeline.py</code> <pre><code>def run(self, user_input: str):\n    \"\"\"\n    Run the pipeline with user input\n    \u30e6\u30fc\u30b6\u30fc\u5165\u529b\u3067\u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u3092\u5b9f\u884c\u3059\u308b\n\n    Args:\n        user_input: User input text / \u30e6\u30fc\u30b6\u30fc\u5165\u529b\u30c6\u30ad\u30b9\u30c8\n\n    Returns:\n        Any: Processed output or None if evaluation fails / \u51e6\u7406\u6e08\u307f\u51fa\u529b\u3001\u307e\u305f\u306f\u8a55\u4fa1\u5931\u6557\u6642\u306fNone\n    \"\"\"\n    attempt = 0\n    last_eval_result: Optional[EvaluationResult] = None  # Store last evaluation result for retry\n    while attempt &lt;= self.retries:\n        # ---------------- Generation ----------------\n        # On retry, include prior evaluation comments if configured\n        if attempt &gt; 0 and last_eval_result and self.retry_comment_importance:\n            # Filter comments by importance\n            try:\n                comments = [c for c in last_eval_result.comment if c.get(\"importance\") in self.retry_comment_importance]\n            except Exception:\n                comments = []\n            # Format comments\n            comment_lines = \"\\n\".join(f\"- ({c.get('importance')}) {c.get('content')}\" for c in comments)\n        else:\n            comment_lines = \"\"\n        # Build base prompt\n        if attempt &gt; 0 and comment_lines:\n            base = self.dynamic_prompt(user_input) if self.dynamic_prompt else self._build_generation_prompt(user_input)\n            gen_prompt = \"\\n\".join([comment_lines, base])\n        else:\n            if self.dynamic_prompt:\n                gen_prompt = self.dynamic_prompt(user_input)\n            else:\n                gen_prompt = self._build_generation_prompt(user_input)\n\n        gen_result = self._runner.run_sync(self.gen_agent, gen_prompt)\n        raw_output_text = getattr(gen_result, \"final_output\", str(gen_result))\n        if hasattr(gen_result, \"tool_calls\") and gen_result.tool_calls:\n            raw_output_text = str(gen_result.tool_calls[0].call())\n\n        parsed_output = self._coerce_output(raw_output_text)\n        self._pipeline_history.append({\"input\": user_input, \"output\": raw_output_text})\n\n        # ---------------- Evaluation ----------------\n        if not self.eval_agent:\n            return self._route(parsed_output)\n\n        eval_prompt = self._build_evaluation_prompt(user_input, raw_output_text)\n\n        eval_raw = self._runner.run_sync(self.eval_agent, eval_prompt)\n        eval_text = getattr(eval_raw, \"final_output\", str(eval_raw))\n        try:\n            eval_dict = self._extract_json(eval_text)\n            eval_result = EvaluationResult(**eval_dict)\n        except Exception:\n            eval_result = EvaluationResult(score=0, comment=[\"\u8a55\u4fa1 JSON \u306e\u89e3\u6790\u306b\u5931\u6557\"])\n\n        if eval_result.score &gt;= self.threshold:\n            self._append_to_session(user_input, raw_output_text)\n            return self._route(parsed_output)\n\n        # Store for next retry\n        last_eval_result = eval_result\n        attempt += 1\n\n    if self.improvement_callback:\n        self.improvement_callback(parsed_output, eval_result)\n    return None\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.ClaudeModel","title":"<code>ClaudeModel</code>","text":"<p>               Bases: <code>OpenAIChatCompletionsModel</code></p> <p>Anthropic Claude model implementation that extends OpenAI's chat completions model OpenAI\u306e\u30c1\u30e3\u30c3\u30c8\u88dc\u5b8c\u30e2\u30c7\u30eb\u3092\u62e1\u5f35\u3057\u305fAnthropic Claude\u30e2\u30c7\u30eb\u306e\u5b9f\u88c5</p> Source code in <code>src\\agents_sdk_models\\anthropic.py</code> <pre><code>class ClaudeModel(OpenAIChatCompletionsModel):\n    \"\"\"\n    Anthropic Claude model implementation that extends OpenAI's chat completions model\n    OpenAI\u306e\u30c1\u30e3\u30c3\u30c8\u88dc\u5b8c\u30e2\u30c7\u30eb\u3092\u62e1\u5f35\u3057\u305fAnthropic Claude\u30e2\u30c7\u30eb\u306e\u5b9f\u88c5\n    \"\"\"\n\n    def __init__(\n        self,\n        model: str = \"claude-3-5-sonnet-latest\",\n        temperature: float = 0.3,\n        api_key: str = None,\n        base_url: str = \"https://api.anthropic.com/v1/\",\n        thinking: bool = False,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the Anthropic Claude model with OpenAI compatible interface\n        OpenAI\u4e92\u63db\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30fc\u30b9\u3067Anthropic Claude\u30e2\u30c7\u30eb\u3092\u521d\u671f\u5316\u3059\u308b\n\n        Args:\n            model (str): Name of the Claude model to use (e.g. \"claude-3-5-sonnet-latest\")\n                \u4f7f\u7528\u3059\u308bClaude\u30e2\u30c7\u30eb\u306e\u540d\u524d\uff08\u4f8b\uff1a\"claude-3-5-sonnet-latest\"\uff09\n            temperature (float): Sampling temperature between 0 and 1\n                \u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u6e29\u5ea6\uff080\u304b\u30891\u306e\u9593\uff09\n            api_key (str): Anthropic API key\n                Anthropic API\u30ad\u30fc\n            base_url (str): Base URL for the Anthropic OpenAI compatibility API\n                Anthropic OpenAI\u4e92\u63dbAPI\u306e\u30d9\u30fc\u30b9URL\n            thinking (bool): Enable extended thinking for complex reasoning\n                \u8907\u96d1\u306a\u63a8\u8ad6\u306e\u305f\u3081\u306e\u62e1\u5f35\u601d\u8003\u3092\u6709\u52b9\u306b\u3059\u308b\n            **kwargs: Additional arguments to pass to the OpenAI API\n                OpenAI API\u306b\u6e21\u3059\u8ffd\u52a0\u306e\u5f15\u6570\n        \"\"\"\n        # get_llm\u7d4c\u7531\u3067 base_url \u304c None \u306e\u5834\u5408\u306f\u30c7\u30d5\u30a9\u30eb\u30c8\u306e URL \u3092\u8a2d\u5b9a\n        if base_url == None:\n            base_url = \"https://api.anthropic.com/v1/\"\n\n        # api_key \u304c None \u306e\u5834\u5408\u306f\u74b0\u5883\u5909\u6570\u304b\u3089\u53d6\u5f97\n        if api_key is None:\n            api_key = os.environ.get(\"ANTHROPIC_API_KEY\")\n            if api_key is None:\n                raise ValueError(\"Anthropic API key is required. Get one from https://console.anthropic.com/\")\n\n        # Create AsyncOpenAI client with Anthropic base URL\n        # Anthropic\u306e\u30d9\u30fc\u30b9URL\u3067AsyncOpenAI\u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u3092\u4f5c\u6210\n        openai_client = AsyncOpenAI(base_url=base_url, api_key=api_key)\n\n        # Store the AsyncOpenAI client on the instance for direct access\n        # \u30c6\u30b9\u30c8\u3067\u53c2\u7167\u3067\u304d\u308b\u3088\u3046 AsyncOpenAI \u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u3092\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u306b\u4fdd\u5b58\u3059\u308b\n        self.openai_client = openai_client\n\n        # Store parameters for later use in API calls\n        # \u5f8c\u3067API\u30b3\u30fc\u30eb\u3067\u4f7f\u7528\u3059\u308b\u305f\u3081\u306b\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u4fdd\u5b58\n        self.temperature = temperature\n        self.thinking = thinking\n        self.kwargs = kwargs\n\n        # Initialize the parent class with our custom client\n        # \u30ab\u30b9\u30bf\u30e0\u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u3067\u89aa\u30af\u30e9\u30b9\u3092\u521d\u671f\u5316\n        super().__init__(\n            model=model,\n            openai_client=openai_client\n        )\n\n    # Override methods that make API calls to include our parameters\n    # API\u30b3\u30fc\u30eb\u3092\u884c\u3046\u30e1\u30bd\u30c3\u30c9\u3092\u30aa\u30fc\u30d0\u30fc\u30e9\u30a4\u30c9\u3057\u3066\u3001\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u542b\u3081\u308b\n    async def _create_chat_completion(self, *args, **kwargs):\n        \"\"\"Override to include temperature, thinking and other parameters\"\"\"\n        kwargs[\"temperature\"] = self.temperature\n\n        # Add thinking parameter if enabled\n        # \u62e1\u5f35\u601d\u8003\u304c\u6709\u52b9\u306a\u5834\u5408\u306fthinking\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u8ffd\u52a0\n        if self.thinking:\n            kwargs[\"thinking\"] = True\n\n        # Add any other custom parameters\n        # \u305d\u306e\u4ed6\u306e\u30ab\u30b9\u30bf\u30e0\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u8ffd\u52a0\n        kwargs.update(self.kwargs)\n\n        return await super()._create_chat_completion(*args, **kwargs)\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.ClaudeModel.__init__","title":"<code>__init__(model='claude-3-5-sonnet-latest', temperature=0.3, api_key=None, base_url='https://api.anthropic.com/v1/', thinking=False, **kwargs)</code>","text":"<p>Initialize the Anthropic Claude model with OpenAI compatible interface OpenAI\u4e92\u63db\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30fc\u30b9\u3067Anthropic Claude\u30e2\u30c7\u30eb\u3092\u521d\u671f\u5316\u3059\u308b</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Name of the Claude model to use (e.g. \"claude-3-5-sonnet-latest\") \u4f7f\u7528\u3059\u308bClaude\u30e2\u30c7\u30eb\u306e\u540d\u524d\uff08\u4f8b\uff1a\"claude-3-5-sonnet-latest\"\uff09</p> <code>'claude-3-5-sonnet-latest'</code> <code>temperature</code> <code>float</code> <p>Sampling temperature between 0 and 1 \u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u6e29\u5ea6\uff080\u304b\u30891\u306e\u9593\uff09</p> <code>0.3</code> <code>api_key</code> <code>str</code> <p>Anthropic API key Anthropic API\u30ad\u30fc</p> <code>None</code> <code>base_url</code> <code>str</code> <p>Base URL for the Anthropic OpenAI compatibility API Anthropic OpenAI\u4e92\u63dbAPI\u306e\u30d9\u30fc\u30b9URL</p> <code>'https://api.anthropic.com/v1/'</code> <code>thinking</code> <code>bool</code> <p>Enable extended thinking for complex reasoning \u8907\u96d1\u306a\u63a8\u8ad6\u306e\u305f\u3081\u306e\u62e1\u5f35\u601d\u8003\u3092\u6709\u52b9\u306b\u3059\u308b</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments to pass to the OpenAI API OpenAI API\u306b\u6e21\u3059\u8ffd\u52a0\u306e\u5f15\u6570</p> <code>{}</code> Source code in <code>src\\agents_sdk_models\\anthropic.py</code> <pre><code>def __init__(\n    self,\n    model: str = \"claude-3-5-sonnet-latest\",\n    temperature: float = 0.3,\n    api_key: str = None,\n    base_url: str = \"https://api.anthropic.com/v1/\",\n    thinking: bool = False,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initialize the Anthropic Claude model with OpenAI compatible interface\n    OpenAI\u4e92\u63db\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30fc\u30b9\u3067Anthropic Claude\u30e2\u30c7\u30eb\u3092\u521d\u671f\u5316\u3059\u308b\n\n    Args:\n        model (str): Name of the Claude model to use (e.g. \"claude-3-5-sonnet-latest\")\n            \u4f7f\u7528\u3059\u308bClaude\u30e2\u30c7\u30eb\u306e\u540d\u524d\uff08\u4f8b\uff1a\"claude-3-5-sonnet-latest\"\uff09\n        temperature (float): Sampling temperature between 0 and 1\n            \u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u6e29\u5ea6\uff080\u304b\u30891\u306e\u9593\uff09\n        api_key (str): Anthropic API key\n            Anthropic API\u30ad\u30fc\n        base_url (str): Base URL for the Anthropic OpenAI compatibility API\n            Anthropic OpenAI\u4e92\u63dbAPI\u306e\u30d9\u30fc\u30b9URL\n        thinking (bool): Enable extended thinking for complex reasoning\n            \u8907\u96d1\u306a\u63a8\u8ad6\u306e\u305f\u3081\u306e\u62e1\u5f35\u601d\u8003\u3092\u6709\u52b9\u306b\u3059\u308b\n        **kwargs: Additional arguments to pass to the OpenAI API\n            OpenAI API\u306b\u6e21\u3059\u8ffd\u52a0\u306e\u5f15\u6570\n    \"\"\"\n    # get_llm\u7d4c\u7531\u3067 base_url \u304c None \u306e\u5834\u5408\u306f\u30c7\u30d5\u30a9\u30eb\u30c8\u306e URL \u3092\u8a2d\u5b9a\n    if base_url == None:\n        base_url = \"https://api.anthropic.com/v1/\"\n\n    # api_key \u304c None \u306e\u5834\u5408\u306f\u74b0\u5883\u5909\u6570\u304b\u3089\u53d6\u5f97\n    if api_key is None:\n        api_key = os.environ.get(\"ANTHROPIC_API_KEY\")\n        if api_key is None:\n            raise ValueError(\"Anthropic API key is required. Get one from https://console.anthropic.com/\")\n\n    # Create AsyncOpenAI client with Anthropic base URL\n    # Anthropic\u306e\u30d9\u30fc\u30b9URL\u3067AsyncOpenAI\u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u3092\u4f5c\u6210\n    openai_client = AsyncOpenAI(base_url=base_url, api_key=api_key)\n\n    # Store the AsyncOpenAI client on the instance for direct access\n    # \u30c6\u30b9\u30c8\u3067\u53c2\u7167\u3067\u304d\u308b\u3088\u3046 AsyncOpenAI \u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u3092\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u306b\u4fdd\u5b58\u3059\u308b\n    self.openai_client = openai_client\n\n    # Store parameters for later use in API calls\n    # \u5f8c\u3067API\u30b3\u30fc\u30eb\u3067\u4f7f\u7528\u3059\u308b\u305f\u3081\u306b\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u4fdd\u5b58\n    self.temperature = temperature\n    self.thinking = thinking\n    self.kwargs = kwargs\n\n    # Initialize the parent class with our custom client\n    # \u30ab\u30b9\u30bf\u30e0\u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u3067\u89aa\u30af\u30e9\u30b9\u3092\u521d\u671f\u5316\n    super().__init__(\n        model=model,\n        openai_client=openai_client\n    )\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.EvaluationResult","title":"<code>EvaluationResult</code>  <code>dataclass</code>","text":"<p>Result of evaluation for generated content \u751f\u6210\u3055\u308c\u305f\u30b3\u30f3\u30c6\u30f3\u30c4\u306e\u8a55\u4fa1\u7d50\u679c\u3092\u4fdd\u6301\u3059\u308b\u30af\u30e9\u30b9</p> Source code in <code>src\\agents_sdk_models\\pipeline.py</code> <pre><code>@dataclass\nclass EvaluationResult:\n    \"\"\"\n    Result of evaluation for generated content\n    \u751f\u6210\u3055\u308c\u305f\u30b3\u30f3\u30c6\u30f3\u30c4\u306e\u8a55\u4fa1\u7d50\u679c\u3092\u4fdd\u6301\u3059\u308b\u30af\u30e9\u30b9\n    \"\"\"\n    score: int  # Evaluation score (0-100) / \u8a55\u4fa1\u30b9\u30b3\u30a2\uff080-100\uff09\n    comment: List[str]  # List of evaluation comments / \u8a55\u4fa1\u30b3\u30e1\u30f3\u30c8\u306e\u30ea\u30b9\u30c8\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.GeminiModel","title":"<code>GeminiModel</code>","text":"<p>               Bases: <code>OpenAIChatCompletionsModel</code></p> <p>Gemini model implementation that extends OpenAI's chat completions model OpenAI\u306e\u30c1\u30e3\u30c3\u30c8\u88dc\u5b8c\u30e2\u30c7\u30eb\u3092\u62e1\u5f35\u3057\u305fGemini\u30e2\u30c7\u30eb\u306e\u5b9f\u88c5</p> Source code in <code>src\\agents_sdk_models\\gemini.py</code> <pre><code>class GeminiModel(OpenAIChatCompletionsModel):\n    \"\"\"\n    Gemini model implementation that extends OpenAI's chat completions model\n    OpenAI\u306e\u30c1\u30e3\u30c3\u30c8\u88dc\u5b8c\u30e2\u30c7\u30eb\u3092\u62e1\u5f35\u3057\u305fGemini\u30e2\u30c7\u30eb\u306e\u5b9f\u88c5\n    \"\"\"\n\n    def __init__(\n        self,\n        model: str = \"gemini-2.0-flash\",\n        temperature: float = 0.3,\n        api_key: str = None,\n        base_url: str = \"https://generativelanguage.googleapis.com/v1beta/openai/\",\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the Gemini model with OpenAI compatible interface\n        OpenAI\u4e92\u63db\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30fc\u30b9\u3067Gemini\u30e2\u30c7\u30eb\u3092\u521d\u671f\u5316\u3059\u308b\n\n        Args:\n            model (str): Name of the Gemini model to use (e.g. \"gemini-2.0-flash\")\n                \u4f7f\u7528\u3059\u308bGemini\u30e2\u30c7\u30eb\u306e\u540d\u524d\uff08\u4f8b\uff1a\"gemini-2.0-flash\"\uff09\n            temperature (float): Sampling temperature between 0 and 1\n                \u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u6e29\u5ea6\uff080\u304b\u30891\u306e\u9593\uff09\n            api_key (str): Gemini API key\n                Gemini API\u30ad\u30fc\n            base_url (str): Base URL for the Gemini API\n                Gemini API\u306e\u30d9\u30fc\u30b9URL\n            **kwargs: Additional arguments to pass to the OpenAI API\n                OpenAI API\u306b\u6e21\u3059\u8ffd\u52a0\u306e\u5f15\u6570\n        \"\"\"\n        if base_url == None:\n            base_url = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n\n        # api_key \u304c None \u306e\u5834\u5408\u306f\u74b0\u5883\u5909\u6570\u304b\u3089\u53d6\u5f97\n        if api_key is None:\n            api_key = os.environ.get(\"GOOGLE_API_KEY\")\n            if api_key is None:\n                raise ValueError(\"Google API key is required. Get one from https://ai.google.dev/\")\n\n        # Create AsyncOpenAI client with Gemini base URL\n        # Gemini\u306e\u30d9\u30fc\u30b9URL\u3067AsyncOpenAI\u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u3092\u4f5c\u6210\n        openai_client = AsyncOpenAI(base_url=base_url, api_key=api_key)\n\n        # Store parameters for later use in API calls\n        # \u5f8c\u3067API\u30b3\u30fc\u30eb\u3067\u4f7f\u7528\u3059\u308b\u305f\u3081\u306b\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u4fdd\u5b58\n        self.temperature = temperature\n        self.kwargs = kwargs\n\n        # Initialize the parent class with our custom client\n        # \u30ab\u30b9\u30bf\u30e0\u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u3067\u89aa\u30af\u30e9\u30b9\u3092\u521d\u671f\u5316\n        super().__init__(\n            model=model,\n            openai_client=openai_client\n        )\n\n    # Override methods that make API calls to include our parameters\n    # API\u30b3\u30fc\u30eb\u3092\u884c\u3046\u30e1\u30bd\u30c3\u30c9\u3092\u30aa\u30fc\u30d0\u30fc\u30e9\u30a4\u30c9\u3057\u3066\u3001\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u542b\u3081\u308b\n    async def _create_chat_completion(self, *args, **kwargs):\n        \"\"\"Override to include temperature and other parameters\"\"\"\n        kwargs[\"temperature\"] = self.temperature\n        kwargs.update(self.kwargs)\n        return await super()._create_chat_completion(*args, **kwargs)\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.GeminiModel.__init__","title":"<code>__init__(model='gemini-2.0-flash', temperature=0.3, api_key=None, base_url='https://generativelanguage.googleapis.com/v1beta/openai/', **kwargs)</code>","text":"<p>Initialize the Gemini model with OpenAI compatible interface OpenAI\u4e92\u63db\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30fc\u30b9\u3067Gemini\u30e2\u30c7\u30eb\u3092\u521d\u671f\u5316\u3059\u308b</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Name of the Gemini model to use (e.g. \"gemini-2.0-flash\") \u4f7f\u7528\u3059\u308bGemini\u30e2\u30c7\u30eb\u306e\u540d\u524d\uff08\u4f8b\uff1a\"gemini-2.0-flash\"\uff09</p> <code>'gemini-2.0-flash'</code> <code>temperature</code> <code>float</code> <p>Sampling temperature between 0 and 1 \u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u6e29\u5ea6\uff080\u304b\u30891\u306e\u9593\uff09</p> <code>0.3</code> <code>api_key</code> <code>str</code> <p>Gemini API key Gemini API\u30ad\u30fc</p> <code>None</code> <code>base_url</code> <code>str</code> <p>Base URL for the Gemini API Gemini API\u306e\u30d9\u30fc\u30b9URL</p> <code>'https://generativelanguage.googleapis.com/v1beta/openai/'</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments to pass to the OpenAI API OpenAI API\u306b\u6e21\u3059\u8ffd\u52a0\u306e\u5f15\u6570</p> <code>{}</code> Source code in <code>src\\agents_sdk_models\\gemini.py</code> <pre><code>def __init__(\n    self,\n    model: str = \"gemini-2.0-flash\",\n    temperature: float = 0.3,\n    api_key: str = None,\n    base_url: str = \"https://generativelanguage.googleapis.com/v1beta/openai/\",\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initialize the Gemini model with OpenAI compatible interface\n    OpenAI\u4e92\u63db\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30fc\u30b9\u3067Gemini\u30e2\u30c7\u30eb\u3092\u521d\u671f\u5316\u3059\u308b\n\n    Args:\n        model (str): Name of the Gemini model to use (e.g. \"gemini-2.0-flash\")\n            \u4f7f\u7528\u3059\u308bGemini\u30e2\u30c7\u30eb\u306e\u540d\u524d\uff08\u4f8b\uff1a\"gemini-2.0-flash\"\uff09\n        temperature (float): Sampling temperature between 0 and 1\n            \u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u6e29\u5ea6\uff080\u304b\u30891\u306e\u9593\uff09\n        api_key (str): Gemini API key\n            Gemini API\u30ad\u30fc\n        base_url (str): Base URL for the Gemini API\n            Gemini API\u306e\u30d9\u30fc\u30b9URL\n        **kwargs: Additional arguments to pass to the OpenAI API\n            OpenAI API\u306b\u6e21\u3059\u8ffd\u52a0\u306e\u5f15\u6570\n    \"\"\"\n    if base_url == None:\n        base_url = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n\n    # api_key \u304c None \u306e\u5834\u5408\u306f\u74b0\u5883\u5909\u6570\u304b\u3089\u53d6\u5f97\n    if api_key is None:\n        api_key = os.environ.get(\"GOOGLE_API_KEY\")\n        if api_key is None:\n            raise ValueError(\"Google API key is required. Get one from https://ai.google.dev/\")\n\n    # Create AsyncOpenAI client with Gemini base URL\n    # Gemini\u306e\u30d9\u30fc\u30b9URL\u3067AsyncOpenAI\u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u3092\u4f5c\u6210\n    openai_client = AsyncOpenAI(base_url=base_url, api_key=api_key)\n\n    # Store parameters for later use in API calls\n    # \u5f8c\u3067API\u30b3\u30fc\u30eb\u3067\u4f7f\u7528\u3059\u308b\u305f\u3081\u306b\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u4fdd\u5b58\n    self.temperature = temperature\n    self.kwargs = kwargs\n\n    # Initialize the parent class with our custom client\n    # \u30ab\u30b9\u30bf\u30e0\u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u3067\u89aa\u30af\u30e9\u30b9\u3092\u521d\u671f\u5316\n    super().__init__(\n        model=model,\n        openai_client=openai_client\n    )\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.OllamaModel","title":"<code>OllamaModel</code>","text":"<p>               Bases: <code>OpenAIChatCompletionsModel</code></p> <p>Ollama model implementation that extends OpenAI's chat completions model OpenAI\u306e\u30c1\u30e3\u30c3\u30c8\u88dc\u5b8c\u30e2\u30c7\u30eb\u3092\u62e1\u5f35\u3057\u305fOllama\u30e2\u30c7\u30eb\u306e\u5b9f\u88c5</p> Source code in <code>src\\agents_sdk_models\\ollama.py</code> <pre><code>class OllamaModel(OpenAIChatCompletionsModel):\n    \"\"\"\n    Ollama model implementation that extends OpenAI's chat completions model\n    OpenAI\u306e\u30c1\u30e3\u30c3\u30c8\u88dc\u5b8c\u30e2\u30c7\u30eb\u3092\u62e1\u5f35\u3057\u305fOllama\u30e2\u30c7\u30eb\u306e\u5b9f\u88c5\n    \"\"\"\n\n    def __init__(\n        self,\n        model: str = \"phi4-mini:latest\",\n        temperature: float = 0.3,\n        base_url: str = None, # \u30c7\u30d5\u30a9\u30eb\u30c8\u306eURL\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the Ollama model with OpenAI compatible interface\n        OpenAI\u4e92\u63db\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30fc\u30b9\u3067Ollama\u30e2\u30c7\u30eb\u3092\u521d\u671f\u5316\u3059\u308b\n\n        Args:\n            model (str): Name of the Ollama model to use (e.g. \"phi4-mini\")\n                \u4f7f\u7528\u3059\u308bOllama\u30e2\u30c7\u30eb\u306e\u540d\u524d\uff08\u4f8b\uff1a\"phi4-mini\"\uff09\n            temperature (float): Sampling temperature between 0 and 1\n                \u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u6e29\u5ea6\uff080\u304b\u30891\u306e\u9593\uff09\n            base_url (str): Base URL for the Ollama API\n                Ollama API\u306e\u30d9\u30fc\u30b9URL\n            **kwargs: Additional arguments to pass to the OpenAI API\n                OpenAI API\u306b\u6e21\u3059\u8ffd\u52a0\u306e\u5f15\u6570\n        \"\"\"\n        # get_llm\u7d4c\u7531\u3067 base_url \u304c None \u306e\u5834\u5408\u306f\u30c7\u30d5\u30a9\u30eb\u30c8\u306e URL \u3092\u8a2d\u5b9a\n        if base_url == None:\n            base_url = os.environ.get(\"OLLAMA_BASE_URL\", \"http://localhost:11434\")\n\n        base_url = base_url.rstrip(\"/\")\n        if not base_url.endswith(\"v1\"):\n            base_url = base_url + \"/v1\"\n\n        # Create AsyncOpenAI client with Ollama base URL\n        # Ollama\u306e\u30d9\u30fc\u30b9URL\u3067AsyncOpenAI\u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u3092\u4f5c\u6210\n        openai_client = AsyncOpenAI(base_url=base_url, api_key=\"ollama\")\n\n        # Store the AsyncOpenAI client on the instance for direct access\n        # \u30c6\u30b9\u30c8\u3067\u53c2\u7167\u3067\u304d\u308b\u3088\u3046 AsyncOpenAI \u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u3092\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u306b\u4fdd\u5b58\u3059\u308b\n        self.openai_client = openai_client\n\n        # Store parameters for later use in API calls\n        # \u5f8c\u3067API\u30b3\u30fc\u30eb\u3067\u4f7f\u7528\u3059\u308b\u305f\u3081\u306b\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u4fdd\u5b58\n        self.temperature = temperature\n        self.kwargs = kwargs\n\n        # Initialize the parent class with our custom client\n        # \u30ab\u30b9\u30bf\u30e0\u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u3067\u89aa\u30af\u30e9\u30b9\u3092\u521d\u671f\u5316\n        super().__init__(\n            model=model,\n            openai_client=openai_client\n        )\n\n    # Override methods that make API calls to include our parameters\n    # API\u30b3\u30fc\u30eb\u3092\u884c\u3046\u30e1\u30bd\u30c3\u30c9\u3092\u30aa\u30fc\u30d0\u30fc\u30e9\u30a4\u30c9\u3057\u3066\u3001\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u542b\u3081\u308b\n    async def _create_chat_completion(self, *args, **kwargs):\n        \"\"\"Override to include temperature and other parameters\"\"\"\n        kwargs[\"temperature\"] = self.temperature\n        kwargs.update(self.kwargs)\n        return await super()._create_chat_completion(*args, **kwargs)\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.OllamaModel.__init__","title":"<code>__init__(model='phi4-mini:latest', temperature=0.3, base_url=None, **kwargs)</code>","text":"<p>Initialize the Ollama model with OpenAI compatible interface OpenAI\u4e92\u63db\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30fc\u30b9\u3067Ollama\u30e2\u30c7\u30eb\u3092\u521d\u671f\u5316\u3059\u308b</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Name of the Ollama model to use (e.g. \"phi4-mini\") \u4f7f\u7528\u3059\u308bOllama\u30e2\u30c7\u30eb\u306e\u540d\u524d\uff08\u4f8b\uff1a\"phi4-mini\"\uff09</p> <code>'phi4-mini:latest'</code> <code>temperature</code> <code>float</code> <p>Sampling temperature between 0 and 1 \u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u6e29\u5ea6\uff080\u304b\u30891\u306e\u9593\uff09</p> <code>0.3</code> <code>base_url</code> <code>str</code> <p>Base URL for the Ollama API Ollama API\u306e\u30d9\u30fc\u30b9URL</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments to pass to the OpenAI API OpenAI API\u306b\u6e21\u3059\u8ffd\u52a0\u306e\u5f15\u6570</p> <code>{}</code> Source code in <code>src\\agents_sdk_models\\ollama.py</code> <pre><code>def __init__(\n    self,\n    model: str = \"phi4-mini:latest\",\n    temperature: float = 0.3,\n    base_url: str = None, # \u30c7\u30d5\u30a9\u30eb\u30c8\u306eURL\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initialize the Ollama model with OpenAI compatible interface\n    OpenAI\u4e92\u63db\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30fc\u30b9\u3067Ollama\u30e2\u30c7\u30eb\u3092\u521d\u671f\u5316\u3059\u308b\n\n    Args:\n        model (str): Name of the Ollama model to use (e.g. \"phi4-mini\")\n            \u4f7f\u7528\u3059\u308bOllama\u30e2\u30c7\u30eb\u306e\u540d\u524d\uff08\u4f8b\uff1a\"phi4-mini\"\uff09\n        temperature (float): Sampling temperature between 0 and 1\n            \u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u6e29\u5ea6\uff080\u304b\u30891\u306e\u9593\uff09\n        base_url (str): Base URL for the Ollama API\n            Ollama API\u306e\u30d9\u30fc\u30b9URL\n        **kwargs: Additional arguments to pass to the OpenAI API\n            OpenAI API\u306b\u6e21\u3059\u8ffd\u52a0\u306e\u5f15\u6570\n    \"\"\"\n    # get_llm\u7d4c\u7531\u3067 base_url \u304c None \u306e\u5834\u5408\u306f\u30c7\u30d5\u30a9\u30eb\u30c8\u306e URL \u3092\u8a2d\u5b9a\n    if base_url == None:\n        base_url = os.environ.get(\"OLLAMA_BASE_URL\", \"http://localhost:11434\")\n\n    base_url = base_url.rstrip(\"/\")\n    if not base_url.endswith(\"v1\"):\n        base_url = base_url + \"/v1\"\n\n    # Create AsyncOpenAI client with Ollama base URL\n    # Ollama\u306e\u30d9\u30fc\u30b9URL\u3067AsyncOpenAI\u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u3092\u4f5c\u6210\n    openai_client = AsyncOpenAI(base_url=base_url, api_key=\"ollama\")\n\n    # Store the AsyncOpenAI client on the instance for direct access\n    # \u30c6\u30b9\u30c8\u3067\u53c2\u7167\u3067\u304d\u308b\u3088\u3046 AsyncOpenAI \u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u3092\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u306b\u4fdd\u5b58\u3059\u308b\n    self.openai_client = openai_client\n\n    # Store parameters for later use in API calls\n    # \u5f8c\u3067API\u30b3\u30fc\u30eb\u3067\u4f7f\u7528\u3059\u308b\u305f\u3081\u306b\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u4fdd\u5b58\n    self.temperature = temperature\n    self.kwargs = kwargs\n\n    # Initialize the parent class with our custom client\n    # \u30ab\u30b9\u30bf\u30e0\u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u3067\u89aa\u30af\u30e9\u30b9\u3092\u521d\u671f\u5316\n    super().__init__(\n        model=model,\n        openai_client=openai_client\n    )\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.disable_tracing","title":"<code>disable_tracing()</code>","text":"<p>English: Disable all tracing. \u65e5\u672c\u8a9e: \u30c8\u30ec\u30fc\u30b7\u30f3\u30b0\u6a5f\u80fd\u3092\u3059\u3079\u3066\u7121\u52b9\u5316\u3057\u307e\u3059\u3002</p> Source code in <code>src\\agents_sdk_models\\tracing.py</code> <pre><code>def disable_tracing():\n    \"\"\"\n    English: Disable all tracing.\n    \u65e5\u672c\u8a9e: \u30c8\u30ec\u30fc\u30b7\u30f3\u30b0\u6a5f\u80fd\u3092\u3059\u3079\u3066\u7121\u52b9\u5316\u3057\u307e\u3059\u3002\n    \"\"\"\n    set_tracing_disabled(True)\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.enable_console_tracing","title":"<code>enable_console_tracing()</code>","text":"<p>English: Enable console tracing by registering ConsoleTracingProcessor and enabling tracing. \u65e5\u672c\u8a9e: ConsoleTracingProcessor\u3092\u767b\u9332\u3057\u3066\u30c8\u30ec\u30fc\u30b7\u30f3\u30b0\u3092\u6709\u52b9\u5316\u3057\u307e\u3059\u3002</p> Source code in <code>src\\agents_sdk_models\\tracing.py</code> <pre><code>def enable_console_tracing():\n    \"\"\"\n    English: Enable console tracing by registering ConsoleTracingProcessor and enabling tracing.\n    \u65e5\u672c\u8a9e: ConsoleTracingProcessor\u3092\u767b\u9332\u3057\u3066\u30c8\u30ec\u30fc\u30b7\u30f3\u30b0\u3092\u6709\u52b9\u5316\u3057\u307e\u3059\u3002\n    \"\"\"\n    # Enable tracing in Agents SDK\n    set_tracing_disabled(False)\n    # Register console tracing processor\n    add_trace_processor(ConsoleTracingProcessor())\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.get_llm","title":"<code>get_llm(model=None, provider=None, temperature=0.3, api_key=None, base_url=None, thinking=False, **kwargs)</code>","text":"<p>Factory function to get an instance of a language model based on the provider.</p> <p>English: Factory function to get an instance of a language model based on the provider.</p> <p>\u65e5\u672c\u8a9e: \u30d7\u30ed\u30d0\u30a4\u30c0\u30fc\u306b\u57fa\u3065\u3044\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u306e\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u3092\u53d6\u5f97\u3059\u308b\u30d5\u30a1\u30af\u30c8\u30ea\u95a2\u6570\u3002</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>ProviderType</code> <p>The LLM provider (\"openai\", \"google\", \"anthropic\", \"ollama\"). Defaults to \"openai\". LLM \u30d7\u30ed\u30d0\u30a4\u30c0\u30fc (\"openai\", \"google\", \"anthropic\", \"ollama\")\u3002\u30c7\u30d5\u30a9\u30eb\u30c8\u306f \"openai\"\u3002</p> <code>None</code> <code>model</code> <code>Optional[str]</code> <p>The specific model name for the provider. If None, uses the default for the provider. \u30d7\u30ed\u30d0\u30a4\u30c0\u30fc\u56fa\u6709\u306e\u30e2\u30c7\u30eb\u540d\u3002None \u306e\u5834\u5408\u3001\u30d7\u30ed\u30d0\u30a4\u30c0\u30fc\u306e\u30c7\u30d5\u30a9\u30eb\u30c8\u3092\u4f7f\u7528\u3057\u307e\u3059\u3002</p> <code>None</code> <code>temperature</code> <code>float</code> <p>Sampling temperature. Defaults to 0.3. \u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u6e29\u5ea6\u3002\u30c7\u30d5\u30a9\u30eb\u30c8\u306f 0.3\u3002</p> <code>0.3</code> <code>api_key</code> <code>Optional[str]</code> <p>API key for the provider, if required. \u30d7\u30ed\u30d0\u30a4\u30c0\u30fc\u306e API \u30ad\u30fc (\u5fc5\u8981\u306a\u5834\u5408)\u3002</p> <code>None</code> <code>base_url</code> <code>Optional[str]</code> <p>Base URL for the provider's API, if needed (e.g., for self-hosted Ollama or OpenAI-compatible APIs). \u30d7\u30ed\u30d0\u30a4\u30c0\u30fc API \u306e\u30d9\u30fc\u30b9 URL (\u5fc5\u8981\u306a\u5834\u5408\u3001\u4f8b: \u30bb\u30eb\u30d5\u30db\u30b9\u30c8\u306e Ollama \u3084 OpenAI \u4e92\u63db API)\u3002</p> <code>None</code> <code>thinking</code> <code>bool</code> <p>Enable thinking mode for Claude models. Defaults to False. Claude \u30e2\u30c7\u30eb\u306e\u601d\u8003\u30e2\u30fc\u30c9\u3092\u6709\u52b9\u306b\u3059\u308b\u304b\u3002\u30c7\u30d5\u30a9\u30eb\u30c8\u306f False\u3002</p> <code>False</code> <code>tracing</code> <code>bool</code> <p>Whether to enable tracing for the Agents SDK. Defaults to False. Agents SDK \u306e\u30c8\u30ec\u30fc\u30b7\u30f3\u30b0\u3092\u6709\u52b9\u5316\u3059\u308b\u304b\u3002\u30c7\u30d5\u30a9\u30eb\u30c8\u306f False\u3002</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the model constructor. \u30e2\u30c7\u30eb\u306e\u30b3\u30f3\u30b9\u30c8\u30e9\u30af\u30bf\u306b\u6e21\u3059\u8ffd\u52a0\u306e\u30ad\u30fc\u30ef\u30fc\u30c9\u5f15\u6570\u3002</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Model</code> <code>Model</code> <p>An instance of the appropriate language model class.    \u9069\u5207\u306a\u8a00\u8a9e\u30e2\u30c7\u30eb\u30af\u30e9\u30b9\u306e\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u3002</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an unsupported provider is specified.         \u30b5\u30dd\u30fc\u30c8\u3055\u308c\u3066\u3044\u306a\u3044\u30d7\u30ed\u30d0\u30a4\u30c0\u30fc\u304c\u6307\u5b9a\u3055\u308c\u305f\u5834\u5408\u3002</p> Source code in <code>src\\agents_sdk_models\\llm.py</code> <pre><code>def get_llm(\n    model: Optional[str] = None,\n    provider: Optional[ProviderType] = None,\n    temperature: float = 0.3,\n    api_key: Optional[str] = None,\n    base_url: Optional[str] = None,\n    thinking: bool = False,\n    **kwargs: Any,\n) -&gt; Model:\n    \"\"\"\n    Factory function to get an instance of a language model based on the provider.\n\n    English:\n    Factory function to get an instance of a language model based on the provider.\n\n    \u65e5\u672c\u8a9e:\n    \u30d7\u30ed\u30d0\u30a4\u30c0\u30fc\u306b\u57fa\u3065\u3044\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u306e\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u3092\u53d6\u5f97\u3059\u308b\u30d5\u30a1\u30af\u30c8\u30ea\u95a2\u6570\u3002\n\n    Args:\n        provider (ProviderType): The LLM provider (\"openai\", \"google\", \"anthropic\", \"ollama\"). Defaults to \"openai\".\n            LLM \u30d7\u30ed\u30d0\u30a4\u30c0\u30fc (\"openai\", \"google\", \"anthropic\", \"ollama\")\u3002\u30c7\u30d5\u30a9\u30eb\u30c8\u306f \"openai\"\u3002\n        model (Optional[str]): The specific model name for the provider. If None, uses the default for the provider.\n            \u30d7\u30ed\u30d0\u30a4\u30c0\u30fc\u56fa\u6709\u306e\u30e2\u30c7\u30eb\u540d\u3002None \u306e\u5834\u5408\u3001\u30d7\u30ed\u30d0\u30a4\u30c0\u30fc\u306e\u30c7\u30d5\u30a9\u30eb\u30c8\u3092\u4f7f\u7528\u3057\u307e\u3059\u3002\n        temperature (float): Sampling temperature. Defaults to 0.3.\n            \u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u6e29\u5ea6\u3002\u30c7\u30d5\u30a9\u30eb\u30c8\u306f 0.3\u3002\n        api_key (Optional[str]): API key for the provider, if required.\n            \u30d7\u30ed\u30d0\u30a4\u30c0\u30fc\u306e API \u30ad\u30fc (\u5fc5\u8981\u306a\u5834\u5408)\u3002\n        base_url (Optional[str]): Base URL for the provider's API, if needed (e.g., for self-hosted Ollama or OpenAI-compatible APIs).\n            \u30d7\u30ed\u30d0\u30a4\u30c0\u30fc API \u306e\u30d9\u30fc\u30b9 URL (\u5fc5\u8981\u306a\u5834\u5408\u3001\u4f8b: \u30bb\u30eb\u30d5\u30db\u30b9\u30c8\u306e Ollama \u3084 OpenAI \u4e92\u63db API)\u3002\n        thinking (bool): Enable thinking mode for Claude models. Defaults to False.\n            Claude \u30e2\u30c7\u30eb\u306e\u601d\u8003\u30e2\u30fc\u30c9\u3092\u6709\u52b9\u306b\u3059\u308b\u304b\u3002\u30c7\u30d5\u30a9\u30eb\u30c8\u306f False\u3002\n        tracing (bool): Whether to enable tracing for the Agents SDK. Defaults to False.\n            Agents SDK \u306e\u30c8\u30ec\u30fc\u30b7\u30f3\u30b0\u3092\u6709\u52b9\u5316\u3059\u308b\u304b\u3002\u30c7\u30d5\u30a9\u30eb\u30c8\u306f False\u3002\n        **kwargs (Any): Additional keyword arguments to pass to the model constructor.\n            \u30e2\u30c7\u30eb\u306e\u30b3\u30f3\u30b9\u30c8\u30e9\u30af\u30bf\u306b\u6e21\u3059\u8ffd\u52a0\u306e\u30ad\u30fc\u30ef\u30fc\u30c9\u5f15\u6570\u3002\n\n    Returns:\n        Model: An instance of the appropriate language model class.\n               \u9069\u5207\u306a\u8a00\u8a9e\u30e2\u30c7\u30eb\u30af\u30e9\u30b9\u306e\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u3002\n\n    Raises:\n        ValueError: If an unsupported provider is specified.\n                    \u30b5\u30dd\u30fc\u30c8\u3055\u308c\u3066\u3044\u306a\u3044\u30d7\u30ed\u30d0\u30a4\u30c0\u30fc\u304c\u6307\u5b9a\u3055\u308c\u305f\u5834\u5408\u3002\n    \"\"\"\n    # English: Configure OpenAI Agents SDK tracing\n    # \u65e5\u672c\u8a9e: OpenAI Agents SDK \u306e\u30c8\u30ec\u30fc\u30b7\u30f3\u30b0\u3092\u8a2d\u5b9a\u3059\u308b\n    # set_tracing_disabled(not tracing)\n\n\n    if model is None:\n        model = os.environ.get(\"LLM_MODEL\", \"gpt-4o-mini\")\n\n    def get_provider_canditate(model: str) -&gt; ProviderType:\n        if \"gpt\" in model:\n            return \"openai\"\n        if \"o3\" in model or \"o4\" in model:\n            return \"openai\"\n        elif \"gemini\" in model:\n            return \"google\"\n        elif \"claude\" in model:\n            return \"anthropic\"\n        else:\n            return \"ollama\"\n\n    if provider is None:\n        provider = get_provider_canditate(model)\n\n    if provider == \"openai\":\n        # Use the standard OpenAI model from the agents library\n        # agents\u30e9\u30a4\u30d6\u30e9\u30ea\u306e\u6a19\u6e96 OpenAI \u30e2\u30c7\u30eb\u3092\u4f7f\u7528\n        openai_kwargs = kwargs.copy()\n\n        # English: Prepare arguments for OpenAI client and model\n        # \u65e5\u672c\u8a9e: OpenAI \u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u3068\u30e2\u30c7\u30eb\u306e\u5f15\u6570\u3092\u6e96\u5099\n        client_args = {}\n        model_args = {}\n\n        # English: Set API key for client\n        # \u65e5\u672c\u8a9e: \u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u306b API \u30ad\u30fc\u3092\u8a2d\u5b9a\n        if api_key:\n            client_args['api_key'] = api_key\n        # English: Set base URL for client\n        # \u65e5\u672c\u8a9e: \u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u306b\u30d9\u30fc\u30b9 URL \u3092\u8a2d\u5b9a\n        if base_url:\n            client_args['base_url'] = base_url\n\n        # English: Set model name for model constructor\n        # \u65e5\u672c\u8a9e: \u30e2\u30c7\u30eb\u30b3\u30f3\u30b9\u30c8\u30e9\u30af\u30bf\u306b\u30e2\u30c7\u30eb\u540d\u3092\u8a2d\u5b9a\n        model_args['model'] = model if model else \"gpt-4o-mini\" # Default to gpt-4o-mini\n\n        # English: Temperature is likely handled by the runner or set post-init,\n        # English: so remove it from constructor args.\n        # \u65e5\u672c\u8a9e: temperature \u306f\u30e9\u30f3\u30ca\u30fc\u306b\u3088\u3063\u3066\u51e6\u7406\u3055\u308c\u308b\u304b\u3001\u521d\u671f\u5316\u5f8c\u306b\u8a2d\u5b9a\u3055\u308c\u308b\u53ef\u80fd\u6027\u304c\u9ad8\u3044\u305f\u3081\u3001\n        # \u65e5\u672c\u8a9e: \u30b3\u30f3\u30b9\u30c8\u30e9\u30af\u30bf\u5f15\u6570\u304b\u3089\u524a\u9664\u3057\u307e\u3059\u3002\n        # model_args['temperature'] = temperature # Removed based on TypeError\n\n        # English: Add any other relevant kwargs passed in, EXCLUDING temperature\n        # \u65e5\u672c\u8a9e: \u6e21\u3055\u308c\u305f\u4ed6\u306e\u95a2\u9023\u3059\u308b kwargs \u3092\u8ffd\u52a0 (temperature \u3092\u9664\u304f)\n        # Example: max_tokens, etc. Filter out args meant for the client.\n        # \u4f8b: max_tokens \u306a\u3069\u3002\u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u5411\u3051\u306e\u5f15\u6570\u3092\u9664\u5916\u3057\u307e\u3059\u3002\n        for key, value in kwargs.items():\n            # English: Exclude client args, thinking, temperature, and tracing\n            # \u65e5\u672c\u8a9e: \u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u5f15\u6570\u3001thinking\u3001temperature\u3001tracing \u3092\u9664\u5916\n            if key not in ['api_key', 'base_url', 'thinking', 'temperature', 'tracing']:\n                model_args[key] = value\n\n        # English: Remove 'thinking' as it's not used by OpenAI model\n        # \u65e5\u672c\u8a9e: OpenAI \u30e2\u30c7\u30eb\u3067\u306f\u4f7f\u7528\u3055\u308c\u306a\u3044\u305f\u3081 'thinking' \u3092\u524a\u9664\n        model_args.pop('thinking', None)\n\n        # English: Instantiate the OpenAI client\n        # \u65e5\u672c\u8a9e: OpenAI \u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u3092\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u5316\n        openai_client = AsyncOpenAI(**client_args)\n\n        # English: Instantiate and return the model, passing the client and model args\n        # \u65e5\u672c\u8a9e: \u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u3068\u30e2\u30c7\u30eb\u5f15\u6570\u3092\u6e21\u3057\u3066\u30e2\u30c7\u30eb\u3092\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u5316\u3057\u3066\u8fd4\u3059\n        return OpenAIResponsesModel(\n            openai_client=openai_client,\n            **model_args\n        )\n    elif provider == \"google\":\n        gemini_kwargs = kwargs.copy()\n        if model:\n            gemini_kwargs['model'] = model\n        # thinking is not used by GeminiModel\n        gemini_kwargs.pop('thinking', None)\n        return GeminiModel(\n            temperature=temperature,\n            api_key=api_key,\n            base_url=base_url, # Although Gemini doesn't typically use base_url, pass it if provided\n            **gemini_kwargs\n        )\n    elif provider == \"anthropic\":\n        claude_kwargs = kwargs.copy()\n        if model:\n            claude_kwargs['model'] = model\n        return ClaudeModel(\n            temperature=temperature,\n            api_key=api_key,\n            base_url=base_url, # Although Claude doesn't typically use base_url, pass it if provided\n            thinking=thinking,\n            **claude_kwargs\n        )\n    elif provider == \"ollama\":\n        ollama_kwargs = kwargs.copy()\n        if model:\n            ollama_kwargs['model'] = model\n        if not base_url:\n            base_url = \"http://localhost:11434\"\n        # thinking is not used by OllamaModel\n        ollama_kwargs.pop('thinking', None)\n        return OllamaModel(\n            temperature=temperature,\n            base_url=base_url,\n            api_key=api_key, # Although Ollama doesn't typically use api_key, pass it if provided\n            **ollama_kwargs\n        )\n    else:\n        raise ValueError(f\"Unsupported provider: {provider}. Must be one of {ProviderType.__args__}\") \n</code></pre>"},{"location":"api_reference/#_1","title":"\u30af\u30e9\u30b9\u30fb\u95a2\u6570\u4e00\u89a7","text":"\u540d\u524d \u7a2e\u5225 \u6982\u8981 get_llm \u95a2\u6570 \u30e2\u30c7\u30eb\u540d\u30fb\u30d7\u30ed\u30d0\u30a4\u30c0\u30fc\u540d\u304b\u3089LLM\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u3092\u53d6\u5f97 AgentPipeline \u30af\u30e9\u30b9 \u751f\u6210\u30fb\u8a55\u4fa1\u30fb\u30c4\u30fc\u30eb\u30fb\u30ac\u30fc\u30c9\u30ec\u30fc\u30eb\u3092\u7d71\u5408\u3057\u305f\u30d1\u30a4\u30d7\u30e9\u30a4\u30f3 ConsoleTracingProcessor \u30af\u30e9\u30b9 \u30b3\u30f3\u30bd\u30fc\u30eb\u8272\u5206\u3051\u30c8\u30ec\u30fc\u30b9\u51fa\u529b\u7528\u30d7\u30ed\u30bb\u30c3\u30b5 enable_console_tracing \u95a2\u6570 \u30b3\u30f3\u30bd\u30fc\u30eb\u30c8\u30ec\u30fc\u30b7\u30f3\u30b0\u3092\u6709\u52b9\u5316 disable_tracing \u95a2\u6570 \u30c8\u30ec\u30fc\u30b7\u30f3\u30b0\u6a5f\u80fd\u3092\u3059\u3079\u3066\u7121\u52b9\u5316 OpenAIResponsesModel \u30af\u30e9\u30b9 OpenAI\u7528\u30e2\u30c7\u30eb\u30e9\u30c3\u30d1\u30fc GeminiModel \u30af\u30e9\u30b9 Google Gemini\u7528\u30e2\u30c7\u30eb\u30e9\u30c3\u30d1\u30fc ClaudeModel \u30af\u30e9\u30b9 Anthropic Claude\u7528\u30e2\u30c7\u30eb\u30e9\u30c3\u30d1\u30fc OllamaModel \u30af\u30e9\u30b9 Ollama\u7528\u30e2\u30c7\u30eb\u30e9\u30c3\u30d1\u30fc"},{"location":"api_reference/#get_llm","title":"get_llm","text":"<ul> <li> <p>\u30e2\u30c7\u30eb\u540d\u30fb\u30d7\u30ed\u30d0\u30a4\u30c0\u30fc\u540d\u304b\u3089LLM\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u3092\u8fd4\u3059\u30d5\u30a1\u30af\u30c8\u30ea\u95a2\u6570</p> </li> <li> <p>\u5f15\u6570:</p> <ul> <li>model (str): \u30e2\u30c7\u30eb\u540d</li> <li>provider (str, optional): \u30d7\u30ed\u30d0\u30a4\u30c0\u30fc\u540d\uff08\u7701\u7565\u6642\u306f\u81ea\u52d5\u63a8\u8ad6\uff09</li> <li>temperature (float, optional): \u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u6e29\u5ea6</li> <li>api_key (str, optional): \u30d7\u30ed\u30d0\u30a4\u30c0\u30fcAPI\u30ad\u30fc</li> <li>base_url (str, optional): \u30d7\u30ed\u30d0\u30a4\u30c0\u30fcAPI\u30d9\u30fc\u30b9URL</li> <li>thinking (bool, optional): Claude\u30e2\u30c7\u30eb\u601d\u8003\u30e2\u30fc\u30c9</li> <li>tracing (bool, optional): Agents SDK \u30c8\u30ec\u30fc\u30b7\u30f3\u30b0\u6709\u52b9\u5316</li> </ul> </li> <li>\u623b\u308a\u5024: LLM\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9</li> </ul>"},{"location":"api_reference/#_2","title":"\u5f15\u6570","text":"\u540d\u524d \u578b \u5fc5\u9808/\u30aa\u30d7\u30b7\u30e7\u30f3 \u30c7\u30d5\u30a9\u30eb\u30c8 \u8aac\u660e model str \u5fc5\u9808 - \u4f7f\u7528\u3059\u308bLLM\u30e2\u30c7\u30eb\u540d provider str (optional) \u30aa\u30d7\u30b7\u30e7\u30f3 None \u30e2\u30c7\u30eb\u306e\u30d7\u30ed\u30d0\u30a4\u30c0\u30fc\u540d\uff08\u81ea\u52d5\u63a8\u8ad6\u53ef\uff09 temperature float (optional) \u30aa\u30d7\u30b7\u30e7\u30f3 0.3 \u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u6e29\u5ea6 api_key str (optional) \u30aa\u30d7\u30b7\u30e7\u30f3 None \u30d7\u30ed\u30d0\u30a4\u30c0\u30fcAPI\u30ad\u30fc base_url str (optional) \u30aa\u30d7\u30b7\u30e7\u30f3 None \u30d7\u30ed\u30d0\u30a4\u30c0\u30fcAPI\u30d9\u30fc\u30b9URL thinking bool (optional) \u30aa\u30d7\u30b7\u30e7\u30f3 False Claude\u30e2\u30c7\u30eb\u306e\u601d\u8003\u30e2\u30fc\u30c9 tracing bool (optional) \u30aa\u30d7\u30b7\u30e7\u30f3 False Agents SDK\u306e\u30c8\u30ec\u30fc\u30b7\u30f3\u30b0\u3092\u6709\u52b9\u5316\u3059\u308b\u304b"},{"location":"api_reference/#_3","title":"\u623b\u308a\u5024","text":"<p><code>LLM\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9</code></p>"},{"location":"api_reference/#enable_console_tracing","title":"enable_console_tracing","text":"<ul> <li>\u30b3\u30f3\u30bd\u30fc\u30eb\u30c8\u30ec\u30fc\u30b7\u30f3\u30b0\uff08<code>ConsoleTracingProcessor</code>\uff09\u3092\u6709\u52b9\u5316\u3057\u307e\u3059\u3002</li> <li>\u5f15\u6570: \u306a\u3057</li> <li>\u623b\u308a\u5024: \u306a\u3057</li> </ul>"},{"location":"api_reference/#disable_tracing","title":"disable_tracing","text":"<ul> <li>\u5168\u3066\u306e\u30c8\u30ec\u30fc\u30b7\u30f3\u30b0\u6a5f\u80fd\uff08SDK\u304a\u3088\u3073\u30b3\u30f3\u30bd\u30fc\u30eb\uff09\u3092\u7121\u52b9\u5316\u3057\u307e\u3059\u3002</li> <li>\u5f15\u6570: \u306a\u3057</li> <li>\u623b\u308a\u5024: \u306a\u3057</li> </ul>"},{"location":"api_reference/#agentpipeline","title":"AgentPipeline","text":"<ul> <li>\u751f\u6210\u30fb\u8a55\u4fa1\u30fb\u30c4\u30fc\u30eb\u30fb\u30ac\u30fc\u30c9\u30ec\u30fc\u30eb\u3092\u7d71\u5408\u3057\u305f\u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u7ba1\u7406\u30af\u30e9\u30b9</li> <li>\u4e3b\u306a\u5f15\u6570:<ul> <li>name (str): \u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u540d</li> <li>generation_instructions (str): \u751f\u6210\u7528\u30d7\u30ed\u30f3\u30d7\u30c8</li> <li>evaluation_instructions (str, optional): \u8a55\u4fa1\u7528\u30d7\u30ed\u30f3\u30d7\u30c8</li> <li>model (str or LLM): \u4f7f\u7528\u3059\u308b\u30e2\u30c7\u30eb</li> <li>evaluation_model (str or LLM, optional): \u8a55\u4fa1\u306b\u4f7f\u7528\u3059\u308b\u30e2\u30c7\u30eb\u540d\u307e\u305f\u306fLLM\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\uff08\u7701\u7565\u6642\u306f<code>model</code>\u3092\u4f7f\u7528\uff09</li> <li>generation_tools (list, optional): \u751f\u6210\u6642\u30c4\u30fc\u30eb</li> <li>input_guardrails/output_guardrails (list, optional): \u5165\u51fa\u529b\u30ac\u30fc\u30c9\u30ec\u30fc\u30eb</li> <li>threshold (int): \u8a55\u4fa1\u95be\u5024</li> <li>retries (int): \u30ea\u30c8\u30e9\u30a4\u56de\u6570</li> <li>retry_comment_importance (list[str], optional): \u91cd\u8981\u5ea6\u6307\u5b9a</li> </ul> </li> <li>\u4e3b\u306a\u30e1\u30bd\u30c3\u30c9:<ul> <li>run(input): \u5165\u529b\u306b\u5bfe\u3057\u3066\u751f\u6210\u30fb\u8a55\u4fa1\u30fb\u81ea\u5df1\u6539\u5584\u3092\u5b9f\u884c</li> </ul> </li> <li>\u623b\u308a\u5024: \u751f\u6210\u30fb\u8a55\u4fa1\u7d50\u679c</li> </ul>"},{"location":"api_reference/#_4","title":"\u5f15\u6570","text":"\u540d\u524d \u578b \u5fc5\u9808/\u30aa\u30d7\u30b7\u30e7\u30f3 \u30c7\u30d5\u30a9\u30eb\u30c8 \u8aac\u660e name str \u5fc5\u9808 - \u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u540d generation_instructions str \u5fc5\u9808 - \u751f\u6210\u7528\u30b7\u30b9\u30c6\u30e0\u30d7\u30ed\u30f3\u30d7\u30c8 evaluation_instructions str (optional) \u30aa\u30d7\u30b7\u30e7\u30f3 None \u8a55\u4fa1\u7528\u30b7\u30b9\u30c6\u30e0\u30d7\u30ed\u30f3\u30d7\u30c8 model str or LLM \u30aa\u30d7\u30b7\u30e7\u30f3 None \u4f7f\u7528\u3059\u308bLLM\u30e2\u30c7\u30eb\u540d\u307e\u305f\u306fLLM\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9 evaluation_model str or LLM \u30aa\u30d7\u30b7\u30e7\u30f3 None \u8a55\u4fa1\u306b\u4f7f\u7528\u3059\u308b\u30e2\u30c7\u30eb\u540d\u307e\u305f\u306fLLM\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\uff08\u7701\u7565\u6642\u306fmodel\u3092\u4f7f\u7528\uff09 generation_tools list (optional) \u30aa\u30d7\u30b7\u30e7\u30f3 [] \u751f\u6210\u6642\u306b\u4f7f\u7528\u3059\u308b\u30c4\u30fc\u30eb\u306e\u30ea\u30b9\u30c8 evaluation_tools list (optional) \u30aa\u30d7\u30b7\u30e7\u30f3 [] \u8a55\u4fa1\u6642\u306b\u4f7f\u7528\u3059\u308b\u30c4\u30fc\u30eb\u306e\u30ea\u30b9\u30c8 input_guardrails list (optional) \u30aa\u30d7\u30b7\u30e7\u30f3 [] \u751f\u6210\u6642\u306e\u5165\u529b\u30ac\u30fc\u30c9\u30ec\u30fc\u30eb\u30ea\u30b9\u30c8 output_guardrails list (optional) \u30aa\u30d7\u30b7\u30e7\u30f3 [] \u8a55\u4fa1\u6642\u306e\u51fa\u529b\u30ac\u30fc\u30c9\u30ec\u30fc\u30eb\u30ea\u30b9\u30c8 routing_func Callable (optional) \u30aa\u30d7\u30b7\u30e7\u30f3 None \u51fa\u529b\u30eb\u30fc\u30c6\u30a3\u30f3\u30b0\u7528\u95a2\u6570 session_history list (optional) \u30aa\u30d7\u30b7\u30e7\u30f3 [] \u30bb\u30c3\u30b7\u30e7\u30f3\u5c65\u6b74 history_size int \u30aa\u30d7\u30b7\u30e7\u30f3 10 \u5c65\u6b74\u4fdd\u6301\u6570 threshold int \u30aa\u30d7\u30b7\u30e7\u30f3 85 \u8a55\u4fa1\u30b9\u30b3\u30a2\u306e\u95be\u5024 retries int \u30aa\u30d7\u30b7\u30e7\u30f3 3 \u30ea\u30c8\u30e9\u30a4\u8a66\u884c\u56de\u6570 improvement_callback Callable[[Any, EvaluationResult], None] (optional) \u30aa\u30d7\u30b7\u30e7\u30f3 None \u6539\u5584\u63d0\u6848\u7528\u30b3\u30fc\u30eb\u30d0\u30c3\u30af dynamic_prompt Callable[[str], str] (optional) \u30aa\u30d7\u30b7\u30e7\u30f3 None \u52d5\u7684\u30d7\u30ed\u30f3\u30d7\u30c8\u751f\u6210\u95a2\u6570 retry_comment_importance list[str] (optional) \u30aa\u30d7\u30b7\u30e7\u30f3 [] \u30ea\u30c8\u30e9\u30a4\u6642\u306b\u30d7\u30ed\u30f3\u30d7\u30c8\u306b\u542b\u3081\u308b\u30b3\u30e1\u30f3\u30c8\u91cd\u5927\u5ea6"},{"location":"api_reference/#_5","title":"\u623b\u308a\u5024","text":"<p><code>\u751f\u6210\u30fb\u8a55\u4fa1\u7d50\u679c\uff08</code>EvaluationResult<code>\u3092\u542b\u3080\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\uff09</code></p>"},{"location":"api_reference/#_6","title":"\u30e2\u30c7\u30eb\u30e9\u30c3\u30d1\u30fc\u30af\u30e9\u30b9","text":"\u30af\u30e9\u30b9\u540d \u6982\u8981 OpenAIResponsesModel OpenAI API\u7528 GeminiModel Google Gemini API\u7528 ClaudeModel Anthropic Claude API\u7528 OllamaModel Ollama API\u7528"},{"location":"api_reference/#mermaid","title":"\u30af\u30e9\u30b9\u56f3\uff08mermaid\uff09","text":"<pre><code>classDiagram\n    class AgentPipeline {\n        +run(input)\n        -_build_generation_prompt()\n        -_build_evaluation_prompt()\n    }\n    class OpenAIResponsesModel\n    class GeminiModel\n    class ClaudeModel\n    class OllamaModel\n\n    AgentPipeline --&gt; OpenAIResponsesModel\n    AgentPipeline --&gt; GeminiModel\n    AgentPipeline --&gt; ClaudeModel\n    AgentPipeline --&gt; OllamaModel</code></pre>"},{"location":"architecture/","title":"\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u8a2d\u8a08\u66f8","text":""},{"location":"architecture/#_2","title":"\u30b7\u30b9\u30c6\u30e0\u69cb\u6210\u30fb\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u306e\u6982\u8981","text":"<p>\u672c\u30b7\u30b9\u30c6\u30e0\u306f\u30ec\u30a4\u30e4\u30fc\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u3092\u63a1\u7528\u3057\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u69cb\u6210\u3068\u306a\u3063\u3066\u3044\u307e\u3059\u3002</p> <ul> <li>UI/\u5229\u7528\u4f8b\u5c64\uff08examples/\uff09</li> <li>\u30e6\u30fc\u30b9\u30b1\u30fc\u30b9\u5c64\uff08AgentPipeline\u30af\u30e9\u30b9\uff09</li> <li>\u6a5f\u80fd\u30af\u30e9\u30b9\u5c64\uff08\u30c4\u30fc\u30eb\u95a2\u6570\u3001\u30ac\u30fc\u30c9\u30ec\u30fc\u30eb\u95a2\u6570\u3001\u8a55\u4fa1\u95a2\u6570\u306a\u3069\uff09</li> <li>\u30c7\u30fc\u30bf\u30af\u30e9\u30b9\u5c64\uff08pydantic\u30e2\u30c7\u30eb\u3001dataclass\u7b49\uff09</li> <li>\u30b2\u30fc\u30c8\u30a6\u30a7\u30a4\u5c64\uff08get_llm\u7b49\u306e\u30e2\u30c7\u30eb\u53d6\u5f97\uff09</li> <li>\u30e6\u30fc\u30c6\u30a3\u30ea\u30c6\u30a3\u5c64\uff08\u8a2d\u5b9a\u3001\u30ed\u30b0\u7b49\uff09</li> </ul>"},{"location":"architecture/#_3","title":"\u4e3b\u8981\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30fc\u30b9","text":"\u30af\u30e9\u30b9\u540d \u5f79\u5272 \u30ec\u30a4\u30e4\u30fc AgentPipeline \u30ef\u30fc\u30af\u30d5\u30ed\u30fc\u7d71\u5408\u30fb\u5b9f\u884c \u30e6\u30fc\u30b9\u30b1\u30fc\u30b9 Agent LLM\u30a8\u30fc\u30b8\u30a7\u30f3\u30c8 \u6a5f\u80fd/\u30e6\u30fc\u30b9\u30b1\u30fc\u30b9 function_tool\u3067\u5b9a\u7fa9\u3057\u305f\u95a2\u6570 \u30c4\u30fc\u30eb\u3068\u3057\u3066\u5229\u7528 \u6a5f\u80fd input_guardrail\u3067\u5b9a\u7fa9\u3057\u305f\u95a2\u6570 \u5165\u529b\u30ac\u30fc\u30c9\u30ec\u30fc\u30eb \u6a5f\u80fd get_llm \u30e2\u30c7\u30eb\u53d6\u5f97 \u30b2\u30fc\u30c8\u30a6\u30a7\u30a4 pydantic.BaseModel \u69cb\u9020\u5316\u51fa\u529b \u30c7\u30fc\u30bf"},{"location":"architecture/#_4","title":"\u4e3b\u8981\u30c7\u30fc\u30bf\uff08\u30c7\u30fc\u30bf\u306e\u7a2e\u985e\u3001\u69cb\u9020\uff09","text":"\u30af\u30e9\u30b9\u540d \u4fdd\u6301\u30c7\u30fc\u30bf EvaluationResult score, comment MathHomeworkOutput is_math_homework, reasoning"},{"location":"architecture/#erplantuml","title":"ER\u56f3\uff08PlantUML\uff09","text":"<pre><code>@startuml\nclass AgentPipeline {\n  - name\n  - generation_instructions\n  - evaluation_instructions\n  - ...\n}\nclass Agent {\n  - name\n  - model\n  - instructions\n  - ...\n}\nclass EvaluationResult {\n  score: int\n  comment: List~str~\n}\nclass MathHomeworkOutput {\n  is_math_homework: bool\n  reasoning: str\n}\nAgentPipeline --&gt; Agent\nAgent --&gt; EvaluationResult\nAgent --&gt; MathHomeworkOutput\n@enduml\n</code></pre>"},{"location":"architecture/#_5","title":"\u53c2\u8003","text":"<ul> <li>\u8a73\u7d30\u306a\u4e8b\u4f8b\u30fb\u4f7f\u3044\u65b9\u306f docs/pipeline_examples.md \u3092\u53c2\u7167\u3002 </li> </ul>"},{"location":"concept/","title":"\u5bfe\u5fdc\u3059\u308b\u8ab2\u984c","text":"<p>OpenAI Agents SDK\u30e9\u30a4\u30d6\u30e9\u30ea\u3067\u306f\u30b5\u30dd\u30fc\u30c8\u3055\u308c\u3066\u3044\u308b\u306e\u306f\u3001OpenAI\u304c\u63d0\u4f9b\u3059\u308bLLM\u306e\u307f\u3067\u3042\u308b\u3002\u305d\u306e\u305f\u3081\u3001Ollama\u3084Anthropic,Google\u306a\u3069\u306e\u6709\u529b\u30e2\u30c7\u30eb\u3092\u4f7f\u7528\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u306a\u3044\u3002</p> <p>\u305d\u3053\u3067\u3001OpenAI Agents\u3067\u52d5\u4f5c\u3059\u308b\u5404LLM\u30d7\u30ed\u30d0\u30a4\u30c0\u30fc\u3092\u3055\u307d\u30fc\u3068\u3067\u304d\u308b\u3088\u3046\u306b\u3059\u308b\u3082\u306e\u3067\u3042\u308b\u3002</p>"},{"location":"concept/#_2","title":"\u5bfe\u5fdc\u30d7\u30ed\u30d0\u30a4\u30c0","text":"<ul> <li>Ollama</li> <li>Gemini</li> <li>Claude</li> </ul>"},{"location":"concept/#_3","title":"\u5bfe\u5fdc\u65b9\u6cd5","text":"<p>OpenAI Agents\u3067\u306f\u5404\u30c1\u30e3\u30c3\u30c8\u30e2\u30c7\u30eb\u306fModel\u30af\u30e9\u30b9\u3068ModelFactory\u30af\u30e9\u30b9\u3092\u63d0\u4f9b\u3057\u3066\u304a\u308a\u3001\u4e0b\u8a18\u3001URL\u306e\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30fc\u30b9\u306b\u306a\u3089\u3063\u305f\u30af\u30e9\u30b9\u3092\u63d0\u4f9b\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\u3002</p> <p>@https://github.com/openai/openai-agents-python/blob/main/src/agents/models/interface.py</p> <p>Model\u30af\u30e9\u30b9\u306f\u7279\u306bOpenAI\u306e\u30e2\u30c7\u30eb\u3068\u5165\u51fa\u529b\u3092\u63c3\u3048\u308b\u5fc5\u8981\u304c\u3042\u308a\u3001\u4e0b\u8a18\u306e\u5165\u51fa\u529b\u306b\u5b8c\u5168\u306b\u5408\u308f\u305b\u308b\u5fc5\u8981\u304c\u3042\u308b\u3002</p> <p>@https://github.com/openai/openai-agents-python/blob/main/src/agents/models/openai_chatcompletions.py</p>"},{"location":"example_stdout_tracing/","title":"Example stdout tracing","text":"<pre><code>from agents.tracing import TracingProcessor, add_trace_processor, set_tracing_disabled\nfrom agents.tracing.span_data import GenerationSpanData\n\nclass StdoutTracer(TracingProcessor):\n    # ---- trace \u30ec\u30d9\u30eb ----\n    def on_trace_start(self, trace): ...\n    def on_trace_end(self, trace): ...\n    # ---- span \u30ec\u30d9\u30eb ----\n    def on_span_start(self, span): ...\n    def on_span_end(self, span):\n        data = span.span_data\n        if isinstance(data, GenerationSpanData):\n            # messages \u306f list[dict(role, content)]\n            sys_msg   = next((m[\"content\"] for m in data.input if m[\"role\"]==\"system\"), \"\")\n            user_msg  = next((m[\"content\"] for m in data.input if m[\"role\"]==\"user\"), \"\")\n            assistant = \"\\n\".join(m[\"content\"] for m in data.output or [])\n            print(\"\\n=== Instruction ===\\n\", sys_msg)\n            print(\"=== Prompt ===\\n\", user_msg)\n            print(\"=== Output ===\\n\", assistant, \"\\n\")\n\n    def shutdown(self): ...            # \u5fc5\u8981\u306a\u3089 flush\n    def force_flush(self): ...\n\n# OpenAI \u3078\u306e\u30a8\u30af\u30b9\u30dd\u30fc\u30c8\u3092\u7121\u52b9\u5316\u3059\u308b\u5834\u5408\nset_tracing_disabled(True)             # \u3042\u308b\u3044\u306f set_trace_processors([StdoutTracer()])\n\n# Processor \u3092\u767b\u9332\nadd_trace_processor(StdoutTracer())\n\n# \u3042\u3068\u306f\u666e\u901a\u306b Agent \u3092\u5b9f\u884c\n# result = await Runner.run(agent, \"\u3053\u3093\u306b\u3061\u306f\")\n</code></pre>"},{"location":"function_spec/","title":"\u6a5f\u80fd\u4ed5\u69d8\u66f8","text":""},{"location":"function_spec/#1","title":"1. \u30b7\u30f3\u30d7\u30eb\u306a\u751f\u6210","text":"<ul> <li>\u30e6\u30fc\u30b9\u30b1\u30fc\u30b9\u624b\u9806</li> <li>\u30e6\u30fc\u30b6\u30fc\u304c\u5165\u529b\u6587\u3092\u4e0e\u3048\u308b</li> <li>AgentPipeline\u304c\u751f\u6210\u6307\u793a\u3092\u3082\u3068\u306bLLM\u3067\u751f\u6210</li> <li>\u7d50\u679c\u3092\u8fd4\u3059</li> <li>\u30e6\u30fc\u30b9\u30b1\u30fc\u30b9\u30d5\u30ed\u30fc\u56f3 <pre><code>@startuml\nactor User\nparticipant AgentPipeline\nparticipant Agent\nUser -&gt; AgentPipeline: \u5165\u529b\u6587\nAgentPipeline -&gt; Agent: \u751f\u6210\u6307\u793a+\u5165\u529b\nAgent -&gt; AgentPipeline: \u751f\u6210\u7d50\u679c\nAgentPipeline -&gt; User: \u7d50\u679c\u8fd4\u5374\n@enduml\n</code></pre></li> </ul>"},{"location":"function_spec/#2","title":"2. \u751f\u6210\u7269\u306e\u8a55\u4fa1\u4ed8\u304d\u751f\u6210","text":"<ul> <li>\u30e6\u30fc\u30b9\u30b1\u30fc\u30b9\u624b\u9806</li> <li>\u30e6\u30fc\u30b6\u30fc\u304c\u5165\u529b\u6587\u3092\u4e0e\u3048\u308b</li> <li>AgentPipeline\u304c\u751f\u6210\u6307\u793a\u3067\u751f\u6210</li> <li>AgentPipeline\u304c\u8a55\u4fa1\u6307\u793a\u3067\u8a55\u4fa1</li> <li>\u8a55\u4fa1\u30b9\u30b3\u30a2\u304c\u95be\u5024\u4ee5\u4e0a\u306a\u3089\u7d50\u679c\u8fd4\u5374\u3001\u672a\u6e80\u306a\u3089\u30ea\u30c8\u30e9\u30a4or\u5931\u6557</li> <li>\u30e6\u30fc\u30b9\u30b1\u30fc\u30b9\u30d5\u30ed\u30fc\u56f3 <pre><code>@startuml\nactor User\nparticipant AgentPipeline\nparticipant Agent as Generator\nparticipant Agent as Evaluator\nUser -&gt; AgentPipeline: \u5165\u529b\u6587\nAgentPipeline -&gt; Generator: \u751f\u6210\u6307\u793a+\u5165\u529b\nGenerator -&gt; AgentPipeline: \u751f\u6210\u7d50\u679c\nAgentPipeline -&gt; Evaluator: \u8a55\u4fa1\u6307\u793a+\u751f\u6210\u7d50\u679c\nEvaluator -&gt; AgentPipeline: \u8a55\u4fa1\u30b9\u30b3\u30a2\nalt \u30b9\u30b3\u30a2&gt;=\u95be\u5024\n  AgentPipeline -&gt; User: \u7d50\u679c\u8fd4\u5374\nelse \u30b9\u30b3\u30a2&lt;\u95be\u5024\n  AgentPipeline -&gt; User: \u5931\u6557\u901a\u77e5\nend\n@enduml\n</code></pre></li> </ul>"},{"location":"function_spec/#3","title":"3. \u30c4\u30fc\u30eb\u9023\u643a","text":"<ul> <li>\u30e6\u30fc\u30b9\u30b1\u30fc\u30b9\u624b\u9806</li> <li>\u30e6\u30fc\u30b6\u30fc\u304c\u5165\u529b\u6587\u3092\u4e0e\u3048\u308b</li> <li>AgentPipeline\u304c\u30c4\u30fc\u30eb\u4ed8\u304d\u3067\u751f\u6210</li> <li>\u5fc5\u8981\u306b\u5fdc\u3058\u3066\u30c4\u30fc\u30eb\u95a2\u6570\u304c\u547c\u3070\u308c\u308b</li> <li>\u7d50\u679c\u3092\u8fd4\u3059</li> <li>\u30e6\u30fc\u30b9\u30b1\u30fc\u30b9\u30d5\u30ed\u30fc\u56f3 <pre><code>@startuml\nactor User\nparticipant AgentPipeline\nparticipant Agent\nparticipant Tool\nUser -&gt; AgentPipeline: \u5165\u529b\u6587\nAgentPipeline -&gt; Agent: \u751f\u6210\u6307\u793a+\u5165\u529b+\u30c4\u30fc\u30eb\nAgent -&gt; Tool: \u30c4\u30fc\u30eb\u547c\u3073\u51fa\u3057\nTool -&gt; Agent: \u30c4\u30fc\u30eb\u7d50\u679c\nAgent -&gt; AgentPipeline: \u751f\u6210\u7d50\u679c\nAgentPipeline -&gt; User: \u7d50\u679c\u8fd4\u5374\n@enduml\n</code></pre></li> </ul>"},{"location":"function_spec/#4","title":"4. \u30ac\u30fc\u30c9\u30ec\u30fc\u30eb\uff08\u5165\u529b\u30ac\u30fc\u30c9\u30ec\u30fc\u30eb\uff09","text":"<ul> <li>\u30e6\u30fc\u30b9\u30b1\u30fc\u30b9\u624b\u9806</li> <li>\u30e6\u30fc\u30b6\u30fc\u304c\u5165\u529b\u6587\u3092\u4e0e\u3048\u308b</li> <li>AgentPipeline\u304c\u30ac\u30fc\u30c9\u30ec\u30fc\u30eb\u95a2\u6570\u3067\u5165\u529b\u691c\u67fb</li> <li>\u554f\u984c\u306a\u3051\u308c\u3070\u751f\u6210\u3001\u554f\u984c\u3042\u308c\u3070\u30d6\u30ed\u30c3\u30af</li> <li>\u30e6\u30fc\u30b9\u30b1\u30fc\u30b9\u30d5\u30ed\u30fc\u56f3 <pre><code>@startuml\nactor User\nparticipant AgentPipeline\nparticipant Guardrail\nparticipant Agent\nUser -&gt; AgentPipeline: \u5165\u529b\u6587\nAgentPipeline -&gt; Guardrail: \u5165\u529b\u691c\u67fb\nalt \u554f\u984c\u306a\u3057\n  AgentPipeline -&gt; Agent: \u751f\u6210\n  Agent -&gt; AgentPipeline: \u751f\u6210\u7d50\u679c\n  AgentPipeline -&gt; User: \u7d50\u679c\u8fd4\u5374\nelse \u554f\u984c\u3042\u308a\n  AgentPipeline -&gt; User: \u30d6\u30ed\u30c3\u30af\u901a\u77e5\nend\n@enduml\n</code></pre></li> </ul>"},{"location":"function_spec/#_2","title":"\u53c2\u8003","text":"<ul> <li>\u8a73\u7d30\u306a\u30b3\u30fc\u30c9\u4f8b\u306f docs/pipeline_examples.md \u3092\u53c2\u7167\u3002 </li> </ul>"},{"location":"pipeline_examples/","title":"AgentPipeline\u6d3b\u7528\u4e8b\u4f8b\u96c6","text":"<p>\u3053\u306e\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u3067\u306f\u3001<code>AgentPipeline</code>\u30af\u30e9\u30b9\u3092\u6d3b\u7528\u3057\u305f\u5404\u7a2e\u4e8b\u4f8b\u3092\u7d39\u4ecb\u3057\u307e\u3059\u3002</p>"},{"location":"pipeline_examples/#1","title":"1. \u30b7\u30f3\u30d7\u30eb\u306a\u751f\u6210\uff08\u8a55\u4fa1\u306a\u3057\uff09","text":"<ul> <li>\u30d5\u30a1\u30a4\u30eb: <code>examples/pipeline_simple_generation.py</code></li> <li>\u6982\u8981: \u30e6\u30fc\u30b6\u30fc\u5165\u529b\u306b\u57fa\u3065\u304d\u3001\u8a55\u4fa1\u306a\u3057\u3067\u76f4\u63a5\u751f\u6210\u7d50\u679c\u3092\u8fd4\u3059\u6700\u5c0f\u69cb\u6210\u306e\u4f8b\u3002</li> </ul>"},{"location":"pipeline_examples/#_1","title":"\u30b3\u30fc\u30c9\u4f8b","text":"<pre><code>pipeline = AgentPipeline(\n    name=\"simple_generator\",\n    generation_instructions=\"\"\"\n    You are a helpful assistant that generates creative stories.\n    \u3042\u306a\u305f\u306f\u5275\u9020\u7684\u306a\u7269\u8a9e\u3092\u751f\u6210\u3059\u308b\u5f79\u7acb\u3064\u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u3067\u3059\u3002\n    Please generate a short story based on the user's input.\n    \u30e6\u30fc\u30b6\u30fc\u306e\u5165\u529b\u306b\u57fa\u3065\u3044\u3066\u77ed\u3044\u7269\u8a9e\u3092\u751f\u6210\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n    \"\"\",\n    evaluation_instructions=None,\n    model=\"gpt-3.5-turbo\"\n)\nresult = pipeline.run(\"A story about a robot learning to paint\")\n</code></pre>"},{"location":"pipeline_examples/#2","title":"2. \u751f\u6210\u7269\u306e\u8a55\u4fa1\u4ed8\u304d\u751f\u6210","text":"<ul> <li>\u30d5\u30a1\u30a4\u30eb: <code>examples/pipeline_with_evaluation.py</code></li> <li>\u6982\u8981: \u751f\u6210\u7269\u306b\u5bfe\u3057\u3066\u81ea\u52d5\u8a55\u4fa1\u3092\u884c\u3044\u3001\u95be\u5024\u3092\u6e80\u305f\u3057\u305f\u5834\u5408\u306e\u307f\u7d50\u679c\u3092\u8fd4\u3059\u4f8b\u3002</li> </ul>"},{"location":"pipeline_examples/#_2","title":"\u30b3\u30fc\u30c9\u4f8b","text":"<pre><code>pipeline = AgentPipeline(\n    name=\"evaluated_generator\",\n    generation_instructions=\"...\",\n    evaluation_instructions=\"...\",\n    model=\"gpt-3.5-turbo\",\n    threshold=70\n)\nresult = pipeline.run(\"A story about a robot learning to paint\")\n</code></pre>"},{"location":"pipeline_examples/#3","title":"3. \u30c4\u30fc\u30eb\u9023\u643a\u306b\u3088\u308b\u751f\u6210","text":"<ul> <li>\u30d5\u30a1\u30a4\u30eb: <code>examples/pipeline_with_tools.py</code></li> <li>\u6982\u8981: <code>@function_tool</code>\u3067\u5b9a\u7fa9\u3057\u305fPython\u95a2\u6570\u3092\u30c4\u30fc\u30eb\u3068\u3057\u3066\u7d44\u307f\u8fbc\u307f\u3001\u5916\u90e8\u60c5\u5831\u53d6\u5f97\u3084\u8a08\u7b97\u306a\u3069\u3092\u7d44\u307f\u5408\u308f\u305b\u305f\u751f\u6210\u3092\u884c\u3046\u4f8b\u3002</li> </ul>"},{"location":"pipeline_examples/#_3","title":"\u30b3\u30fc\u30c9\u4f8b","text":"<pre><code>from agents import function_tool\n\n@function_tool\ndef search_web(query: str) -&gt; str:\n    ...\n\n@function_tool\ndef get_weather(location: str) -&gt; str:\n    ...\n\npipeline = AgentPipeline(\n    name=\"tooled_generator\",\n    generation_instructions=\"...\",\n    evaluation_instructions=None,\n    model=\"gpt-3.5-turbo\",\n    generation_tools=[search_web, get_weather]\n)\nresult = pipeline.run(\"What's the weather like in Tokyo?\")\n</code></pre>"},{"location":"pipeline_examples/#4","title":"4. \u30ac\u30fc\u30c9\u30ec\u30fc\u30eb\uff08\u5165\u529b\u30ac\u30fc\u30c9\u30ec\u30fc\u30eb\uff09\u306b\u3088\u308b\u5165\u529b\u5236\u5fa1","text":"<ul> <li>\u30d5\u30a1\u30a4\u30eb: <code>examples/pipeline_with_guardrails.py</code></li> <li>\u6982\u8981: \u5165\u529b\u5185\u5bb9\u306b\u5bfe\u3057\u3066\u30ac\u30fc\u30c9\u30ec\u30fc\u30eb\uff08\u4f8b: \u6570\u5b66\u306e\u5bbf\u984c\u4f9d\u983c\u306e\u691c\u51fa\uff09\u3092\u8a2d\u3051\u3001\u4e0d\u9069\u5207\u306a\u30ea\u30af\u30a8\u30b9\u30c8\u3092\u30d6\u30ed\u30c3\u30af\u3059\u308b\u4f8b\u3002</li> </ul>"},{"location":"pipeline_examples/#_4","title":"\u30b3\u30fc\u30c9\u4f8b","text":"<pre><code>from agents import input_guardrail, GuardrailFunctionOutput, InputGuardrailTripwireTriggered\n\n@input_guardrail\nasync def math_guardrail(ctx, agent, input):\n    ...\n\npipeline = AgentPipeline(\n    name=\"guardrail_pipeline\",\n    generation_instructions=\"...\",\n    evaluation_instructions=None,\n    model=\"gpt-4o\",\n    input_guardrails=[math_guardrail]\n)\n\ntry:\n    result = pipeline.run(\"Can you help me solve for x: 2x + 3 = 11?\")\nexcept InputGuardrailTripwireTriggered:\n    print(\"[Guardrail Triggered] Math homework detected. Request blocked.\")\n</code></pre>"},{"location":"pipeline_examples/#5","title":"5. \u30ea\u30c8\u30e9\u30a4\u6642\u306e\u30b3\u30e1\u30f3\u30c8\u30d5\u30a3\u30fc\u30c9\u30d0\u30c3\u30af","text":"<ul> <li>\u6a5f\u80fd: \u524d\u56de\u306e\u8a55\u4fa1\u30b3\u30e1\u30f3\u30c8\u3092\u6307\u5b9a\u3057\u305f\u91cd\u5927\u5ea6\u30ec\u30d9\u30eb\u3067\u751f\u6210\u30d7\u30ed\u30f3\u30d7\u30c8\u306b\u4ed8\u4e0e\u3057\u3001\u6539\u5584\u3092\u4fc3\u3059</li> <li>\u30d1\u30e9\u30e1\u30fc\u30bf:</li> <li><code>retry_comment_importance</code>: <code>serious</code>, <code>normal</code>, <code>minor</code> \u306e\u3044\u305a\u308c\u304b\u3092\u6307\u5b9a\u53ef\u80fd</li> </ul>"},{"location":"pipeline_examples/#_5","title":"\u30b3\u30fc\u30c9\u4f8b","text":"<pre><code>from agents_sdk_models.pipeline import AgentPipeline\n\npipeline = AgentPipeline(\n    name=\"comment_retry\",\n    generation_instructions=\"\u751f\u6210\u30d7\u30ed\u30f3\u30d7\u30c8\",\n    evaluation_instructions=\"\u8a55\u4fa1\u30d7\u30ed\u30f3\u30d7\u30c8\",\n    model=\"gpt-4o-mini\",\n    threshold=80,\n    retries=2,\n    retry_comment_importance=[\"serious\", \"normal\"]\n)\nresult = pipeline.run(\"\u8a55\u4fa1\u5bfe\u8c61\u306e\u30c6\u30ad\u30b9\u30c8\")\nprint(result)\n</code></pre>"},{"location":"pipeline_examples/#_6","title":"\u53c2\u8003","text":"<ul> <li>\u5404\u30b5\u30f3\u30d7\u30eb\u306f <code>examples/</code> \u30d5\u30a9\u30eb\u30c0\u306b\u683c\u7d0d\u3055\u308c\u3066\u3044\u307e\u3059\u3002</li> <li>\u8a73\u7d30\u306a\u4f7f\u3044\u65b9\u3084\u5fdc\u7528\u4f8b\u306fREADME\u3082\u53c2\u7167\u3057\u3066\u304f\u3060\u3055\u3044\u3002 </li> </ul>"},{"location":"requirements/","title":"\u8981\u4ef6\u5b9a\u7fa9\u66f8","text":""},{"location":"requirements/#_2","title":"\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306e\u76ee\u7684\u30fb\u80cc\u666f","text":"<p>OpenAI Agents SDK\u3092\u6d3b\u7528\u3057\u3001\u751f\u6210\u30fb\u8a55\u4fa1\u30fb\u30c4\u30fc\u30eb\u9023\u643a\u30fb\u30ac\u30fc\u30c9\u30ec\u30fc\u30eb\u306a\u3069\u306eAI\u30ef\u30fc\u30af\u30d5\u30ed\u30fc\u3092\u7c21\u5358\u306b\u69cb\u7bc9\u30fb\u62e1\u5f35\u3067\u304d\u308bPython\u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u63d0\u4f9b\u3059\u308b\u3002</p>"},{"location":"requirements/#_3","title":"\u5229\u7528\u8005\u3068\u56f0\u308a\u3054\u3068","text":"\u5229\u7528\u8005\u306e\u7a2e\u985e \u30b4\u30fc\u30eb \u5236\u7d04 \u56f0\u308a\u3054\u3068 AI\u958b\u767a\u8005 LLM\u3092\u6d3b\u7528\u3057\u305f\u591a\u69d8\u306a\u30ef\u30fc\u30af\u30d5\u30ed\u30fc\u3092\u7d20\u65e9\u304f\u69cb\u7bc9 Python, SDK\u4f9d\u5b58 \u30b5\u30f3\u30d7\u30eb\u3084\u8a2d\u8a08\u4f8b\u304c\u5c11\u306a\u3044\u3001\u30ac\u30fc\u30c9\u30ec\u30fc\u30eb\u3084\u30c4\u30fc\u30eb\u9023\u643a\u306e\u5b9f\u88c5\u304c\u7169\u96d1 \u7814\u7a76\u8005 \u8a55\u4fa1\u3084\u5b89\u5168\u6027\u3092\u62c5\u4fdd\u3057\u305fAI\u5b9f\u9a13 \u67d4\u8edf\u306a\u30ab\u30b9\u30bf\u30de\u30a4\u30ba \u8a55\u4fa1\u30fb\u5b89\u5168\u6027\u306e\u4ed5\u7d44\u307f\u304c\u4e0d\u8db3"},{"location":"requirements/#_4","title":"\u63a1\u7528\u3059\u308b\u6280\u8853\u30b9\u30bf\u30c3\u30af","text":"\u6280\u8853 \u30d0\u30fc\u30b8\u30e7\u30f3/\u5099\u8003 Python 3.10\u4ee5\u4e0a OpenAI Agents SDK \u6700\u65b0 pydantic 2.x"},{"location":"requirements/#_5","title":"\u6a5f\u80fd\uff08\u30e6\u30fc\u30b9\u30b1\u30fc\u30b9\uff09\u4e00\u89a7","text":"\u56f0\u308a\u3054\u3068 \u6a5f\u80fd\uff08\u30e6\u30fc\u30b9\u30b1\u30fc\u30b9\uff09 \u6a5f\u80fd\u306e\u30a4\u30e1\u30fc\u30b8 \u30b5\u30f3\u30d7\u30eb\u3084\u8a2d\u8a08\u4f8b\u304c\u5c11\u306a\u3044 AgentPipeline\u306b\u3088\u308b\u751f\u6210\u30fb\u8a55\u4fa1\u30fb\u30c4\u30fc\u30eb\u9023\u643a\u30fb\u30ac\u30fc\u30c9\u30ec\u30fc\u30eb\u306e\u7d71\u5408 \u30b3\u30fc\u30c9\u4f8b\u30fbexamples/pipeline_*.py \u8a55\u4fa1\u30fb\u5b89\u5168\u6027\u306e\u4ed5\u7d44\u307f\u304c\u4e0d\u8db3 \u5165\u529b\u30ac\u30fc\u30c9\u30ec\u30fc\u30eb\u30fb\u81ea\u52d5\u8a55\u4fa1 \u30ac\u30fc\u30c9\u30ec\u30fc\u30eb/\u8a55\u4fa1\u30c6\u30f3\u30d7\u30ec\u30fc\u30c8\u306e\u6d3b\u7528"},{"location":"requirements/#_6","title":"\u53c2\u8003","text":"<ul> <li>\u8a73\u7d30\u306a\u4e8b\u4f8b\u306f docs/pipeline_examples.md \u3092\u53c2\u7167\u3002 </li> </ul>"},{"location":"tracing/","title":"\u30c8\u30ec\u30fc\u30b7\u30f3\u30b0","text":""},{"location":"tracing/#openai-agents-sdk","title":"OpenAI Agents SDK \u306e\u30c8\u30ec\u30fc\u30b7\u30f3\u30b0\u6a5f\u80fd","text":"<p>OpenAI Agents SDK \u306f\u30c7\u30d5\u30a9\u30eb\u30c8\u3067 OpenAI \u306e\u30c8\u30ec\u30fc\u30b7\u30f3\u30b0\u30b5\u30fc\u30d3\u30b9\u3092\u5229\u7528\u3057\u3001\u5b9f\u884c\u4e2d\u306e\u30b9\u30d1\u30f3\u60c5\u5831\u3092\u81ea\u52d5\u7684\u306b\u53ce\u96c6\u30fb\u8a18\u9332\u3057\u307e\u3059\u3002 - \u30d7\u30ed\u30d0\u30a4\u30c0\u30fc\u306b\u95a2\u308f\u3089\u305a\u3001\u5185\u90e8\u3067\u306f OpenAI \u306e Trace API \u3092\u4f7f\u7528\u3057\u307e\u3059\u3002 - <code>OPENAI_API_KEY</code> \u306e\u8a2d\u5b9a\u304c\u5fc5\u8981\u3067\u3059\u3002</p>"},{"location":"tracing/#_2","title":"\u672c\u30e9\u30a4\u30d6\u30e9\u30ea\u3067\u306e\u30a2\u30d7\u30ed\u30fc\u30c1","text":"<ul> <li>\u30c7\u30d5\u30a9\u30eb\u30c8\u3067\u30b3\u30f3\u30bd\u30fc\u30eb\u5411\u3051\u306e <code>ConsoleTracingProcessor</code> \u3092\u63d0\u4f9b\u3057\u307e\u3059\u3002</li> <li>\u6709\u52b9\u5316: <code>enable_console_tracing()</code> \u3092\u547c\u3073\u51fa\u3059\u3068\u3001\u30b3\u30f3\u30bd\u30fc\u30eb\u306b\u30ab\u30e9\u30fc\u4ed8\u304d\u3067\u30b9\u30d1\u30f3\u60c5\u5831\u3092\u8868\u793a\u3057\u307e\u3059\u3002</li> <li>\u30c8\u30ec\u30fc\u30b7\u30f3\u30b0\u3092\u30ab\u30b9\u30bf\u30de\u30a4\u30ba\u3057\u305f\u3044\u5834\u5408\u306f\u3001\u30b5\u30fc\u30c9\u30d1\u30fc\u30c6\u30a3\u30fc\u304c\u63d0\u4f9b\u3059\u308b\u72ec\u81ea\u306e Processor \u3092\u4f7f\u7528\u3057\u3066\u304f\u3060\u3055\u3044\u3002</li> <li>\u30c8\u30ec\u30fc\u30b7\u30f3\u30b0\u3092\u505c\u6b62\u3057\u305f\u3044\u5834\u5408\u306f\u3001<code>disable_tracing()</code> \u3092\u547c\u3073\u51fa\u3057\u3066\u5168\u3066\u306e\u30c8\u30ec\u30fc\u30b7\u30f3\u30b0\u6a5f\u80fd\u3092\u7121\u52b9\u5316\u3057\u307e\u3059\u3002</li> </ul> <p>\u4ee5\u4e0a\u306e\u4ed5\u7d44\u307f\u306b\u3088\u308a\u3001\u30c7\u30d0\u30c3\u30b0\u6642\u3084\u30ed\u30fc\u30ab\u30eb\u958b\u767a\u74b0\u5883\u3067\u3082\u624b\u8efd\u306b\u30c8\u30ec\u30fc\u30b7\u30f3\u30b0\u60c5\u5831\u3092\u78ba\u8a8d\u3067\u304d\u3001\u5fc5\u8981\u306b\u5fdc\u3058\u3066\u30b5\u30fc\u30d3\u30b9\u9023\u643a\u3084\u30ed\u30b0\u51fa\u529b\u5148\u3092\u5207\u308a\u66ff\u3048\u3089\u308c\u307e\u3059\u3002 </p>"},{"location":"tutorials/advanced/","title":"\u5fdc\u7528\u4f8b","text":"<p>\u3053\u306e\u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb\u3067\u306f\u3001Agents SDK Models \u306e\u5fdc\u7528\u7684\u306a\u4f7f\u3044\u65b9\u3092\u7d39\u4ecb\u3057\u307e\u3059\u3002</p>"},{"location":"tutorials/advanced/#1","title":"1. \u30c4\u30fc\u30eb\u9023\u643a","text":"<pre><code>from agents import function_tool\nfrom agents_sdk_models import AgentPipeline\n\n@function_tool\ndef get_weather(location: str) -&gt; str:\n    return f\"Weather in {location}: Sunny, 25\u00b0C\"\n\npipeline = AgentPipeline(\n    name=\"tool_example\",\n    generation_instructions=\"\"\"\n    \u3042\u306a\u305f\u306f\u5929\u6c17\u60c5\u5831\u3092\u63d0\u4f9b\u3059\u308b\u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u3067\u3059\u3002\u5fc5\u8981\u306b\u5fdc\u3058\u3066get_weather\u30c4\u30fc\u30eb\u3092\u4f7f\u3063\u3066\u304f\u3060\u3055\u3044\u3002\n    \"\"\",\n    model=\"gpt-4o-mini\",\n    generation_tools=[get_weather]\n)\nresult = pipeline.run(\"\u6771\u4eac\u306e\u5929\u6c17\u306f\uff1f\")\nprint(result)\n</code></pre>"},{"location":"tutorials/advanced/#2","title":"2. \u30ac\u30fc\u30c9\u30ec\u30fc\u30eb\uff08\u5165\u529b\u5236\u5fa1\uff09","text":"<pre><code>from agents import input_guardrail, GuardrailFunctionOutput, Runner, RunContextWrapper, Agent\nfrom agents_sdk_models import AgentPipeline\nfrom pydantic import BaseModel\n\nclass MathCheck(BaseModel):\n    is_math: bool\n    reason: str\n\nguardrail_agent = Agent(\n    name=\"math_check\",\n    instructions=\"\u6570\u5b66\u306e\u5bbf\u984c\u304b\u5224\u5b9a\u3057\u3066\u304f\u3060\u3055\u3044\u3002\",\n    output_type=MathCheck\n)\n\n@input_guardrail\nasync def math_guardrail(ctx: RunContextWrapper, agent: Agent, input: str):\n    result = await Runner.run(guardrail_agent, input, context=ctx.context)\n    return GuardrailFunctionOutput(\n        output_info=result.final_output,\n        tripwire_triggered=result.final_output.is_math,\n    )\n\npipeline = AgentPipeline(\n    name=\"guardrail_example\",\n    generation_instructions=\"\u8cea\u554f\u306b\u7b54\u3048\u3066\u304f\u3060\u3055\u3044\u3002\",\n    model=\"gpt-4o-mini\",\n    input_guardrails=[math_guardrail]\n)\ntry:\n    result = pipeline.run(\"2x+3=11\u3092\u89e3\u3044\u3066\")\n    print(result)\nexcept Exception:\n    print(\"\u30ac\u30fc\u30c9\u30ec\u30fc\u30eb\u767a\u52d5: \u6570\u5b66\u306e\u5bbf\u984c\u4f9d\u983c\u3092\u691c\u51fa\")\n</code></pre>"},{"location":"tutorials/advanced/#3","title":"3. \u30c0\u30a4\u30ca\u30df\u30c3\u30af\u30d7\u30ed\u30f3\u30d7\u30c8","text":"<pre><code>def dynamic_prompt(user_input: str) -&gt; str:\n    return f\"[DYNAMIC] {user_input.upper()}\"\n\npipeline = AgentPipeline(\n    name=\"dynamic_example\",\n    generation_instructions=\"\u30ea\u30af\u30a8\u30b9\u30c8\u306b\u7b54\u3048\u3066\u304f\u3060\u3055\u3044\u3002\",\n    model=\"gpt-4o-mini\",\n    dynamic_prompt=dynamic_prompt\n)\nresult = pipeline.run(\"\u9762\u767d\u3044\u8a71\u3092\u3057\u3066\")\nprint(result)\n</code></pre>"},{"location":"tutorials/advanced/#4","title":"4. \u30ea\u30c8\u30e9\u30a4\uff06\u81ea\u5df1\u6539\u5584","text":"<pre><code>pipeline = AgentPipeline(\n    name=\"retry_example\",\n    generation_instructions=\"\u6587\u7ae0\u3092\u751f\u6210\u3057\u3066\u304f\u3060\u3055\u3044\u3002\",\n    evaluation_instructions=\"\u5206\u304b\u308a\u3084\u3059\u3055\u3067\u8a55\u4fa1\u3057\u3001\u30b3\u30e1\u30f3\u30c8\u3082\u8fd4\u3057\u3066\u304f\u3060\u3055\u3044\u3002\",\n    model=\"gpt-4o-mini\",\n    threshold=80,\n    retries=2,\n    retry_comment_importance=[\"serious\", \"normal\"]\n)\nresult = pipeline.run(\"AI\u306e\u6b74\u53f2\u3092\u6559\u3048\u3066\")\nprint(result)\n</code></pre>"},{"location":"tutorials/advanced/#_2","title":"\u30dd\u30a4\u30f3\u30c8","text":"<ul> <li>\u30c4\u30fc\u30eb\u3084\u30ac\u30fc\u30c9\u30ec\u30fc\u30eb\u3001\u52d5\u7684\u30d7\u30ed\u30f3\u30d7\u30c8\u3001\u81ea\u5df1\u6539\u5584\u3092\u67d4\u8edf\u306b\u7d44\u307f\u5408\u308f\u305b\u53ef\u80fd</li> <li>\u5b9f\u904b\u7528\u306b\u8fd1\u3044\u9ad8\u5ea6\u306a\u30ef\u30fc\u30af\u30d5\u30ed\u30fc\u3082\u30b7\u30f3\u30d7\u30eb\u306b\u8a18\u8ff0\u3067\u304d\u307e\u3059 </li> </ul>"},{"location":"tutorials/quickstart/","title":"\u30af\u30a4\u30c3\u30af\u30b9\u30bf\u30fc\u30c8","text":"<p>\u3053\u306e\u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb\u3067\u306f\u3001Agents SDK Models \u3092\u4f7f\u3063\u305f\u6700\u5c0f\u9650\u306eLLM\u6d3b\u7528\u4f8b\u3092\u7d39\u4ecb\u3057\u307e\u3059\u3002</p>"},{"location":"tutorials/quickstart/#1","title":"1. \u30e2\u30c7\u30eb\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u306e\u53d6\u5f97","text":"<pre><code>from agents_sdk_models import get_llm\nllm = get_llm(\"gpt-4o-mini\")\n</code></pre>"},{"location":"tutorials/quickstart/#2-agent","title":"2. Agent \u3067\u30b7\u30f3\u30d7\u30eb\u306a\u5bfe\u8a71","text":"<pre><code>from agents import Agent, Runner\nagent = Agent(\n    name=\"Assistant\",\n    model=llm,\n    instructions=\"\u3042\u306a\u305f\u306f\u89aa\u5207\u306a\u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u3067\u3059\u3002\"\n)\nresult = Runner.run_sync(agent, \"\u3053\u3093\u306b\u3061\u306f\uff01\")\nprint(result.final_output)\n</code></pre>"},{"location":"tutorials/quickstart/#3-agentpipeline","title":"3. AgentPipeline \u3067\u751f\u6210\uff0b\u8a55\u4fa1","text":"<pre><code>from agents_sdk_models import AgentPipeline\npipeline = AgentPipeline(\n    name=\"eval_example\",\n    generation_instructions=\"\"\"\n    \u3042\u306a\u305f\u306f\u5f79\u7acb\u3064\u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u3067\u3059\u3002\u30e6\u30fc\u30b6\u30fc\u306e\u8981\u671b\u306b\u5fdc\u3058\u3066\u6587\u7ae0\u3092\u751f\u6210\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n    \"\"\",\n    evaluation_instructions=\"\"\"\n    \u751f\u6210\u3055\u308c\u305f\u6587\u7ae0\u3092\u5206\u304b\u308a\u3084\u3059\u3055\u3067100\u70b9\u6e80\u70b9\u8a55\u4fa1\u3057\u3001\u30b3\u30e1\u30f3\u30c8\u3082\u4ed8\u3051\u3066\u304f\u3060\u3055\u3044\u3002\n    \"\"\",\n    model=\"gpt-4o-mini\",\n    threshold=70\n)\nresult = pipeline.run(\"AI\u306e\u6d3b\u7528\u4e8b\u4f8b\u3092\u6559\u3048\u3066\")\nprint(result)\n</code></pre>"},{"location":"tutorials/quickstart/#_2","title":"\u30dd\u30a4\u30f3\u30c8","text":"<ul> <li><code>get_llm</code> \u3067\u4e3b\u8981\u306aLLM\u3092\u7c21\u5358\u53d6\u5f97</li> <li><code>Agent</code> \u3067\u30b7\u30f3\u30d7\u30eb\u306a\u5bfe\u8a71</li> <li><code>AgentPipeline</code> \u3067\u751f\u6210\u30fb\u8a55\u4fa1\u30fb\u81ea\u5df1\u6539\u5584\u307e\u3067\u4e00\u6c17\u901a\u8cab</li> <li>\u3069\u3061\u3089\u3082\u6700\u5c0f\u9650\u306e\u8a18\u8ff0\u3067\u9ad8\u5ea6\u306a\u30ef\u30fc\u30af\u30d5\u30ed\u30fc\u304c\u5b9f\u73fe\u3067\u304d\u307e\u3059 </li> </ul>"}]}