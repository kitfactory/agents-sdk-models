{"config":{"lang":["ja"],"separator":"[\\s\\-\u3000\u3001\u3002\uff0c\uff0e]+","pipeline":["stemmer"]},"docs":[{"location":"","title":"Agents SDK Models \u3078\u3088\u3046\u3053\u305d","text":"<p>OpenAI Agents SDK \u3092\u62e1\u5f35\u3057\u3001\u8907\u6570\u306eLLM\u30d7\u30ed\u30d0\u30a4\u30c0\u30fc\u3092\u7d71\u4e00\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30fc\u30b9\u3067\u6271\u3048\u308b\u30e2\u30c7\u30eb\u30a2\u30c0\u30d7\u30bf\u30fc\uff06\u30ef\u30fc\u30af\u30d5\u30ed\u30fc\u62e1\u5f35\u96c6\u3067\u3059\u3002</p>"},{"location":"#_1","title":"\u4e3b\u306a\u7279\u5fb4","text":"<ul> <li>OpenAI, Gemini, Claude, Ollama \u306a\u3069\u4e3b\u8981LLM\u3092\u7c21\u5358\u5207\u66ff</li> <li>\ud83d\ude80 \u65b0\u6a5f\u80fd\uff1a <code>Flow(steps=gen_agent)</code> \u3067\u8d85\u30b7\u30f3\u30d7\u30eb\u306a\u30ef\u30fc\u30af\u30d5\u30ed\u30fc\u4f5c\u6210</li> <li>\ud83d\ude80 \u65b0\u6a5f\u80fd\uff1a <code>Flow(steps=[step1, step2])</code> \u3067\u81ea\u52d5\u30b7\u30fc\u30b1\u30f3\u30b7\u30e3\u30eb\u5b9f\u884c</li> <li>\u751f\u6210\u30fb\u8a55\u4fa1\u30fb\u30c4\u30fc\u30eb\u30fb\u30ac\u30fc\u30c9\u30ec\u30fc\u30eb\u30921\u3064\u306e\u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u3067\u7d71\u5408</li> <li>\u30e2\u30c7\u30eb\u540d\u3068\u30d7\u30ed\u30f3\u30d7\u30c8\u3060\u3051\u3067\u81ea\u5df1\u6539\u5584\u30b5\u30a4\u30af\u30eb\u3082\u5b9f\u73fe</li> <li>Pydantic\u306b\u3088\u308b\u69cb\u9020\u5316\u51fa\u529b\u5bfe\u5fdc</li> <li>Python 3.9+ / Windows, Linux, MacOS\u5bfe\u5fdc</li> </ul>"},{"location":"#_2","title":"\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb","text":""},{"location":"#pypi","title":"PyPI \u304b\u3089","text":"<pre><code>pip install agents-sdk-models\n</code></pre>"},{"location":"#uv","title":"uv \u3092\u4f7f\u3046\u5834\u5408","text":"<pre><code>uv pip install agents-sdk-models\n</code></pre>"},{"location":"#_3","title":"\u958b\u767a\u7528\uff08\u63a8\u5968\uff09","text":"<pre><code>git clone https://github.com/kitfactory/agents-sdk-models.git\ncd agents-sdk-models\npython -m venv .venv\n.venv\\Scripts\\activate  # Windows\nsource .venv/bin/activate  # Linux/Mac\nuv pip install -e .[dev]\n</code></pre>"},{"location":"#_4","title":"\u30b5\u30dd\u30fc\u30c8\u74b0\u5883","text":"<ul> <li>Python 3.9+</li> <li>OpenAI Agents SDK 0.0.9+</li> <li>Windows, Linux, MacOS </li> </ul>"},{"location":"#_5","title":"\u30c8\u30ec\u30fc\u30b7\u30f3\u30b0","text":"<p>\u672c\u30e9\u30a4\u30d6\u30e9\u30ea\u3067\u306f OpenAI Agents SDK \u306e\u30c8\u30ec\u30fc\u30b7\u30f3\u30b0\u6a5f\u80fd\u3092\u30b5\u30dd\u30fc\u30c8\u3057\u3066\u3044\u307e\u3059\u3002\u8a73\u7d30\u306f \u30c8\u30ec\u30fc\u30b7\u30f3\u30b0 \u3092\u53c2\u7167\u3057\u3066\u304f\u3060\u3055\u3044\u3002 </p>"},{"location":"README_ja/","title":"agents-sdk-models \u30c9\u30ad\u30e5\u30e1\u30f3\u30c8","text":""},{"location":"README_ja/#_1","title":"\ud83c\udf1f \u306f\u3058\u3081\u306b","text":"<p>\u3053\u306e\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306f\u3001OpenAI Agents SDK\u3092\u6d3b\u7528\u3057\u305f\u30a8\u30fc\u30b8\u30a7\u30f3\u30c8\u30fb\u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u306e\u69cb\u7bc9\u3092\u652f\u63f4\u3059\u308bPython\u30e9\u30a4\u30d6\u30e9\u30ea\u3067\u3059\u3002 \u751f\u6210\u30fb\u8a55\u4fa1\u30fb\u30c4\u30fc\u30eb\u9023\u643a\u30fb\u30ac\u30fc\u30c9\u30ec\u30fc\u30eb\u306a\u3069\u3001\u5b9f\u8df5\u7684\u306aAI\u30ef\u30fc\u30af\u30d5\u30ed\u30fc\u3092\u6700\u5c0f\u9650\u306e\u8a18\u8ff0\u3067\u5b9f\u73fe\u3067\u304d\u307e\u3059\u3002</p>"},{"location":"README_ja/#_2","title":"\ud83d\ude80 \u7279\u5fb4\u30fb\u30e1\u30ea\u30c3\u30c8","text":"<ul> <li>\ud83e\udde9 \u751f\u6210\u30fb\u8a55\u4fa1\u30fb\u30c4\u30fc\u30eb\u30fb\u30ac\u30fc\u30c9\u30ec\u30fc\u30eb\u3092\u67d4\u8edf\u306b\u7d44\u307f\u5408\u308f\u305b\u305f\u30ef\u30fc\u30af\u30d5\u30ed\u30fc\u3092\u7c21\u5358\u306b\u69cb\u7bc9</li> <li>\ud83d\udee0\ufe0f Python\u95a2\u6570\u3092\u305d\u306e\u307e\u307e\u30c4\u30fc\u30eb\u3068\u3057\u3066\u5229\u7528\u53ef\u80fd</li> <li>\ud83d\udee1\ufe0f \u30ac\u30fc\u30c9\u30ec\u30fc\u30eb\u3067\u5b89\u5168\u30fb\u5805\u7262\u306a\u5165\u529b/\u51fa\u529b\u5236\u5fa1</li> <li>\ud83d\udce6 \u8c4a\u5bcc\u306a\u30b5\u30f3\u30d7\u30eb\uff08<code>examples/</code>\uff09\u3067\u3059\u3050\u306b\u8a66\u305b\u308b</li> <li>\ud83d\ude80 \u6700\u5c0f\u9650\u306e\u8a18\u8ff0\u3067\u7d20\u65e9\u304f\u30d7\u30ed\u30c8\u30bf\u30a4\u30d4\u30f3\u30b0</li> </ul>"},{"location":"README_ja/#_3","title":"\u26a1 \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb","text":"<p><pre><code>pip install agents-sdk-models\n</code></pre> - OpenAI Agents SDK, pydantic 2.x \u306a\u3069\u304c\u5fc5\u8981\u3067\u3059\u3002\u8a73\u7d30\u306f\u516c\u5f0f\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u3082\u53c2\u7167\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p>"},{"location":"README_ja/#agentpipeline","title":"\ud83c\udfd7\ufe0f AgentPipeline\u30af\u30e9\u30b9\u306e\u4f7f\u3044\u65b9","text":"<p><code>AgentPipeline</code> \u30af\u30e9\u30b9\u306f\u3001\u751f\u6210\u6307\u793a\u30fb\u8a55\u4fa1\u6307\u793a\u30fb\u30c4\u30fc\u30eb\u30fb\u30ac\u30fc\u30c9\u30ec\u30fc\u30eb\u306a\u3069\u3092\u67d4\u8edf\u306b\u7d44\u307f\u5408\u308f\u305b\u3066\u3001LLM\u30a8\u30fc\u30b8\u30a7\u30f3\u30c8\u306e\u30ef\u30fc\u30af\u30d5\u30ed\u30fc\u3092\u7c21\u5358\u306b\u69cb\u7bc9\u3067\u304d\u307e\u3059\u3002</p>"},{"location":"README_ja/#_4","title":"\u57fa\u672c\u69cb\u6210","text":"<pre><code>from agents_sdk_models import AgentPipeline\n\npipeline = AgentPipeline(\n    name=\"my_pipeline\",\n    generation_instructions=\"...\",  # \u751f\u6210\u6307\u793a\n    evaluation_instructions=None,    # \u8a55\u4fa1\u4e0d\u8981\u306a\u3089None\n    model=\"gpt-3.5-turbo\"\n)\nresult = pipeline.run(\"\u30e6\u30fc\u30b6\u30fc\u5165\u529b\")\n</code></pre>"},{"location":"README_ja/#_5","title":"\u751f\u6210\u7269\u306e\u81ea\u52d5\u8a55\u4fa1","text":"<pre><code>pipeline = AgentPipeline(\n    name=\"evaluated_generator\",\n    generation_instructions=\"...\",\n    evaluation_instructions=\"...\",  # \u8a55\u4fa1\u6307\u793a\n    model=\"gpt-3.5-turbo\",\n    threshold=70\n)\nresult = pipeline.run(\"\u8a55\u4fa1\u5bfe\u8c61\u306e\u5165\u529b\")\n</code></pre>"},{"location":"README_ja/#_6","title":"\u30c4\u30fc\u30eb\u9023\u643a","text":"<pre><code>from agents import function_tool\n\n@function_tool\ndef search_web(query: str) -&gt; str:\n    ...\n\npipeline = AgentPipeline(\n    name=\"tooled_generator\",\n    generation_instructions=\"...\",\n    evaluation_instructions=None,\n    model=\"gpt-3.5-turbo\",\n    generation_tools=[search_web]\n)\n</code></pre>"},{"location":"README_ja/#_7","title":"\u30ac\u30fc\u30c9\u30ec\u30fc\u30eb\uff08\u5165\u529b\u5236\u5fa1\uff09","text":"<pre><code>from agents import input_guardrail, GuardrailFunctionOutput, InputGuardrailTripwireTriggered\n\n@input_guardrail\nasync def math_guardrail(ctx, agent, input):\n    ...\n\npipeline = AgentPipeline(\n    name=\"guardrail_pipeline\",\n    generation_instructions=\"...\",\n    evaluation_instructions=None,\n    model=\"gpt-4o\",\n    input_guardrails=[math_guardrail]\n)\n\ntry:\n    result = pipeline.run(\"Can you help me solve for x: 2x + 3 = 11?\")\nexcept InputGuardrailTripwireTriggered:\n    print(\"[Guardrail Triggered] Math homework detected. Request blocked.\")\n</code></pre>"},{"location":"README_ja/#_8","title":"\u30ea\u30c8\u30e9\u30a4\u6642\u306e\u30b3\u30e1\u30f3\u30c8\u30d5\u30a3\u30fc\u30c9\u30d0\u30c3\u30af","text":"<p><pre><code>from agents_sdk_models import AgentPipeline\n\npipeline = AgentPipeline(\n    name=\"comment_retry\",\n    generation_instructions=\"\u751f\u6210\u30d7\u30ed\u30f3\u30d7\u30c8\",\n    evaluation_instructions=\"\u8a55\u4fa1\u30d7\u30ed\u30f3\u30d7\u30c8\",\n    model=\"gpt-4o-mini\",\n    threshold=80,\n    retries=2,\n    retry_comment_importance=[\"serious\", \"normal\"]\n)\nresult = pipeline.run(\"\u5165\u529b\u30c6\u30ad\u30b9\u30c8\")\nprint(result)\n</code></pre> \u30ea\u30c8\u30e9\u30a4\u6642\u306b\u524d\u56de\u306e\u8a55\u4fa1\u30b3\u30e1\u30f3\u30c8\uff08\u6307\u5b9a\u3057\u305f\u91cd\u5927\u5ea6\u306e\u307f\uff09\u304c\u751f\u6210\u30d7\u30ed\u30f3\u30d7\u30c8\u306b\u81ea\u52d5\u3067\u4ed8\u4e0e\u3055\u308c\u3001\u6539\u5584\u3092\u4fc3\u3057\u307e\u3059\u3002</p>"},{"location":"README_ja/#flowv008","title":"\ud83d\ude80 \u65b0\u6a5f\u80fd\uff1a\u8d85\u30b7\u30f3\u30d7\u30ebFlow\uff08v0.0.8+\uff09","text":"<p>\u65b0\u3057\u3044Flow\u30b3\u30f3\u30b9\u30c8\u30e9\u30af\u30bf\u30673\u3064\u306e\u65b9\u6cd5\u3067\u30ef\u30fc\u30af\u30d5\u30ed\u30fc\u3092\u4f5c\u6210\u3067\u304d\u307e\u3059\uff1a</p>"},{"location":"README_ja/#flow","title":"\u5358\u4e00\u30b9\u30c6\u30c3\u30d7Flow\uff08\u6700\u3082\u30b7\u30f3\u30d7\u30eb\uff01\uff09","text":"<pre><code>from agents_sdk_models import create_simple_gen_agent, Flow\n\ngen_agent = create_simple_gen_agent(\"assistant\", \"\u89aa\u5207\u306b\u56de\u7b54\u3057\u307e\u3059\", \"gpt-4o-mini\")\nflow = Flow(steps=gen_agent)  # \u305f\u3063\u305f1\u884c\uff01\nresult = await flow.run(input_data=\"\u3053\u3093\u306b\u3061\u306f\")\n</code></pre>"},{"location":"README_ja/#flow_1","title":"\u30b7\u30fc\u30b1\u30f3\u30b7\u30e3\u30ebFlow\uff08\u81ea\u52d5\u63a5\u7d9a\uff01\uff09","text":"<pre><code>from agents_sdk_models import create_simple_gen_agent, Flow\n\nidea_gen = create_simple_gen_agent(\"idea\", \"\u30a2\u30a4\u30c7\u30a2\u751f\u6210\", \"gpt-4o-mini\")\nwriter = create_simple_gen_agent(\"writer\", \"\u8a18\u4e8b\u57f7\u7b46\", \"gpt-4o\")\nreviewer = create_simple_gen_agent(\"reviewer\", \"\u30ec\u30d3\u30e5\u30fc\", \"claude-3-5-sonnet-latest\")\n\nflow = Flow(steps=[idea_gen, writer, reviewer])  # \u81ea\u52d5\u30b7\u30fc\u30b1\u30f3\u30b7\u30e3\u30eb\u5b9f\u884c\uff01\nresult = await flow.run(input_data=\"AI\u6280\u8853\u306b\u3064\u3044\u3066\")\n</code></pre>"},{"location":"README_ja/#_9","title":"\u5f93\u6765\u65b9\u5f0f\uff08\u8907\u96d1\u306a\u30d5\u30ed\u30fc\u7528\uff09","text":"<pre><code>flow = Flow(\n    start=\"step1\",\n    steps={\"step1\": step1, \"step2\": step2}\n)\n</code></pre> <p>\ud83d\udcda \u8a73\u7d30\u30ac\u30a4\u30c9\uff1a \u65b0\u3057\u3044Flow\u6a5f\u80fd\u5b8c\u5168\u30ac\u30a4\u30c9</p>"},{"location":"README_ja/#_10","title":"\ud83d\udcda \u95a2\u9023\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8","text":"<ul> <li>\u65b0\u3057\u3044Flow\u6a5f\u80fd\u5b8c\u5168\u30ac\u30a4\u30c9 - v0.0.8\u3067\u8ffd\u52a0\u3055\u308c\u305f\u8d85\u30b7\u30f3\u30d7\u30eb\u306aFlow\u4f5c\u6210\u65b9\u6cd5</li> <li>\u30af\u30a4\u30c3\u30af\u30b9\u30bf\u30fc\u30c8 - 3\u884c\u3067AI\u30ef\u30fc\u30af\u30d5\u30ed\u30fc\u3092\u69cb\u7bc9</li> <li>\u5fdc\u7528\u4f8b - \u30de\u30eb\u30c1\u30a8\u30fc\u30b8\u30a7\u30f3\u30c8\u5354\u8abf\u3068\u30c4\u30fc\u30eb\u9023\u643a</li> <li>Flow/Step API \u30ea\u30d5\u30a1\u30ec\u30f3\u30b9 - \u8a73\u7d30\u306aAPI\u4ed5\u69d8</li> </ul>"},{"location":"analysis_of_library/","title":"Agents SDK Models: \u6bd4\u8f03\u5206\u6790\u3068\u5c06\u6765\u5c55\u671b","text":""},{"location":"analysis_of_library/#1-langchainlanggraph","title":"1. LangChain\u3084LangGraph\u306a\u3069\u306e\u4ed6\u30e9\u30a4\u30d6\u30e9\u30ea\u3068\u306e\u6bd4\u8f03\u306b\u3088\u308b\u5dee\u5225\u5316","text":"<p>\u4f4e\u3044\u5b66\u7fd2\u30b3\u30b9\u30c8\u3068\u30b7\u30f3\u30d7\u30eb\u3055: Agents SDK Models\u30e9\u30a4\u30d6\u30e9\u30ea\uff08OpenAI\u306eAgents SDK\u4e0a\u306b\u69cb\u7bc9\uff09\u306f\u3001\u6700\u5c0f\u9650\u306e\u62bd\u8c61\u5316\u3092\u91cd\u8996\u3057\u3066\u304a\u308a\u3001\u958b\u767a\u8005\u306b\u3068\u3063\u3066\u975e\u5e38\u306b\u89aa\u3057\u307f\u3084\u3059\u3044\u8a2d\u8a08\u3067\u3059\u3002LangChain\u306e\u5e83\u7bc4\u306a\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u3084LangGraph\u306e\u8907\u96d1\u306a\u30b0\u30e9\u30d5\u8a2d\u5b9a\u3068\u306f\u5bfe\u7167\u7684\u306b\u3001Agents SDK\u306f\u8efd\u91cf\u306aAPI\u8a2d\u8a08\u3092\u63a1\u3063\u3066\u3044\u307e\u3059\u3002\u305d\u306e\u305f\u3081\u3001ML\u30a8\u30f3\u30b8\u30cb\u30a2\u306f\u30a8\u30fc\u30b8\u30a7\u30f3\u30c8\u3068\u30c4\u30fc\u30eb\u3092\u6570\u884c\u306e\u30b3\u30fc\u30c9\u3067\u5b9a\u7fa9\u3059\u308b\u3060\u3051\u3067\u5229\u7528\u3092\u958b\u59cb\u3067\u304d\u307e\u3059\u3002LangGraph\u3067\u306f\u8907\u96d1\u306a\u72b6\u614b\u69cb\u6210\u3084\u30ce\u30fc\u30c9\u8a2d\u8a08\u304c\u6c42\u3081\u3089\u308c\u307e\u3059\u304c\u3001\u672c\u30e9\u30a4\u30d6\u30e9\u30ea\u3067\u306f\u3001get_llm\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30fc\u30b9\u3068AgentPipeline\u306b\u3088\u308a\u3001\u72b6\u614b\u9077\u79fb\u306e\u7ba1\u7406\u3092\u610f\u8b58\u305b\u305a\u306b\u6e08\u3080\u70b9\u304c\u5dee\u5225\u5316\u8981\u7d20\u3067\u3059\u3002</p> <p>\u3053\u306e\u30b7\u30f3\u30d7\u30eb\u3055\u3092\u3055\u3089\u306b\u5f37\u5316\u3059\u308b\u306b\u306f\u3001\u5178\u578b\u7684\u306a\u30a8\u30fc\u30b8\u30a7\u30f3\u30c8\u8a2d\u8a08\u30d1\u30bf\u30fc\u30f3\u306e\u30af\u30c3\u30af\u30d6\u30c3\u30af\u7684\u306a\u30c6\u30f3\u30d7\u30ec\u30fc\u30c8\u96c6\u3084\u3001LangChain\u306b\u3042\u308b\u4e00\u822c\u7684\u306a\u30e6\u30fc\u30b9\u30b1\u30fc\u30b9\uff08Q\\&amp;A\u30dc\u30c3\u30c8\u3001\u30c7\u30fc\u30bf\u62bd\u51fa\u306a\u3069\uff09\u306b\u5bfe\u5fdc\u3057\u305f\u7c21\u6f54\u306a\u4f8b\u3092\u7528\u610f\u3059\u308b\u3053\u3068\u3067\u3001\u521d\u5b66\u8005\u306e\u5c0e\u5165\u969c\u58c1\u3092\u3055\u3089\u306b\u4e0b\u3052\u3089\u308c\u307e\u3059\u3002</p> <p>\u6700\u5c0f\u9650\u306e\u4f9d\u5b58\u95a2\u4fc2\u3068\u8efd\u91cf\u8a2d\u8a08: \u672c\u30e9\u30a4\u30d6\u30e9\u30ea\u306e\u30b3\u30a2\u4f9d\u5b58\u306fOpenAI Python API\u304a\u3088\u3073Agents SDK\u306e\u307f\u3067\u3042\u308a\u3001\u5fc5\u8981\u306b\u5fdc\u3058\u3066Pydantic\u3092\u4f7f\u7528\u3059\u308b\u8a2d\u8a08\u3067\u3059\u3002LangChain\u3067\u306f\u6570\u591a\u304f\u306e\u4f9d\u5b58\u30d1\u30c3\u30b1\u30fc\u30b8\u3092\u5c0e\u5165\u3059\u308b\u305f\u3081\u3001\u4f9d\u5b58\u95a2\u4fc2\u306e\u885d\u7a81\u3084\u30d1\u30d5\u30a9\u30fc\u30de\u30f3\u30b9\u554f\u984c\u304c\u8d77\u304d\u3084\u3059\u3044\u3067\u3059\u304c\u3001Agents SDK Models\u306f\u305d\u308c\u3092\u907f\u3051\u3001\u5fc5\u8981\u306a\u6a5f\u80fd\u306b\u5fdc\u3058\u305f\u8ffd\u52a0\u3092extras\u3067\u63d0\u4f9b\u3057\u3066\u3044\u307e\u3059\u3002</p> <p>\u5c06\u6765\u7684\u306a\u6a5f\u80fd\u62e1\u5f35\u306b\u304a\u3044\u3066\u3082\u3053\u306e\u8a2d\u8a08\u601d\u60f3\u3092\u7dad\u6301\u3059\u308b\u306b\u306f\u3001\u65b0\u6a5f\u80fd\u3092\u30d7\u30e9\u30b0\u30a4\u30f3\u30e2\u30b8\u30e5\u30fc\u30eb\u3068\u3057\u3066\u5206\u96e2\u3057\u3001\u30b3\u30a2\u306f\u8efd\u91cf\u306a\u307e\u307e\u4fdd\u3061\u3001\u5fc5\u8981\u306b\u5fdc\u3058\u3066\u5c0e\u5165\u53ef\u80fd\u3068\u3059\u308b\u8a2d\u8a08\u304c\u671b\u307e\u3057\u3044\u3067\u3057\u3087\u3046\u3002</p> <p>\u751f\u6210\u54c1\u8cea\u306e\u4fdd\u8a3c: \u672c\u30e9\u30a4\u30d6\u30e9\u30ea\u306e\u6700\u5927\u306e\u5dee\u5225\u5316\u8981\u7d20\u306e\u4e00\u3064\u304c\u3001\u751f\u6210\u54c1\u8cea\u306e\u8a55\u4fa1\u30fb\u4fdd\u8a3c\u3092\u7d44\u307f\u8fbc\u307f\u3067\u63d0\u4f9b\u3057\u3066\u3044\u308b\u3053\u3068\u3067\u3059\u3002get_llm\u3067\u5f97\u3089\u308c\u308bLLM\u306f\u3001Pydantic\u30e2\u30c7\u30eb\u306b\u3088\u308a\u69cb\u9020\u5316\u51fa\u529b\u3092\u30d0\u30ea\u30c7\u30fc\u30c8\u3067\u304d\u307e\u3059\u3002\u307e\u305f\u3001\u5165\u529b\u30ac\u30fc\u30c9\u30ec\u30fc\u30eb\u3084\u51fa\u529b\u30ac\u30fc\u30c9\u30ec\u30fc\u30eb\u3092\u901a\u3058\u3066\u3001\u502b\u7406\u7684\u306a\u5224\u65ad\u3084\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u691c\u8a3c\u306a\u3069\u3082\u7c21\u5358\u306b\u9069\u7528\u3067\u304d\u307e\u3059\u3002LangChain\u306a\u3069\u3067\u306f\u3053\u308c\u3089\u3092\u81ea\u524d\u3067\u69cb\u6210\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u304c\u3001\u672c\u30e9\u30a4\u30d6\u30e9\u30ea\u3067\u306f\u8a2d\u5b9a\u9805\u76ee\u3067\u6307\u5b9a\u3059\u308b\u3060\u3051\u3067\u8a55\u4fa1\u30fb\u518d\u751f\u6210\u3092\u542b\u3080\u30eb\u30fc\u30d7\u51e6\u7406\u307e\u3067\u81ea\u52d5\u3067\u884c\u3048\u307e\u3059\u3002</p> <p>\u3053\u308c\u3092\u3055\u3089\u306b\u5f37\u5316\u3059\u308b\u306b\u306f\u3001\u8a55\u4fa1\u30d7\u30ed\u30f3\u30d7\u30c8\u3084\u30ac\u30fc\u30c9\u30ec\u30fc\u30eb\u306e\u30c6\u30f3\u30d7\u30ec\u30fc\u30c8\u5316\u3001\u5185\u5bb9\u5909\u63db\u578b\u30ac\u30fc\u30c9\u30ec\u30fc\u30eb\uff08\u4f8b\uff1a\u6a5f\u5bc6\u60c5\u5831\u3092\u81ea\u52d5\u3067\u30de\u30b9\u30ad\u30f3\u30b0\uff09\u306a\u3069\u306e\u62e1\u5f35\u304c\u6319\u3052\u3089\u308c\u307e\u3059\u3002</p> <p>\u6539\u5584\u30dd\u30a4\u30f3\u30c8\u306e\u8981\u7d04:</p> <ul> <li>\u30b7\u30f3\u30d7\u30eb\u306a\u5c0e\u5165\u652f\u63f4: \u3088\u304f\u3042\u308b\u30e6\u30fc\u30b9\u30b1\u30fc\u30b9\u306b\u5373\u3057\u305f\u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb\u3068\u30b3\u30fc\u30c9\u4f8b\u3092\u63d0\u4f9b</li> <li>\u6a5f\u80fd\u306e\u5206\u96e2\u3068\u8efd\u91cf\u6027\u7dad\u6301: \u30d7\u30e9\u30b0\u30a4\u30f3\u69cb\u9020\u306b\u3088\u308a\u5c0e\u5165\u81ea\u7531\u5ea6\u3092\u78ba\u4fdd</li> <li>\u51fa\u529b\u8a55\u4fa1\u30c6\u30f3\u30d7\u30ec\u30fc\u30c8\u306e\u6a19\u6e96\u5316: \u3088\u304f\u4f7f\u308f\u308c\u308b\u8a55\u4fa1\u6307\u6a19\uff08\u6b63\u78ba\u6027\u3001\u7c21\u6f54\u3055\u306a\u3069\uff09\u3092\u30c7\u30d5\u30a9\u30eb\u30c8\u63d0\u4f9b</li> <li>\u5b89\u5b9a\u6027\u3068\u30d0\u30fc\u30b8\u30e7\u30cb\u30f3\u30b0: \u983b\u7e41\u306a\u4ed5\u69d8\u5909\u66f4\u3092\u907f\u3051\u3001\u30bb\u30de\u30f3\u30c6\u30a3\u30c3\u30af\u30d0\u30fc\u30b8\u30e7\u30cb\u30f3\u30b0\u3092\u5c0e\u5165</li> </ul>"},{"location":"analysis_of_library/#2-agentpipelinedag","title":"2. AgentPipeline\u306e\u30eb\u30fc\u30c6\u30a3\u30f3\u30b0\u8a2d\u8a08\u3068DAG\u69cb\u6210\u306e\u6709\u52b9\u6027","text":"<p>AgentPipeline\u306f\u3001\u751f\u6210\u3001\u8a55\u4fa1\u3001\u30c4\u30fc\u30eb\u5b9f\u884c\u3001\u30ac\u30fc\u30c9\u30ec\u30fc\u30eb\u306a\u3069\u306e\u30b9\u30c6\u30c3\u30d7\u3092\u4e00\u8cab\u3057\u305f\u30ef\u30fc\u30af\u30d5\u30ed\u30fc\u3068\u3057\u3066\u7d71\u5408\u3059\u308b\u305f\u3081\u306e\u8efd\u91cf\u30aa\u30fc\u30b1\u30b9\u30c8\u30ec\u30fc\u30b7\u30e7\u30f3\u30a8\u30f3\u30b8\u30f3\u3067\u3059\u3002\u3053\u308c\u306b\u3088\u308a\u3001LLM\u547c\u3073\u51fa\u3057\u2192\u51fa\u529b\u8a55\u4fa1\u2192\u30d5\u30a3\u30fc\u30c9\u30d0\u30c3\u30af\u2192\u518d\u751f\u6210\u3068\u3044\u3063\u305f\u4e00\u9023\u306e\u51e6\u7406\u3092\u4e00\u3064\u306e\u95a2\u6570\u547c\u3073\u51fa\u3057\u306b\u307e\u3068\u3081\u308b\u3053\u3068\u304c\u53ef\u80fd\u3068\u306a\u308a\u307e\u3059\u3002</p> <p>LangChain\u306e\u3088\u3046\u306a\u30c1\u30a7\u30fc\u30f3\u9023\u643a\u3084LangGraph\u306e\u72b6\u614b\u6a5f\u68b0\u69cb\u7bc9\u3068\u6bd4\u8f03\u3057\u3001\u672c\u8a2d\u8a08\u306f\u30b3\u30fc\u30c9\u91cf\u30fb\u6982\u5ff5\u7406\u89e3\u306e\u4e21\u9762\u3067\u5727\u5012\u7684\u306b\u8ca0\u62c5\u304c\u8efd\u3044\u3067\u3059\u3002\u518d\u8a55\u4fa1\u30ed\u30b8\u30c3\u30af\u3082\u30d1\u30e9\u30e1\u30fc\u30bf\u6307\u5b9a\u3067\u81ea\u52d5\u5316\u3055\u308c\u3066\u304a\u308a\u3001retry\u30eb\u30fc\u30d7\u3092\u81ea\u4f5c\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u305b\u3093\u3002</p> <p>\u3053\u306e\u8a2d\u8a08\u306f\u3001\u958b\u767a\u8005\u304c\u81ea\u7136\u306b\u601d\u3044\u63cf\u304f\u554f\u984c\u89e3\u6c7a\u306e\u6d41\u308c\uff08\u4f8b\uff1a\u54c1\u8cea\u78ba\u8a8d\u5f8c\u306b\u518d\u751f\u6210\uff09\u3092\u305d\u306e\u307e\u307ePipeline\u5b9a\u7fa9\u3067\u8a18\u8ff0\u3067\u304d\u308b\u70b9\u304c\u512a\u308c\u3066\u3044\u307e\u3059\u3002\u30e6\u30fc\u30b6\u30fc\u306f\u30b9\u30c6\u30fc\u30b8\u3054\u3068\u306e\u4f4e\u30ec\u30d9\u30eb\u306a\u5236\u5fa1\u3084\u72b6\u614b\u9077\u79fb\u3092\u610f\u8b58\u305b\u305a\u306b\u6e08\u307f\u307e\u3059\u3002</p> <p>\u305f\u3060\u3057\u3001Pipeline\u5185\u90e8\u3067\u4f55\u304c\u8d77\u304d\u3066\u3044\u308b\u304b\u3092\u7406\u89e3\u3059\u308b\u305f\u3081\u306e\u900f\u660e\u6027\u3082\u91cd\u8981\u3067\u3059\u3002\u305d\u306e\u305f\u3081\u3001\u672c\u30e9\u30a4\u30d6\u30e9\u30ea\u3067\u306f\u30c8\u30ec\u30fc\u30b9\u6a5f\u80fd\u3092\u63d0\u4f9b\u3057\u3001\u8a55\u4fa1\u3084\u30ac\u30fc\u30c9\u30ec\u30fc\u30eb\u306e\u52d5\u4f5c\u3092\u53ef\u8996\u5316\u53ef\u80fd\u3068\u3057\u3066\u3044\u307e\u3059\u3002\u9ad8\u5ea6\u306a\u30e6\u30fc\u30b6\u30fc\u306f\u3001\u3088\u308a\u4e0b\u4f4d\u306eAgent\u3084Runner\u306b\u76f4\u63a5\u30a2\u30af\u30bb\u30b9\u3059\u308b\u3053\u3068\u3082\u3067\u304d\u307e\u3059\u3002</p> <p>\u7d50\u8ad6: AgentPipeline\u306eDAG\u30eb\u30fc\u30c6\u30a3\u30f3\u30b0\u8a2d\u8a08\u306f\u3001\u30e6\u30fc\u30b6\u30fc\u306e\u30b3\u30fc\u30c9\u91cf\u3068\u6982\u5ff5\u8ca0\u8377\u3092\u52b9\u679c\u7684\u306b\u8efd\u6e1b\u3057\u3066\u304a\u308a\u3001\u591a\u304f\u306e\u30a8\u30f3\u30b8\u30cb\u30a2\u306b\u3068\u3063\u3066\u5b9f\u88c5\u30b3\u30b9\u30c8\u3068\u4fdd\u5b88\u6027\u306e\u30d0\u30e9\u30f3\u30b9\u304c\u975e\u5e38\u306b\u826f\u597d\u3067\u3059\u3002</p>"},{"location":"analysis_of_library/#3-web-gui","title":"3. \u5bfe\u8a71\u578b\u30ef\u30fc\u30af\u30d5\u30ed\u30fc\u3078\u306e\u62e1\u5f35\uff08\u30bf\u30fc\u30df\u30ca\u30eb\u2192Web GUI\u2192\u30c1\u30e3\u30c3\u30c8\u30dc\u30c3\u30c8\uff09","text":"<p>\u672c\u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u5bfe\u8a71\u7684\u306a\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u306b\u62e1\u5f35\u3059\u308b\u306b\u306f\u3001\u4e3b\u306b\u5165\u529b\u3068\u51fa\u529b\u306e\u30c1\u30e3\u30cd\u30eb\u306e\u5909\u66f4\u304c\u6c42\u3081\u3089\u308c\u307e\u3059\u3002\u73fe\u5728\u306ePipeline\u306f\u3001\u72b6\u614b\u3092\u4fdd\u6301\u3057\u306a\u304c\u3089<code>.run(user_input)</code>\u3067\u9010\u6b21\u5165\u529b\u3092\u53d7\u3051\u4ed8\u3051\u308b\u305f\u3081\u3001\u30bf\u30fc\u30df\u30ca\u30eb\u3067\u306f\u3053\u306e\u307e\u307e\u5bfe\u8a71\u53ef\u80fd\u3067\u3059\u3002</p> <p>Web GUI\u306e\u5834\u5408: Flask\u3084FastAPI\u3092\u7528\u3044\u305fAPI\u30b5\u30fc\u30d0\u3067\u3001Pipeline\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3092\u30bb\u30c3\u30b7\u30e7\u30f3\u3054\u3068\u306b\u4fdd\u6301\u3057\u3001\u5404\u30e6\u30fc\u30b6\u30fc\u306e\u5165\u529b\u306b\u5fdc\u3058\u3066<code>.run()</code>\u3092\u975e\u540c\u671f\u547c\u3073\u51fa\u3057\u3059\u308b\u69cb\u6210\u304c\u73fe\u5b9f\u7684\u3067\u3059\u3002OpenAI API\u306f\u30c8\u30fc\u30af\u30f3\u30b9\u30c8\u30ea\u30fc\u30df\u30f3\u30b0\u306b\u5bfe\u5fdc\u3057\u3066\u3044\u308b\u305f\u3081\u3001\u5fdc\u7b54\u3092\u30ea\u30a2\u30eb\u30bf\u30a4\u30e0\u3067\u8fd4\u3059\u3053\u3068\u3067UX\u3092\u9ad8\u3081\u3089\u308c\u307e\u3059\u3002\u3053\u308c\u306f\u3001WebSocket\u3084SSE\u3092\u7528\u3044\u305f\u30b9\u30c8\u30ea\u30fc\u30df\u30f3\u30b0\u3068\u3057\u3066\u5b9f\u88c5\u53ef\u80fd\u3067\u3059\u3002</p> <p>\u30c1\u30e3\u30c3\u30c8\u30dc\u30c3\u30c8\u306e\u5834\u5408: Slack\u3084Discord\u306a\u3069\u3067\u3082\u3001\u30e6\u30fc\u30b6\u30fcID\u3054\u3068\u306bPipeline\u30bb\u30c3\u30b7\u30e7\u30f3\u3092\u30de\u30c3\u30d4\u30f3\u30b0\u3057\u3066\u4fdd\u6301\u3059\u308b\u3053\u3068\u3067\u7d99\u7d9a\u7684\u306a\u4f1a\u8a71\u304c\u53ef\u80fd\u3067\u3059\u3002\u52a0\u3048\u3066\u3001\u30dc\u30bf\u30f3\u64cd\u4f5c\u3084\u69cb\u9020\u5316\u5165\u529b\uff08\u30d5\u30a9\u30fc\u30e0\uff09\u3092\u6271\u3046\u306b\u306f\u3001\u30a8\u30fc\u30b8\u30a7\u30f3\u30c8\u5185\u3067\u30c4\u30fc\u30eb\u3068\u3057\u3066\u300c\u30e6\u30fc\u30b6\u30fc\u78ba\u8a8d\u5f85\u3061\u300d\u30a4\u30d9\u30f3\u30c8\u3092\u5b9a\u7fa9\u3057\u3001\u30e6\u30fc\u30b6\u30fc\u304b\u3089\u306e\u5165\u529b\u3092\u53d7\u3051\u305f\u5f8c\u306b\u518d\u958b\u3059\u308b\u4ed5\u7d44\u307f\u304c\u6c42\u3081\u3089\u308c\u307e\u3059\u3002\u3053\u308c\u306fhandoff\uff08\u5236\u5fa1\u306e\u4e00\u6642\u7684\u306a\u4eba\u9593\u5074\u3078\u306e\u59d4\u8b72\uff09\u3068\u3057\u3066\u8a2d\u8a08\u3067\u304d\u307e\u3059\u3002</p> <p>\u3053\u306e\u3068\u304d\u3001\u518d\u958b\u51e6\u7406\u3092\u4fdd\u8a3c\u3059\u308b\u306b\u306f\u3001\u30e6\u30fc\u30b6\u30fc\u5165\u529b\u5f85\u3061\u3092\u793a\u3059\u7279\u5225\u306a\u30c4\u30fc\u30eb\u547c\u3073\u51fa\u3057\u3084\u30c8\u30fc\u30af\u30f3\u3092\u4f7f\u3044\u3001UI\u5074\u3067\u306e\u5165\u529b\u5b8c\u4e86\u5f8c\u306bPipeline\u3092resume\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002\u591a\u304f\u306e\u5834\u5408\u3001\u901a\u5e38\u306e\u30bf\u30fc\u30f3\u30c6\u30a4\u30ad\u30f3\u30b0\u3067\u3082\u5341\u5206\u5bfe\u8a71\u304c\u53ef\u80fd\u3067\u3042\u308b\u305f\u3081\u3001\u540c\u671f\u7684\u306a\u30dd\u30fc\u30ba\u306f\u9ad8\u5ea6\u306a\u30e6\u30fc\u30b9\u30b1\u30fc\u30b9\u306e\u307f\u306b\u9650\u5b9a\u3059\u3079\u304d\u3067\u3057\u3087\u3046\u3002</p> <p>\u30e6\u30fc\u30b6\u30fc\u4f53\u9a13\u3068\u4fdd\u5b88\u6027\u3078\u306e\u5f71\u97ff: \u30bf\u30fc\u30df\u30ca\u30eb\u3067\u306e\u30d7\u30ed\u30c8\u30bf\u30a4\u30d4\u30f3\u30b0\u304b\u3089Web\u3084\u30c1\u30e3\u30c3\u30c8UI\u3078\u306e\u79fb\u884c\u304c\u30b7\u30fc\u30e0\u30ec\u30b9\u306b\u884c\u3048\u308b\u8a2d\u8a08\u3068\u306a\u3063\u3066\u3044\u308b\u305f\u3081\u3001\u540c\u4e00\u306ePipeline\u30b3\u30fc\u30c9\u3092\u8907\u6570\u30a4\u30f3\u30bf\u30d5\u30a7\u30fc\u30b9\u3067\u518d\u5229\u7528\u53ef\u80fd\u3068\u3044\u3046\u70b9\u3067\u958b\u767a\u751f\u7523\u6027\u306f\u9ad8\u3044\u3067\u3059\u3002</p> <p>\u6280\u8853\u7684\u5b9f\u73fe\u6027: Python\u306eWeb\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u3084\u975e\u540c\u671f\u51e6\u7406\u3001\u30bb\u30c3\u30b7\u30e7\u30f3\u7ba1\u7406\u306b\u95a2\u3059\u308b\u77e5\u8b58\u304c\u3042\u308c\u3070\u3001\u4e0a\u8a18\u306e\u8a2d\u8a08\u306f\u6bd4\u8f03\u7684\u5bb9\u6613\u306b\u5b9f\u88c5\u53ef\u80fd\u3067\u3059\u3002\u30e6\u30fc\u30b6\u30fc\u6570\u306e\u5897\u52a0\u306b\u5099\u3048\u3066\u306f\u3001\u30bb\u30c3\u30b7\u30e7\u30f3\u306eGC\u3001\u30e2\u30c7\u30eb\u306e\u5171\u6709\u306a\u3069\u30b9\u30b1\u30fc\u30e9\u30d3\u30ea\u30c6\u30a3\u306e\u89b3\u70b9\u3082\u8003\u616e\u3059\u3079\u304d\u3067\u3059\u3002\u307e\u305f\u3001\u30c8\u30ec\u30fc\u30b9\u3084\u30ed\u30b0\u3092UI\u306b\u51fa\u529b\u3059\u308b\u4ed5\u7d44\u307f\uff08\u958b\u767a\u8005\u30e2\u30fc\u30c9\uff09\u3092\u5c0e\u5165\u3059\u308b\u3053\u3068\u3067\u3001\u4fdd\u5b88\u6027\u3068\u4fe1\u983c\u6027\u3082\u9ad8\u3081\u3089\u308c\u307e\u3059\u3002</p> <p>\u7d50\u8ad6: \u5bfe\u8a71\u578b\u62e1\u5f35\u3078\u306e\u9053\u7b4b\u306f\u660e\u78ba\u3067\u3042\u308a\u3001\u8a2d\u8a08\u4e0a\u306e\u969c\u58c1\u306f\u5c11\u306a\u3044\u3067\u3059\u3002Pipeline\u306e\u72b6\u614b\u4fdd\u6301\u8a2d\u8a08\u3084\u4e00\u8cab\u3057\u305fAPI\u69cb\u9020\u306b\u3088\u308a\u3001\u30bf\u30fc\u30df\u30ca\u30eb\u2192Web\u2192\u30c1\u30e3\u30c3\u30c8\u30dc\u30c3\u30c8\u3068\u3044\u3063\u305f\u5c55\u958b\u304c\u6bb5\u968e\u7684\u306b\u884c\u3048\u308b\u5f37\u307f\u3092\u6d3b\u304b\u3057\u3001\u591a\u69d8\u306a\u30e6\u30fc\u30b6\u30fc\u4f53\u9a13\u306e\u69cb\u7bc9\u304c\u53ef\u80fd\u3067\u3059\u3002</p>"},{"location":"analysis_of_library/#4-workflowdag","title":"4. Workflow/DAG\u6a5f\u80fd\u306e\u8a55\u4fa1","text":""},{"location":"analysis_of_library/#41","title":"4.1 \u5f37\u307f","text":"<ul> <li>\u5ba3\u8a00\u7684DAG\u5b9a\u7fa9: <code>dag = {\"A\": a, \"B\": b}</code> \u306e\u3088\u3046\u306a\u8f9e\u66f8\u30d9\u30fc\u30b9\u3067\u30b7\u30f3\u30d7\u30eb\u306b\u30d5\u30ed\u30fc\u8a18\u8ff0\u304c\u53ef\u80fd\u3067\u3001\u5b66\u7fd2\u30b3\u30b9\u30c8\u304c\u4f4e\u3044\u3002</li> <li>Pipeline\u518d\u5229\u7528\u6027: <code>AgentPipeline</code> \u3092\u305d\u306e\u307e\u307e\u5404\u30ce\u30fc\u30c9\u3068\u3057\u3066\u914d\u7f6e\u3067\u304d\u308b\u305f\u3081\u3001\u65e2\u5b58\u8cc7\u7523\u3092\u7d44\u307f\u5408\u308f\u305b\u3066\u8907\u96d1\u306a\u30ef\u30fc\u30af\u30d5\u30ed\u30fc\u3092\u69cb\u7bc9\u3067\u304d\u308b\u3002</li> <li>\u6697\u9ed9\u306e END \u30eb\u30fc\u30eb: \u30b4\u30fc\u30eb\u30ce\u30fc\u30c9\u3092\u660e\u793a\u3057\u306a\u304f\u3066\u3082\u7d42\u4e86\u3067\u304d\u308b\u305f\u3081\u3001\u30a8\u30c3\u30b8\u30b1\u30fc\u30b9\u306e\u5c11\u306a\u3044\u6700\u77ed\u69cb\u6210\u304c\u53ef\u80fd\u3002</li> <li>\u52d5\u7684\u30eb\u30fc\u30c6\u30a3\u30f3\u30b0: <code>router_fn</code> \u306b\u3088\u308b\u6761\u4ef6\u5206\u5c90\u3067\u3001\u8a55\u4fa1\u7d50\u679c\u3084\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u306b\u5fdc\u3058\u305f\u30eb\u30fc\u30c8\u5909\u66f4\u304c\u5bb9\u6613\u3002</li> </ul>"},{"location":"analysis_of_library/#42","title":"4.2 \u8ab2\u984c","text":"<ul> <li>\u30b9\u30b1\u30fc\u30eb\u6642\u306e\u53ef\u8aad\u6027: \u30ce\u30fc\u30c9\u6570\u304c\u5897\u3048\u308b\u3068\u8f9e\u66f8\u5b9a\u7fa9\u304c\u7169\u96d1\u5316\u3057\u3001\u5168\u4f53\u50cf\u3092\u628a\u63e1\u3057\u3065\u3089\u3044\u3002</li> <li>\u72b6\u614b\u5171\u6709\u306e\u66d6\u6627\u3055: <code>AgentPipeline</code> \u304c\u5185\u90e8\u3067\u4f1a\u8a71\u5c65\u6b74\u3092\u4fdd\u6301\u3059\u308b\u4e00\u65b9\u3001\u30ce\u30fc\u30c9\u9593\u3067\u5171\u6709\u3057\u305f\u3044\u5909\u6570\u3084\u4e00\u6642\u30c7\u30fc\u30bf\u3092\u3069\u3046\u6271\u3046\u304b\u306e\u6307\u91dd\u304c\u4e0d\u8db3\u3002</li> <li>\u30e6\u30fc\u30b6\u30fc\u5165\u529b\u30ce\u30fc\u30c9\u306e\u6b20\u5982: \u4eba\u9593\u5074\u30a4\u30f3\u30bf\u30e9\u30af\u30b7\u30e7\u30f3\u3092\u5f85\u3064\u30ce\u30fc\u30c9\u30bf\u30a4\u30d7\u304c\u6a19\u6e96\u5316\u3055\u308c\u3066\u304a\u3089\u305a\u3001\u624b\u52d5\u3067\u30c4\u30fc\u30eb\uff0f\u30cf\u30f3\u30c9\u30aa\u30d5\u3092\u66f8\u304f\u5fc5\u8981\u304c\u3042\u308b\u3002</li> <li>\u4e26\u5217\u5b9f\u884c\u306e\u30b5\u30dd\u30fc\u30c8: \u73fe\u72b6\u3067\u306f\u76f4\u5217\u30d5\u30ed\u30fc\u524d\u63d0\u3002LangGraph \u306e\u3088\u3046\u306a\u5206\u5c90\uff0b\u30de\u30fc\u30b8\u3084\u30d5\u30a1\u30f3\u30a2\u30a6\u30c8\u306e\u8a18\u6cd5\u304c\u306a\u3044\u3002</li> </ul>"},{"location":"analysis_of_library/#43","title":"4.3 \u8a55\u4fa1\u307e\u3068\u3081","text":"<p>\u73fe\u6642\u70b9\u306e Workflow/DAG \u6a5f\u80fd\u306f \u300c80% \u306e\u30e6\u30fc\u30b9\u30b1\u30fc\u30b9\u3092\u6700\u77ed\u30b3\u30fc\u30c9\u3067\u89e3\u6c7a\u3059\u308b\u30e9\u30a4\u30c8\u7d1a\u30bd\u30ea\u30e5\u30fc\u30b7\u30e7\u30f3\u300d \u3068\u3057\u3066\u9ad8\u8a55\u4fa1\u3002\u305f\u3060\u3057\u3001\u4e0a\u8a18\u8ab2\u984c\u3092\u89e3\u6c7a\u3059\u308b\u3053\u3068\u3067\u3001\u3088\u308a\u5927\u898f\u6a21\u30fb\u30a4\u30f3\u30bf\u30e9\u30af\u30c6\u30a3\u30d6\u306a\u30b7\u30ca\u30ea\u30aa\u306b\u3082\u8010\u3048\u3046\u308b\u5805\u7262\u6027\u304c\u5f97\u3089\u308c\u308b\u3002</p>"},{"location":"analysis_of_library/#5-workflow","title":"5. Workflow \u62e1\u5f35\u8a2d\u8a08\u63d0\u6848","text":""},{"location":"analysis_of_library/#51","title":"5.1 \u8a2d\u8a08\u76ee\u6a19","text":"<ol> <li>\u5ba3\u8a00\u7684\u3067\u8aad\u307f\u66f8\u304d\u3057\u3084\u3059\u3044 DSL \u3092\u7dad\u6301\u3057\u3064\u3064\u3001\u5927\u898f\u6a21 DAG \u3067\u3082\u53ef\u8996\u6027\u3092\u78ba\u4fdd\u3002</li> <li>\u30e6\u30fc\u30b6\u30fc\u5165\u529b\u30ce\u30fc\u30c9 \u3092\u30d5\u30a1\u30fc\u30b9\u30c8\u30af\u30e9\u30b9\u6982\u5ff5\u3068\u3057\u3066\u8ffd\u52a0\u3057\u3001\u30bf\u30fc\u30df\u30ca\u30eb\uff0fGUI\uff0f\u30c1\u30e3\u30c3\u30c8\u3067\u7d71\u4e00\u7684\u306b\u6271\u3048\u308b\u3088\u3046\u306b\u3059\u308b\u3002</li> <li>\u72b6\u614b\u5171\u6709\u30b9\u30ad\u30fc\u30de \u3092\u5c0e\u5165\u3057\u3066\u3001\u30ce\u30fc\u30c9\u9593\u30c7\u30fc\u30bf\u4ea4\u63db\u3092\u578b\u5b89\u5168\u304b\u3064\u660e\u793a\u7684\u306b\u3059\u308b\u3002</li> <li>\u975e\u540c\u671f\u30fb\u4e26\u5217\u5b9f\u884c \u30aa\u30d7\u30b7\u30e7\u30f3\u3092\u63d0\u4f9b\u3057\u3001\u9577\u6642\u9593\u30bf\u30b9\u30af\u3084 I/O \u5f85\u3061\u3092\u52b9\u7387\u5316\u3002</li> <li>\u30aa\u30d6\u30b6\u30fc\u30d0\u30d3\u30ea\u30c6\u30a3\uff08\u30c8\u30ec\u30fc\u30b9\u3001\u30e1\u30c8\u30ea\u30af\u30b9\uff09\u3092\u6a19\u6e96\u88c5\u5099\u3057\u3001\u30c7\u30d0\u30c3\u30b0\u3068\u30e2\u30cb\u30bf\u30ea\u30f3\u30b0\u3092\u5bb9\u6613\u306b\u3002</li> </ol> <p>\u3053\u308c\u306b\u3088\u308a\u3001Agents SDK Models \u4e0a\u3067\u5ba3\u8a00\u7684\u304b\u3064\u62e1\u5f35\u6027\u306e\u9ad8\u3044 Workflow \u6a5f\u80fd\u3092\u5b9f\u73fe\u3057\u3001\u30bf\u30fc\u30df\u30ca\u30eb\u304b\u3089GUI/\u30c1\u30e3\u30c3\u30c8\u30dc\u30c3\u30c8\u307e\u3067\u4e00\u8cab\u3057\u305f\u958b\u767a\u4f53\u9a13\u3092\u63d0\u4f9b\u3067\u304d\u307e\u3059\u3002</p>"},{"location":"api_reference/","title":"API\u30ea\u30d5\u30a1\u30ec\u30f3\u30b9","text":"<p>\u3053\u306e\u30da\u30fc\u30b8\u3067\u306f\u3001mkdocstrings\u30d7\u30e9\u30b0\u30a4\u30f3\u3092\u4f7f\u7528\u3057\u3066<code>agents_sdk_models</code>\u30d1\u30c3\u30b1\u30fc\u30b8\u306eAPI\u30ea\u30d5\u30a1\u30ec\u30f3\u30b9\u3092\u81ea\u52d5\u751f\u6210\u3057\u307e\u3059\u3002</p> <p>Agents SDK Models \u30a8\u30fc\u30b8\u30a7\u30f3\u30c8SDK\u30e2\u30c7\u30eb</p>"},{"location":"api_reference/#agents_sdk_models.AgentPipeline","title":"<code>AgentPipeline</code>","text":"<p>AgentPipeline class for managing the generation and evaluation of content using OpenAI Agents SDK OpenAI Agents SDK\u3092\u4f7f\u7528\u3057\u3066\u30b3\u30f3\u30c6\u30f3\u30c4\u306e\u751f\u6210\u3068\u8a55\u4fa1\u3092\u7ba1\u7406\u3059\u308b\u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u30af\u30e9\u30b9</p> <p>.. deprecated:: 0.0.22    AgentPipeline is deprecated and will be removed in v0.1.0.     Use GenAgent with Flow/Step architecture instead.    See migration guide: docs/deprecation_plan.md</p> <p>This class handles: \u3053\u306e\u30af\u30e9\u30b9\u306f\u4ee5\u4e0b\u3092\u51e6\u7406\u3057\u307e\u3059\uff1a - Content generation using instructions / instructions\u3092\u4f7f\u7528\u3057\u305f\u30b3\u30f3\u30c6\u30f3\u30c4\u751f\u6210 - Content evaluation with scoring / \u30b9\u30b3\u30a2\u30ea\u30f3\u30b0\u306b\u3088\u308b\u30b3\u30f3\u30c6\u30f3\u30c4\u8a55\u4fa1 - Session history management / \u30bb\u30c3\u30b7\u30e7\u30f3\u5c65\u6b74\u306e\u7ba1\u7406 - Output formatting and routing / \u51fa\u529b\u306e\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u3068\u30eb\u30fc\u30c6\u30a3\u30f3\u30b0</p> <p>Preferred alternative: \u63a8\u5968\u4ee3\u66ff\u624b\u6bb5\uff1a - Use GenAgent for single-step pipeline functionality - Use Flow/Step architecture for complex workflows - See examples/gen_agent_example.py for migration examples</p> Source code in <code>src\\agents_sdk_models\\pipeline.py</code> <pre><code>class AgentPipeline:\n    \"\"\"\n    AgentPipeline class for managing the generation and evaluation of content using OpenAI Agents SDK\n    OpenAI Agents SDK\u3092\u4f7f\u7528\u3057\u3066\u30b3\u30f3\u30c6\u30f3\u30c4\u306e\u751f\u6210\u3068\u8a55\u4fa1\u3092\u7ba1\u7406\u3059\u308b\u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u30af\u30e9\u30b9\n\n    .. deprecated:: 0.0.22\n       AgentPipeline is deprecated and will be removed in v0.1.0. \n       Use GenAgent with Flow/Step architecture instead.\n       See migration guide: docs/deprecation_plan.md\n\n    This class handles:\n    \u3053\u306e\u30af\u30e9\u30b9\u306f\u4ee5\u4e0b\u3092\u51e6\u7406\u3057\u307e\u3059\uff1a\n    - Content generation using instructions / instructions\u3092\u4f7f\u7528\u3057\u305f\u30b3\u30f3\u30c6\u30f3\u30c4\u751f\u6210\n    - Content evaluation with scoring / \u30b9\u30b3\u30a2\u30ea\u30f3\u30b0\u306b\u3088\u308b\u30b3\u30f3\u30c6\u30f3\u30c4\u8a55\u4fa1\n    - Session history management / \u30bb\u30c3\u30b7\u30e7\u30f3\u5c65\u6b74\u306e\u7ba1\u7406\n    - Output formatting and routing / \u51fa\u529b\u306e\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u3068\u30eb\u30fc\u30c6\u30a3\u30f3\u30b0\n\n    Preferred alternative:\n    \u63a8\u5968\u4ee3\u66ff\u624b\u6bb5\uff1a\n    - Use GenAgent for single-step pipeline functionality\n    - Use Flow/Step architecture for complex workflows\n    - See examples/gen_agent_example.py for migration examples\n    \"\"\"\n\n    def __init__(\n        self,\n        name: str,\n        generation_instructions: str,\n        evaluation_instructions: Optional[str],\n        *,\n        input_guardrails: Optional[list] = None,\n        output_guardrails: Optional[list] = None,\n        output_model: Optional[Type[Any]] = None,\n        model: str | None = None,\n        evaluation_model: str | None = None,\n        generation_tools: Optional[list] = None,\n        evaluation_tools: Optional[list] = None,\n        routing_func: Optional[Callable[[Any], Any]] = None,\n        session_history: Optional[list] = None,\n        history_size: int = 10,\n        threshold: int = 85,\n        retries: int = 3,\n        improvement_callback: Optional[Callable[[Any, EvaluationResult], None]] = None,\n        dynamic_prompt: Optional[Callable[[str], str]] = None,\n        retry_comment_importance: Optional[list[str]] = None,\n        locale: str = \"en\",\n    ) -&gt; None:\n        \"\"\"\n        Initialize the Pipeline with configuration parameters\n        \u8a2d\u5b9a\u30d1\u30e9\u30e1\u30fc\u30bf\u3067\u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u3092\u521d\u671f\u5316\u3059\u308b\n\n        .. deprecated:: 0.0.22\n           AgentPipeline is deprecated and will be removed in v0.1.0. \n           Use GenAgent with Flow/Step architecture instead.\n           See migration guide: docs/deprecation_plan.md\n\n        Args:\n            name: Pipeline name / \u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u540d\n            generation_instructions: System prompt for generation / \u751f\u6210\u7528\u30b7\u30b9\u30c6\u30e0\u30d7\u30ed\u30f3\u30d7\u30c8\n            evaluation_instructions: System prompt for evaluation / \u8a55\u4fa1\u7528\u30b7\u30b9\u30c6\u30e0\u30d7\u30ed\u30f3\u30d7\u30c8\n            input_guardrails: Guardrails for generation / \u751f\u6210\u7528\u30ac\u30fc\u30c9\u30ec\u30fc\u30eb\n            output_guardrails: Guardrails for evaluation / \u8a55\u4fa1\u7528\u30ac\u30fc\u30c9\u30ec\u30fc\u30eb\n            output_model: Model for output formatting / \u51fa\u529b\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u7528\u30e2\u30c7\u30eb\n            model: LLM model name / LLM\u30e2\u30c7\u30eb\u540d\n            evaluation_model: Optional LLM model name for evaluation; if None, uses model. \u65e5\u672c\u8a9e: \u8a55\u4fa1\u7528\u306eLLM\u30e2\u30c7\u30eb\u540d\uff08None\u306e\u5834\u5408\u306fmodel\u3092\u4f7f\u7528\uff09\n            generation_tools: Tools for generation / \u751f\u6210\u7528\u30c4\u30fc\u30eb\n            evaluation_tools: Tools for evaluation / \u8a55\u4fa1\u7528\u30c4\u30fc\u30eb\n            routing_func: Function for output routing / \u51fa\u529b\u30eb\u30fc\u30c6\u30a3\u30f3\u30b0\u7528\u95a2\u6570\n            session_history: Session history / \u30bb\u30c3\u30b7\u30e7\u30f3\u5c65\u6b74\n            history_size: Size of history to keep / \u4fdd\u6301\u3059\u308b\u5c65\u6b74\u30b5\u30a4\u30ba\n            threshold: Evaluation score threshold / \u8a55\u4fa1\u30b9\u30b3\u30a2\u306e\u95be\u5024\n            retries: Number of retry attempts / \u30ea\u30c8\u30e9\u30a4\u8a66\u884c\u56de\u6570\n            improvement_callback: Callback for improvement suggestions / \u6539\u5584\u63d0\u6848\u7528\u30b3\u30fc\u30eb\u30d0\u30c3\u30af\n            dynamic_prompt: Optional function to dynamically build prompt / \u52d5\u7684\u30d7\u30ed\u30f3\u30d7\u30c8\u751f\u6210\u95a2\u6570\uff08\u4efb\u610f\uff09\n            retry_comment_importance: Importance levels of comments to include on retry / \u30ea\u30c8\u30e9\u30a4\u6642\u306b\u30d7\u30ed\u30f3\u30d7\u30c8\u306b\u542b\u3081\u308b\u30b3\u30e1\u30f3\u30c8\u306e\u91cd\u5927\u5ea6\u30ec\u30d9\u30eb\uff08\u4efb\u610f\uff09\n            locale: Language code for localized messages (\"en\" or \"ja\")\n        \"\"\"\n        import warnings\n        warnings.warn(\n            \"AgentPipeline is deprecated and will be removed in v0.1.0. \"\n            \"Use GenAgent with Flow/Step architecture instead. \"\n            \"See migration guide: docs/deprecation_plan.md\",\n            DeprecationWarning,\n            stacklevel=2\n        )\n        self.name = name\n        self.generation_instructions = generation_instructions.strip()\n        self.evaluation_instructions = evaluation_instructions.strip() if evaluation_instructions else None\n        self.output_model = output_model\n\n        self.model = model\n        self.evaluation_model = evaluation_model\n        self.generation_tools = generation_tools or []\n        self.evaluation_tools = evaluation_tools or []\n        self.input_guardrails = input_guardrails or []\n        self.output_guardrails = output_guardrails or []\n        self.routing_func = routing_func\n        self.session_history = session_history if session_history is not None else []\n        self.history_size = history_size\n        self.threshold = threshold\n        self.retries = retries\n        self.improvement_callback = improvement_callback\n        self.dynamic_prompt = dynamic_prompt\n        self.retry_comment_importance = retry_comment_importance or []\n        # Language code for localized messages (\"en\" or \"ja\")\n        self.locale = locale\n\n        # English: Get generation LLM instance; default tracing setting applied in get_llm\n        # \u65e5\u672c\u8a9e: \u751f\u6210\u7528LLM\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u3092\u53d6\u5f97\u3057\u307e\u3059\u3002tracing\u8a2d\u5b9a\u306fget_llm\u5074\u3067\u30c7\u30d5\u30a9\u30eb\u30c8\u5024\u3092\u4f7f\u7528\n        llm = get_llm(model) if model else None\n        # English: Determine evaluation LLM instance, fallback to generation model if evaluation_model is None\n        # \u65e5\u672c\u8a9e: \u8a55\u4fa1\u7528LLM\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u3092\u6c7a\u5b9a\u3002evaluation_model\u304cNone\u306e\u5834\u5408\u306f\u751f\u6210\u30e2\u30c7\u30eb\u3092\u4f7f\u7528\n        eval_source = evaluation_model if evaluation_model else model\n        llm_eval = get_llm(eval_source) if eval_source else None\n\n        # Agents ---------------------------------------------------------\n        self.gen_agent = Agent(\n            name=f\"{name}_generator\",\n            model=llm,\n            tools=self.generation_tools,\n            instructions=self.generation_instructions,\n            input_guardrails=self.input_guardrails,\n        )\n\n        # Localized evaluation format instructions\n        format_header = get_message(\"eval_output_format_header\", self.locale)\n        schema_instruction = get_message(\"eval_json_schema_instruction\", self.locale)\n        # JSON schema remains unlocalized\n        json_schema = textwrap.dedent(\"\"\"\\\n        {\n            \"score\": int(0\uff5e100),\n            \"comment\": [\n                {\n                    \"importance\": \"serious\" | \"normal\" | \"minor\",  # Importance field / \u91cd\u8981\u5ea6\u30d5\u30a3\u30fc\u30eb\u30c9\n                    \"content\": str  # Comment content / \u30b3\u30e1\u30f3\u30c8\u5185\u5bb9\n                }\n            ]\n        }\n        \"\"\")\n        json_instr = \"\\n\".join([\"+----\", format_header, schema_instruction, json_schema])\n        self.eval_agent = (\n            Agent(\n                name=f\"{name}_evaluator\",\n                model=llm_eval,\n                tools=self.evaluation_tools,\n                instructions=self.evaluation_instructions + json_instr,\n                output_guardrails=self.output_guardrails,\n            )\n            if self.evaluation_instructions\n            else None\n        )\n\n        self._runner = Runner()\n        self._pipeline_history: List[Dict[str, str]] = []\n\n    # ------------------------------------------------------------------\n    # helpers\n    # ------------------------------------------------------------------\n\n    def _build_generation_prompt(self, user_input: str) -&gt; str:\n        \"\"\"\n        Build the prompt for content generation\n        \u30b3\u30f3\u30c6\u30f3\u30c4\u751f\u6210\u7528\u306e\u30d7\u30ed\u30f3\u30d7\u30c8\u3092\u69cb\u7bc9\u3059\u308b\n\n        Args:\n            user_input: User input text / \u30e6\u30fc\u30b6\u30fc\u5165\u529b\u30c6\u30ad\u30b9\u30c8\n\n        Returns:\n            str: Formatted prompt for generation / \u751f\u6210\u7528\u306e\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u6e08\u307f\u30d7\u30ed\u30f3\u30d7\u30c8\n        \"\"\"\n        recent = \"\\n\".join(f\"User: {h['input']}\\nAI: {h['output']}\"\n                          for h in self._pipeline_history[-self.history_size:])\n        session = \"\\n\".join(self.session_history)\n        # Use localized prefix for user input\n        prefix = get_message(\"user_input_prefix\", self.locale)\n        return \"\\n\".join(filter(None, [session, recent, f\"{prefix} {user_input}\"]))\n\n    def _build_evaluation_prompt(self, user_input: str, generated_output: str) -&gt; str:\n        \"\"\"\n        Build the prompt for content evaluation\n        \u30b3\u30f3\u30c6\u30f3\u30c4\u8a55\u4fa1\u7528\u306e\u30d7\u30ed\u30f3\u30d7\u30c8\u3092\u69cb\u7bc9\u3059\u308b\n\n        Args:\n            user_input: Original user input / \u5143\u306e\u30e6\u30fc\u30b6\u30fc\u5165\u529b\n            generated_output: Generated content to evaluate / \u8a55\u4fa1\u5bfe\u8c61\u306e\u751f\u6210\u30b3\u30f3\u30c6\u30f3\u30c4\n\n        Returns:\n            str: Formatted prompt for evaluation / \u8a55\u4fa1\u7528\u306e\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u6e08\u307f\u30d7\u30ed\u30f3\u30d7\u30c8\n        \"\"\"\n        parts = []\n\n        # Add evaluation instructions if provided\n        # \u8a55\u4fa1\u6307\u793a\u304c\u63d0\u4f9b\u3055\u308c\u3066\u3044\u308b\u5834\u5408\u306f\u8ffd\u52a0\n        if self.evaluation_instructions:\n            parts.append(self.evaluation_instructions)\n\n        parts.extend([\n            \"----\",\n            f\"\u30e6\u30fc\u30b6\u30fc\u5165\u529b:\\n{user_input}\",\n            \"----\",\n            f\"\u751f\u6210\u7d50\u679c:\\n{generated_output}\",\n            \"\u4e0a\u8a18\u3092 JSON \u3067\u5fc5\u305a\u6b21\u306e\u5f62\u5f0f\u306b\u3057\u3066\u304f\u3060\u3055\u3044\"\n        ])\n        return \"\\n\".join(filter(None, parts)).strip()\n\n    @staticmethod\n    def _extract_json(text: str) -&gt; Dict[str, Any]:\n        \"\"\"\n        Extract JSON from text\n        \u30c6\u30ad\u30b9\u30c8\u304b\u3089JSON\u3092\u62bd\u51fa\u3059\u308b\n\n        Args:\n            text: Text containing JSON / JSON\u3092\u542b\u3080\u30c6\u30ad\u30b9\u30c8\n\n        Returns:\n            Dict[str, Any]: Extracted JSON data / \u62bd\u51fa\u3055\u308c\u305fJSON\u30c7\u30fc\u30bf\n\n        Raises:\n            ValueError: If JSON is not found in text / \u30c6\u30ad\u30b9\u30c8\u5185\u306bJSON\u304c\u898b\u3064\u304b\u3089\u306a\u3044\u5834\u5408\n        \"\"\"\n        match = re.search(r\"\\{.*\\}\", text, re.S)\n        if not match:\n            raise ValueError(\"JSON not found in evaluation output\")\n        return json.loads(match.group(0))\n\n    def _coerce_output(self, text: str):\n        \"\"\"\n        Convert output to specified model format\n        \u51fa\u529b\u3092\u6307\u5b9a\u3055\u308c\u305f\u30e2\u30c7\u30eb\u5f62\u5f0f\u306b\u5909\u63db\u3059\u308b\n\n        Args:\n            text: Output text to convert / \u5909\u63db\u5bfe\u8c61\u306e\u51fa\u529b\u30c6\u30ad\u30b9\u30c8\n\n        Returns:\n            Any: Converted output in specified format / \u6307\u5b9a\u3055\u308c\u305f\u5f62\u5f0f\u306e\u5909\u63db\u6e08\u307f\u51fa\u529b\n        \"\"\"\n        if self.output_model is None:\n            return text\n        try:\n            data = json.loads(text)\n        except json.JSONDecodeError:\n            return text\n        try:\n            if isinstance(self.output_model, type) and issubclass(self.output_model, BaseModel):\n                return self.output_model.model_validate(data)\n            if is_dataclass(self.output_model):\n                return self.output_model(**data)\n            return self.output_model(**data)\n        except Exception:\n            return text\n\n    def _append_to_session(self, user_input: str, raw_output: str):\n        \"\"\"\n        Append interaction to session history\n        \u30bb\u30c3\u30b7\u30e7\u30f3\u5c65\u6b74\u306b\u30a4\u30f3\u30bf\u30e9\u30af\u30b7\u30e7\u30f3\u3092\u8ffd\u52a0\u3059\u308b\n\n        Args:\n            user_input: User input text / \u30e6\u30fc\u30b6\u30fc\u5165\u529b\u30c6\u30ad\u30b9\u30c8\n            raw_output: Generated output text / \u751f\u6210\u3055\u308c\u305f\u51fa\u529b\u30c6\u30ad\u30b9\u30c8\n        \"\"\"\n        if self.session_history is None:\n            return\n        self.session_history.append(f\"User: {user_input}\\nAI: {raw_output}\")\n\n    def _route(self, parsed_output):\n        \"\"\"\n        Route the parsed output through routing function if specified\n        \u6307\u5b9a\u3055\u308c\u3066\u3044\u308b\u5834\u5408\u3001\u30d1\u30fc\u30b9\u6e08\u307f\u51fa\u529b\u3092\u30eb\u30fc\u30c6\u30a3\u30f3\u30b0\u95a2\u6570\u3067\u51e6\u7406\u3059\u308b\n\n        Args:\n            parsed_output: Parsed output to route / \u30eb\u30fc\u30c6\u30a3\u30f3\u30b0\u5bfe\u8c61\u306e\u30d1\u30fc\u30b9\u6e08\u307f\u51fa\u529b\n\n        Returns:\n            Any: Routed output / \u30eb\u30fc\u30c6\u30a3\u30f3\u30b0\u6e08\u307f\u51fa\u529b\n        \"\"\"\n        return self.routing_func(parsed_output) if self.routing_func else parsed_output\n\n    # ------------------------------------------------------------------\n    # public\n    # ------------------------------------------------------------------\n\n    def run(self, user_input: str):\n        \"\"\"\n        Run the pipeline with user input\n        \u30e6\u30fc\u30b6\u30fc\u5165\u529b\u3067\u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u3092\u5b9f\u884c\u3059\u308b\n\n        Args:\n            user_input: User input text / \u30e6\u30fc\u30b6\u30fc\u5165\u529b\u30c6\u30ad\u30b9\u30c8\n\n        Returns:\n            Any: Processed output or None if evaluation fails / \u51e6\u7406\u6e08\u307f\u51fa\u529b\u3001\u307e\u305f\u306f\u8a55\u4fa1\u5931\u6557\u6642\u306fNone\n        \"\"\"\n        attempt = 0\n        last_eval_result: Optional[EvaluationResult] = None  # Store last evaluation result for retry\n        while attempt &lt;= self.retries:\n            # ---------------- Generation ----------------\n            # On retry, include prior evaluation comments if configured\n            if attempt &gt; 0 and last_eval_result and self.retry_comment_importance:\n                # Filter comments by importance\n                try:\n                    comments = [c for c in last_eval_result.comment if c.get(\"importance\") in self.retry_comment_importance]\n                except Exception:\n                    comments = []\n                # Format serious comments with header\n                # Localized header for evaluation feedback\n                feedback_header = get_message(\"evaluation_feedback_header\", self.locale)\n                # English: Format each comment line. \u65e5\u672c\u8a9e: \u5404\u30b3\u30e1\u30f3\u30c8\u884c\u3092\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\n                formatted_comments = [f\"- ({c.get('importance')}) {c.get('content')}\" for c in comments]\n                # English: Combine header and comment lines. \u65e5\u672c\u8a9e: \u30d8\u30c3\u30c0\u30fc\u3068\u30b3\u30e1\u30f3\u30c8\u884c\u3092\u7d50\u5408\n                comment_block = \"\\n\".join([feedback_header] + formatted_comments)\n            else:\n                comment_block = \"\"\n            # Build base prompt\n            if attempt &gt; 0 and comment_block:\n                if self.dynamic_prompt:\n                    # English: Use dynamic prompt if provided. \u65e5\u672c\u8a9e: dynamic_prompt\u304c\u3042\u308c\u3070\u305d\u308c\u3092\u4f7f\u7528\n                    gen_prompt = self.dynamic_prompt(user_input)\n                else:\n                    # Localized header for AI history\n                    ai_history_header = get_message(\"ai_history_header\", self.locale)\n                    # English: Extract AI outputs from pipeline history, omit user inputs. \u65e5\u672c\u8a9e: \u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u5c65\u6b74\u304b\u3089AI\u306e\u51fa\u529b\u306e\u307f\u53d6\u5f97\n                    ai_outputs = \"\\n\".join(h[\"output\"] for h in self._pipeline_history[-self.history_size:])\n                    # Localized prefix for user input line\n                    prefix = get_message(\"user_input_prefix\", self.locale)\n                    # English: Current user input line. \u65e5\u672c\u8a9e: \u73fe\u5728\u306e\u30e6\u30fc\u30b6\u30fc\u5165\u529b\u884c\n                    user_input_line = f\"{prefix} {user_input}\"\n                    # English: Combine AI outputs, feedback, and current user input. \u65e5\u672c\u8a9e: AI\u51fa\u529b\u3001\u30d5\u30a3\u30fc\u30c9\u30d0\u30c3\u30af\u3001\u73fe\u5728\u306e\u30e6\u30fc\u30b6\u30fc\u5165\u529b\u3092\u7d50\u5408\n                    gen_prompt = \"\\n\\n\".join([ai_history_header, ai_outputs, comment_block, user_input_line])\n            else:\n                if self.dynamic_prompt:\n                    gen_prompt = self.dynamic_prompt(user_input)\n                else:\n                    gen_prompt = self._build_generation_prompt(user_input)\n\n            gen_result = self._runner.run_sync(self.gen_agent, gen_prompt)\n            raw_output_text = getattr(gen_result, \"final_output\", str(gen_result))\n            if hasattr(gen_result, \"tool_calls\") and gen_result.tool_calls:\n                raw_output_text = str(gen_result.tool_calls[0].call())\n\n            parsed_output = self._coerce_output(raw_output_text)\n            self._pipeline_history.append({\"input\": user_input, \"output\": raw_output_text})\n\n            # ---------------- Evaluation ----------------\n            if not self.eval_agent:\n                return self._route(parsed_output)\n\n            eval_prompt = self._build_evaluation_prompt(user_input, raw_output_text)\n\n            eval_raw = self._runner.run_sync(self.eval_agent, eval_prompt)\n            eval_text = getattr(eval_raw, \"final_output\", str(eval_raw))\n            try:\n                eval_dict = self._extract_json(eval_text)\n                eval_result = EvaluationResult(**eval_dict)\n            except Exception:\n                eval_result = EvaluationResult(score=0, comment=[Comment(importance=CommentImportance.SERIOUS, content=\"\u8a55\u4fa1 JSON \u306e\u89e3\u6790\u306b\u5931\u6557\")])\n\n            if eval_result.score &gt;= self.threshold:\n                self._append_to_session(user_input, raw_output_text)\n                return self._route(parsed_output)\n\n            # Store for next retry\n            last_eval_result = eval_result\n            attempt += 1\n\n        if self.improvement_callback:\n            self.improvement_callback(parsed_output, eval_result)\n        return None\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.AgentPipeline.__init__","title":"<code>__init__(name, generation_instructions, evaluation_instructions, *, input_guardrails=None, output_guardrails=None, output_model=None, model=None, evaluation_model=None, generation_tools=None, evaluation_tools=None, routing_func=None, session_history=None, history_size=10, threshold=85, retries=3, improvement_callback=None, dynamic_prompt=None, retry_comment_importance=None, locale='en')</code>","text":"<p>Initialize the Pipeline with configuration parameters \u8a2d\u5b9a\u30d1\u30e9\u30e1\u30fc\u30bf\u3067\u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u3092\u521d\u671f\u5316\u3059\u308b</p> <p>.. deprecated:: 0.0.22    AgentPipeline is deprecated and will be removed in v0.1.0.     Use GenAgent with Flow/Step architecture instead.    See migration guide: docs/deprecation_plan.md</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Pipeline name / \u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u540d</p> required <code>generation_instructions</code> <code>str</code> <p>System prompt for generation / \u751f\u6210\u7528\u30b7\u30b9\u30c6\u30e0\u30d7\u30ed\u30f3\u30d7\u30c8</p> required <code>evaluation_instructions</code> <code>Optional[str]</code> <p>System prompt for evaluation / \u8a55\u4fa1\u7528\u30b7\u30b9\u30c6\u30e0\u30d7\u30ed\u30f3\u30d7\u30c8</p> required <code>input_guardrails</code> <code>Optional[list]</code> <p>Guardrails for generation / \u751f\u6210\u7528\u30ac\u30fc\u30c9\u30ec\u30fc\u30eb</p> <code>None</code> <code>output_guardrails</code> <code>Optional[list]</code> <p>Guardrails for evaluation / \u8a55\u4fa1\u7528\u30ac\u30fc\u30c9\u30ec\u30fc\u30eb</p> <code>None</code> <code>output_model</code> <code>Optional[Type[Any]]</code> <p>Model for output formatting / \u51fa\u529b\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u7528\u30e2\u30c7\u30eb</p> <code>None</code> <code>model</code> <code>str | None</code> <p>LLM model name / LLM\u30e2\u30c7\u30eb\u540d</p> <code>None</code> <code>evaluation_model</code> <code>str | None</code> <p>Optional LLM model name for evaluation; if None, uses model. \u65e5\u672c\u8a9e: \u8a55\u4fa1\u7528\u306eLLM\u30e2\u30c7\u30eb\u540d\uff08None\u306e\u5834\u5408\u306fmodel\u3092\u4f7f\u7528\uff09</p> <code>None</code> <code>generation_tools</code> <code>Optional[list]</code> <p>Tools for generation / \u751f\u6210\u7528\u30c4\u30fc\u30eb</p> <code>None</code> <code>evaluation_tools</code> <code>Optional[list]</code> <p>Tools for evaluation / \u8a55\u4fa1\u7528\u30c4\u30fc\u30eb</p> <code>None</code> <code>routing_func</code> <code>Optional[Callable[[Any], Any]]</code> <p>Function for output routing / \u51fa\u529b\u30eb\u30fc\u30c6\u30a3\u30f3\u30b0\u7528\u95a2\u6570</p> <code>None</code> <code>session_history</code> <code>Optional[list]</code> <p>Session history / \u30bb\u30c3\u30b7\u30e7\u30f3\u5c65\u6b74</p> <code>None</code> <code>history_size</code> <code>int</code> <p>Size of history to keep / \u4fdd\u6301\u3059\u308b\u5c65\u6b74\u30b5\u30a4\u30ba</p> <code>10</code> <code>threshold</code> <code>int</code> <p>Evaluation score threshold / \u8a55\u4fa1\u30b9\u30b3\u30a2\u306e\u95be\u5024</p> <code>85</code> <code>retries</code> <code>int</code> <p>Number of retry attempts / \u30ea\u30c8\u30e9\u30a4\u8a66\u884c\u56de\u6570</p> <code>3</code> <code>improvement_callback</code> <code>Optional[Callable[[Any, EvaluationResult], None]]</code> <p>Callback for improvement suggestions / \u6539\u5584\u63d0\u6848\u7528\u30b3\u30fc\u30eb\u30d0\u30c3\u30af</p> <code>None</code> <code>dynamic_prompt</code> <code>Optional[Callable[[str], str]]</code> <p>Optional function to dynamically build prompt / \u52d5\u7684\u30d7\u30ed\u30f3\u30d7\u30c8\u751f\u6210\u95a2\u6570\uff08\u4efb\u610f\uff09</p> <code>None</code> <code>retry_comment_importance</code> <code>Optional[list[str]]</code> <p>Importance levels of comments to include on retry / \u30ea\u30c8\u30e9\u30a4\u6642\u306b\u30d7\u30ed\u30f3\u30d7\u30c8\u306b\u542b\u3081\u308b\u30b3\u30e1\u30f3\u30c8\u306e\u91cd\u5927\u5ea6\u30ec\u30d9\u30eb\uff08\u4efb\u610f\uff09</p> <code>None</code> <code>locale</code> <code>str</code> <p>Language code for localized messages (\"en\" or \"ja\")</p> <code>'en'</code> Source code in <code>src\\agents_sdk_models\\pipeline.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    generation_instructions: str,\n    evaluation_instructions: Optional[str],\n    *,\n    input_guardrails: Optional[list] = None,\n    output_guardrails: Optional[list] = None,\n    output_model: Optional[Type[Any]] = None,\n    model: str | None = None,\n    evaluation_model: str | None = None,\n    generation_tools: Optional[list] = None,\n    evaluation_tools: Optional[list] = None,\n    routing_func: Optional[Callable[[Any], Any]] = None,\n    session_history: Optional[list] = None,\n    history_size: int = 10,\n    threshold: int = 85,\n    retries: int = 3,\n    improvement_callback: Optional[Callable[[Any, EvaluationResult], None]] = None,\n    dynamic_prompt: Optional[Callable[[str], str]] = None,\n    retry_comment_importance: Optional[list[str]] = None,\n    locale: str = \"en\",\n) -&gt; None:\n    \"\"\"\n    Initialize the Pipeline with configuration parameters\n    \u8a2d\u5b9a\u30d1\u30e9\u30e1\u30fc\u30bf\u3067\u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u3092\u521d\u671f\u5316\u3059\u308b\n\n    .. deprecated:: 0.0.22\n       AgentPipeline is deprecated and will be removed in v0.1.0. \n       Use GenAgent with Flow/Step architecture instead.\n       See migration guide: docs/deprecation_plan.md\n\n    Args:\n        name: Pipeline name / \u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u540d\n        generation_instructions: System prompt for generation / \u751f\u6210\u7528\u30b7\u30b9\u30c6\u30e0\u30d7\u30ed\u30f3\u30d7\u30c8\n        evaluation_instructions: System prompt for evaluation / \u8a55\u4fa1\u7528\u30b7\u30b9\u30c6\u30e0\u30d7\u30ed\u30f3\u30d7\u30c8\n        input_guardrails: Guardrails for generation / \u751f\u6210\u7528\u30ac\u30fc\u30c9\u30ec\u30fc\u30eb\n        output_guardrails: Guardrails for evaluation / \u8a55\u4fa1\u7528\u30ac\u30fc\u30c9\u30ec\u30fc\u30eb\n        output_model: Model for output formatting / \u51fa\u529b\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u7528\u30e2\u30c7\u30eb\n        model: LLM model name / LLM\u30e2\u30c7\u30eb\u540d\n        evaluation_model: Optional LLM model name for evaluation; if None, uses model. \u65e5\u672c\u8a9e: \u8a55\u4fa1\u7528\u306eLLM\u30e2\u30c7\u30eb\u540d\uff08None\u306e\u5834\u5408\u306fmodel\u3092\u4f7f\u7528\uff09\n        generation_tools: Tools for generation / \u751f\u6210\u7528\u30c4\u30fc\u30eb\n        evaluation_tools: Tools for evaluation / \u8a55\u4fa1\u7528\u30c4\u30fc\u30eb\n        routing_func: Function for output routing / \u51fa\u529b\u30eb\u30fc\u30c6\u30a3\u30f3\u30b0\u7528\u95a2\u6570\n        session_history: Session history / \u30bb\u30c3\u30b7\u30e7\u30f3\u5c65\u6b74\n        history_size: Size of history to keep / \u4fdd\u6301\u3059\u308b\u5c65\u6b74\u30b5\u30a4\u30ba\n        threshold: Evaluation score threshold / \u8a55\u4fa1\u30b9\u30b3\u30a2\u306e\u95be\u5024\n        retries: Number of retry attempts / \u30ea\u30c8\u30e9\u30a4\u8a66\u884c\u56de\u6570\n        improvement_callback: Callback for improvement suggestions / \u6539\u5584\u63d0\u6848\u7528\u30b3\u30fc\u30eb\u30d0\u30c3\u30af\n        dynamic_prompt: Optional function to dynamically build prompt / \u52d5\u7684\u30d7\u30ed\u30f3\u30d7\u30c8\u751f\u6210\u95a2\u6570\uff08\u4efb\u610f\uff09\n        retry_comment_importance: Importance levels of comments to include on retry / \u30ea\u30c8\u30e9\u30a4\u6642\u306b\u30d7\u30ed\u30f3\u30d7\u30c8\u306b\u542b\u3081\u308b\u30b3\u30e1\u30f3\u30c8\u306e\u91cd\u5927\u5ea6\u30ec\u30d9\u30eb\uff08\u4efb\u610f\uff09\n        locale: Language code for localized messages (\"en\" or \"ja\")\n    \"\"\"\n    import warnings\n    warnings.warn(\n        \"AgentPipeline is deprecated and will be removed in v0.1.0. \"\n        \"Use GenAgent with Flow/Step architecture instead. \"\n        \"See migration guide: docs/deprecation_plan.md\",\n        DeprecationWarning,\n        stacklevel=2\n    )\n    self.name = name\n    self.generation_instructions = generation_instructions.strip()\n    self.evaluation_instructions = evaluation_instructions.strip() if evaluation_instructions else None\n    self.output_model = output_model\n\n    self.model = model\n    self.evaluation_model = evaluation_model\n    self.generation_tools = generation_tools or []\n    self.evaluation_tools = evaluation_tools or []\n    self.input_guardrails = input_guardrails or []\n    self.output_guardrails = output_guardrails or []\n    self.routing_func = routing_func\n    self.session_history = session_history if session_history is not None else []\n    self.history_size = history_size\n    self.threshold = threshold\n    self.retries = retries\n    self.improvement_callback = improvement_callback\n    self.dynamic_prompt = dynamic_prompt\n    self.retry_comment_importance = retry_comment_importance or []\n    # Language code for localized messages (\"en\" or \"ja\")\n    self.locale = locale\n\n    # English: Get generation LLM instance; default tracing setting applied in get_llm\n    # \u65e5\u672c\u8a9e: \u751f\u6210\u7528LLM\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u3092\u53d6\u5f97\u3057\u307e\u3059\u3002tracing\u8a2d\u5b9a\u306fget_llm\u5074\u3067\u30c7\u30d5\u30a9\u30eb\u30c8\u5024\u3092\u4f7f\u7528\n    llm = get_llm(model) if model else None\n    # English: Determine evaluation LLM instance, fallback to generation model if evaluation_model is None\n    # \u65e5\u672c\u8a9e: \u8a55\u4fa1\u7528LLM\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u3092\u6c7a\u5b9a\u3002evaluation_model\u304cNone\u306e\u5834\u5408\u306f\u751f\u6210\u30e2\u30c7\u30eb\u3092\u4f7f\u7528\n    eval_source = evaluation_model if evaluation_model else model\n    llm_eval = get_llm(eval_source) if eval_source else None\n\n    # Agents ---------------------------------------------------------\n    self.gen_agent = Agent(\n        name=f\"{name}_generator\",\n        model=llm,\n        tools=self.generation_tools,\n        instructions=self.generation_instructions,\n        input_guardrails=self.input_guardrails,\n    )\n\n    # Localized evaluation format instructions\n    format_header = get_message(\"eval_output_format_header\", self.locale)\n    schema_instruction = get_message(\"eval_json_schema_instruction\", self.locale)\n    # JSON schema remains unlocalized\n    json_schema = textwrap.dedent(\"\"\"\\\n    {\n        \"score\": int(0\uff5e100),\n        \"comment\": [\n            {\n                \"importance\": \"serious\" | \"normal\" | \"minor\",  # Importance field / \u91cd\u8981\u5ea6\u30d5\u30a3\u30fc\u30eb\u30c9\n                \"content\": str  # Comment content / \u30b3\u30e1\u30f3\u30c8\u5185\u5bb9\n            }\n        ]\n    }\n    \"\"\")\n    json_instr = \"\\n\".join([\"+----\", format_header, schema_instruction, json_schema])\n    self.eval_agent = (\n        Agent(\n            name=f\"{name}_evaluator\",\n            model=llm_eval,\n            tools=self.evaluation_tools,\n            instructions=self.evaluation_instructions + json_instr,\n            output_guardrails=self.output_guardrails,\n        )\n        if self.evaluation_instructions\n        else None\n    )\n\n    self._runner = Runner()\n    self._pipeline_history: List[Dict[str, str]] = []\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.AgentPipeline.run","title":"<code>run(user_input)</code>","text":"<p>Run the pipeline with user input \u30e6\u30fc\u30b6\u30fc\u5165\u529b\u3067\u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u3092\u5b9f\u884c\u3059\u308b</p> <p>Parameters:</p> Name Type Description Default <code>user_input</code> <code>str</code> <p>User input text / \u30e6\u30fc\u30b6\u30fc\u5165\u529b\u30c6\u30ad\u30b9\u30c8</p> required <p>Returns:</p> Name Type Description <code>Any</code> <p>Processed output or None if evaluation fails / \u51e6\u7406\u6e08\u307f\u51fa\u529b\u3001\u307e\u305f\u306f\u8a55\u4fa1\u5931\u6557\u6642\u306fNone</p> Source code in <code>src\\agents_sdk_models\\pipeline.py</code> <pre><code>def run(self, user_input: str):\n    \"\"\"\n    Run the pipeline with user input\n    \u30e6\u30fc\u30b6\u30fc\u5165\u529b\u3067\u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u3092\u5b9f\u884c\u3059\u308b\n\n    Args:\n        user_input: User input text / \u30e6\u30fc\u30b6\u30fc\u5165\u529b\u30c6\u30ad\u30b9\u30c8\n\n    Returns:\n        Any: Processed output or None if evaluation fails / \u51e6\u7406\u6e08\u307f\u51fa\u529b\u3001\u307e\u305f\u306f\u8a55\u4fa1\u5931\u6557\u6642\u306fNone\n    \"\"\"\n    attempt = 0\n    last_eval_result: Optional[EvaluationResult] = None  # Store last evaluation result for retry\n    while attempt &lt;= self.retries:\n        # ---------------- Generation ----------------\n        # On retry, include prior evaluation comments if configured\n        if attempt &gt; 0 and last_eval_result and self.retry_comment_importance:\n            # Filter comments by importance\n            try:\n                comments = [c for c in last_eval_result.comment if c.get(\"importance\") in self.retry_comment_importance]\n            except Exception:\n                comments = []\n            # Format serious comments with header\n            # Localized header for evaluation feedback\n            feedback_header = get_message(\"evaluation_feedback_header\", self.locale)\n            # English: Format each comment line. \u65e5\u672c\u8a9e: \u5404\u30b3\u30e1\u30f3\u30c8\u884c\u3092\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\n            formatted_comments = [f\"- ({c.get('importance')}) {c.get('content')}\" for c in comments]\n            # English: Combine header and comment lines. \u65e5\u672c\u8a9e: \u30d8\u30c3\u30c0\u30fc\u3068\u30b3\u30e1\u30f3\u30c8\u884c\u3092\u7d50\u5408\n            comment_block = \"\\n\".join([feedback_header] + formatted_comments)\n        else:\n            comment_block = \"\"\n        # Build base prompt\n        if attempt &gt; 0 and comment_block:\n            if self.dynamic_prompt:\n                # English: Use dynamic prompt if provided. \u65e5\u672c\u8a9e: dynamic_prompt\u304c\u3042\u308c\u3070\u305d\u308c\u3092\u4f7f\u7528\n                gen_prompt = self.dynamic_prompt(user_input)\n            else:\n                # Localized header for AI history\n                ai_history_header = get_message(\"ai_history_header\", self.locale)\n                # English: Extract AI outputs from pipeline history, omit user inputs. \u65e5\u672c\u8a9e: \u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u5c65\u6b74\u304b\u3089AI\u306e\u51fa\u529b\u306e\u307f\u53d6\u5f97\n                ai_outputs = \"\\n\".join(h[\"output\"] for h in self._pipeline_history[-self.history_size:])\n                # Localized prefix for user input line\n                prefix = get_message(\"user_input_prefix\", self.locale)\n                # English: Current user input line. \u65e5\u672c\u8a9e: \u73fe\u5728\u306e\u30e6\u30fc\u30b6\u30fc\u5165\u529b\u884c\n                user_input_line = f\"{prefix} {user_input}\"\n                # English: Combine AI outputs, feedback, and current user input. \u65e5\u672c\u8a9e: AI\u51fa\u529b\u3001\u30d5\u30a3\u30fc\u30c9\u30d0\u30c3\u30af\u3001\u73fe\u5728\u306e\u30e6\u30fc\u30b6\u30fc\u5165\u529b\u3092\u7d50\u5408\n                gen_prompt = \"\\n\\n\".join([ai_history_header, ai_outputs, comment_block, user_input_line])\n        else:\n            if self.dynamic_prompt:\n                gen_prompt = self.dynamic_prompt(user_input)\n            else:\n                gen_prompt = self._build_generation_prompt(user_input)\n\n        gen_result = self._runner.run_sync(self.gen_agent, gen_prompt)\n        raw_output_text = getattr(gen_result, \"final_output\", str(gen_result))\n        if hasattr(gen_result, \"tool_calls\") and gen_result.tool_calls:\n            raw_output_text = str(gen_result.tool_calls[0].call())\n\n        parsed_output = self._coerce_output(raw_output_text)\n        self._pipeline_history.append({\"input\": user_input, \"output\": raw_output_text})\n\n        # ---------------- Evaluation ----------------\n        if not self.eval_agent:\n            return self._route(parsed_output)\n\n        eval_prompt = self._build_evaluation_prompt(user_input, raw_output_text)\n\n        eval_raw = self._runner.run_sync(self.eval_agent, eval_prompt)\n        eval_text = getattr(eval_raw, \"final_output\", str(eval_raw))\n        try:\n            eval_dict = self._extract_json(eval_text)\n            eval_result = EvaluationResult(**eval_dict)\n        except Exception:\n            eval_result = EvaluationResult(score=0, comment=[Comment(importance=CommentImportance.SERIOUS, content=\"\u8a55\u4fa1 JSON \u306e\u89e3\u6790\u306b\u5931\u6557\")])\n\n        if eval_result.score &gt;= self.threshold:\n            self._append_to_session(user_input, raw_output_text)\n            return self._route(parsed_output)\n\n        # Store for next retry\n        last_eval_result = eval_result\n        attempt += 1\n\n    if self.improvement_callback:\n        self.improvement_callback(parsed_output, eval_result)\n    return None\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.AgentPipelineStep","title":"<code>AgentPipelineStep</code>","text":"<p>               Bases: <code>Step</code></p> <p>Step that wraps AgentPipeline for use in Flow Flow\u3067AgentPipeline\u3092\u4f7f\u7528\u3059\u308b\u305f\u3081\u306e\u30e9\u30c3\u30d1\u30fc\u30b9\u30c6\u30c3\u30d7</p> <p>This step allows using existing AgentPipeline instances as flow steps. \u3053\u306e\u30b9\u30c6\u30c3\u30d7\u306f\u65e2\u5b58\u306eAgentPipeline\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u3092\u30d5\u30ed\u30fc\u30b9\u30c6\u30c3\u30d7\u3068\u3057\u3066\u4f7f\u7528\u3067\u304d\u307e\u3059\u3002</p> Source code in <code>src\\agents_sdk_models\\step.py</code> <pre><code>class AgentPipelineStep(Step):\n    \"\"\"\n    Step that wraps AgentPipeline for use in Flow\n    Flow\u3067AgentPipeline\u3092\u4f7f\u7528\u3059\u308b\u305f\u3081\u306e\u30e9\u30c3\u30d1\u30fc\u30b9\u30c6\u30c3\u30d7\n\n    This step allows using existing AgentPipeline instances as flow steps.\n    \u3053\u306e\u30b9\u30c6\u30c3\u30d7\u306f\u65e2\u5b58\u306eAgentPipeline\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u3092\u30d5\u30ed\u30fc\u30b9\u30c6\u30c3\u30d7\u3068\u3057\u3066\u4f7f\u7528\u3067\u304d\u307e\u3059\u3002\n    \"\"\"\n\n    def __init__(self, name: str, pipeline: Any, next_step: Optional[str] = None):\n        \"\"\"\n        Initialize agent pipeline step\n        \u30a8\u30fc\u30b8\u30a7\u30f3\u30c8\u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u30b9\u30c6\u30c3\u30d7\u3092\u521d\u671f\u5316\n\n        Args:\n            name: Step name / \u30b9\u30c6\u30c3\u30d7\u540d\n            pipeline: AgentPipeline instance / AgentPipeline\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\n            next_step: Next step after pipeline execution / \u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u5b9f\u884c\u5f8c\u306e\u6b21\u30b9\u30c6\u30c3\u30d7\n        \"\"\"\n        super().__init__(name)\n        self.pipeline = pipeline\n        self.next_step = next_step\n\n    async def run(self, user_input: Optional[str], ctx: Context) -&gt; Context:\n        \"\"\"\n        Execute agent pipeline step\n        \u30a8\u30fc\u30b8\u30a7\u30f3\u30c8\u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u30b9\u30c6\u30c3\u30d7\u3092\u5b9f\u884c\n\n        Args:\n            user_input: User input / \u30e6\u30fc\u30b6\u30fc\u5165\u529b\n            ctx: Current context / \u73fe\u5728\u306e\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\n\n        Returns:\n            Context: Updated context / \u66f4\u65b0\u6e08\u307f\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\n        \"\"\"\n        ctx.update_step_info(self.name)\n\n        try:\n            # Use the last user input if available\n            # \u5229\u7528\u53ef\u80fd\u306a\u5834\u5408\u306f\u6700\u5f8c\u306e\u30e6\u30fc\u30b6\u30fc\u5165\u529b\u3092\u4f7f\u7528\n            input_text = user_input or ctx.last_user_input or \"\"\n\n            # Execute pipeline in thread pool to handle sync methods\n            # \u540c\u671f\u30e1\u30bd\u30c3\u30c9\u3092\u51e6\u7406\u3059\u308b\u305f\u3081\u306b\u30b9\u30ec\u30c3\u30c9\u30d7\u30fc\u30eb\u3067\u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u3092\u5b9f\u884c\n            loop = asyncio.get_event_loop()\n            with ThreadPoolExecutor() as executor:\n                future = loop.run_in_executor(executor, self.pipeline.run, input_text)\n                result = await future\n\n            # Store result in context\n            # \u7d50\u679c\u3092\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u306b\u4fdd\u5b58\n            if result is not None:\n                ctx.prev_outputs[self.name] = result\n                ctx.add_assistant_message(str(result))\n\n        except Exception as e:\n            ctx.add_system_message(f\"Pipeline execution error in {self.name}: {e}\")\n            ctx.prev_outputs[self.name] = None\n\n        # Set next step if specified\n        # \u6307\u5b9a\u3055\u308c\u3066\u3044\u308b\u5834\u5408\u306f\u6b21\u30b9\u30c6\u30c3\u30d7\u3092\u8a2d\u5b9a\n        if self.next_step:\n            ctx.goto(self.next_step)\n\n        return ctx\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.AgentPipelineStep.__init__","title":"<code>__init__(name, pipeline, next_step=None)</code>","text":"<p>Initialize agent pipeline step \u30a8\u30fc\u30b8\u30a7\u30f3\u30c8\u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u30b9\u30c6\u30c3\u30d7\u3092\u521d\u671f\u5316</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Step name / \u30b9\u30c6\u30c3\u30d7\u540d</p> required <code>pipeline</code> <code>Any</code> <p>AgentPipeline instance / AgentPipeline\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9</p> required <code>next_step</code> <code>Optional[str]</code> <p>Next step after pipeline execution / \u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u5b9f\u884c\u5f8c\u306e\u6b21\u30b9\u30c6\u30c3\u30d7</p> <code>None</code> Source code in <code>src\\agents_sdk_models\\step.py</code> <pre><code>def __init__(self, name: str, pipeline: Any, next_step: Optional[str] = None):\n    \"\"\"\n    Initialize agent pipeline step\n    \u30a8\u30fc\u30b8\u30a7\u30f3\u30c8\u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u30b9\u30c6\u30c3\u30d7\u3092\u521d\u671f\u5316\n\n    Args:\n        name: Step name / \u30b9\u30c6\u30c3\u30d7\u540d\n        pipeline: AgentPipeline instance / AgentPipeline\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\n        next_step: Next step after pipeline execution / \u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u5b9f\u884c\u5f8c\u306e\u6b21\u30b9\u30c6\u30c3\u30d7\n    \"\"\"\n    super().__init__(name)\n    self.pipeline = pipeline\n    self.next_step = next_step\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.AgentPipelineStep.run","title":"<code>run(user_input, ctx)</code>  <code>async</code>","text":"<p>Execute agent pipeline step \u30a8\u30fc\u30b8\u30a7\u30f3\u30c8\u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u30b9\u30c6\u30c3\u30d7\u3092\u5b9f\u884c</p> <p>Parameters:</p> Name Type Description Default <code>user_input</code> <code>Optional[str]</code> <p>User input / \u30e6\u30fc\u30b6\u30fc\u5165\u529b</p> required <code>ctx</code> <code>Context</code> <p>Current context / \u73fe\u5728\u306e\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8</p> required <p>Returns:</p> Name Type Description <code>Context</code> <code>Context</code> <p>Updated context / \u66f4\u65b0\u6e08\u307f\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8</p> Source code in <code>src\\agents_sdk_models\\step.py</code> <pre><code>async def run(self, user_input: Optional[str], ctx: Context) -&gt; Context:\n    \"\"\"\n    Execute agent pipeline step\n    \u30a8\u30fc\u30b8\u30a7\u30f3\u30c8\u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u30b9\u30c6\u30c3\u30d7\u3092\u5b9f\u884c\n\n    Args:\n        user_input: User input / \u30e6\u30fc\u30b6\u30fc\u5165\u529b\n        ctx: Current context / \u73fe\u5728\u306e\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\n\n    Returns:\n        Context: Updated context / \u66f4\u65b0\u6e08\u307f\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\n    \"\"\"\n    ctx.update_step_info(self.name)\n\n    try:\n        # Use the last user input if available\n        # \u5229\u7528\u53ef\u80fd\u306a\u5834\u5408\u306f\u6700\u5f8c\u306e\u30e6\u30fc\u30b6\u30fc\u5165\u529b\u3092\u4f7f\u7528\n        input_text = user_input or ctx.last_user_input or \"\"\n\n        # Execute pipeline in thread pool to handle sync methods\n        # \u540c\u671f\u30e1\u30bd\u30c3\u30c9\u3092\u51e6\u7406\u3059\u308b\u305f\u3081\u306b\u30b9\u30ec\u30c3\u30c9\u30d7\u30fc\u30eb\u3067\u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u3092\u5b9f\u884c\n        loop = asyncio.get_event_loop()\n        with ThreadPoolExecutor() as executor:\n            future = loop.run_in_executor(executor, self.pipeline.run, input_text)\n            result = await future\n\n        # Store result in context\n        # \u7d50\u679c\u3092\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u306b\u4fdd\u5b58\n        if result is not None:\n            ctx.prev_outputs[self.name] = result\n            ctx.add_assistant_message(str(result))\n\n    except Exception as e:\n        ctx.add_system_message(f\"Pipeline execution error in {self.name}: {e}\")\n        ctx.prev_outputs[self.name] = None\n\n    # Set next step if specified\n    # \u6307\u5b9a\u3055\u308c\u3066\u3044\u308b\u5834\u5408\u306f\u6b21\u30b9\u30c6\u30c3\u30d7\u3092\u8a2d\u5b9a\n    if self.next_step:\n        ctx.goto(self.next_step)\n\n    return ctx\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.ClarificationQuestion","title":"<code>ClarificationQuestion</code>  <code>dataclass</code>","text":"<p>Represents a clarification question from the pipeline \u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u304b\u3089\u306e\u660e\u78ba\u5316\u8cea\u554f\u3092\u8868\u73fe\u3059\u308b\u30af\u30e9\u30b9</p> <p>Attributes:</p> Name Type Description <code>question</code> <code>str</code> <p>The clarification question text / \u660e\u78ba\u5316\u8cea\u554f\u30c6\u30ad\u30b9\u30c8</p> <code>turn</code> <code>int</code> <p>Current turn number / \u73fe\u5728\u306e\u30bf\u30fc\u30f3\u756a\u53f7</p> <code>remaining_turns</code> <code>int</code> <p>Remaining turns / \u6b8b\u308a\u30bf\u30fc\u30f3\u6570</p> Source code in <code>src\\agents_sdk_models\\clearify_pipeline.py</code> <pre><code>@dataclass\nclass ClarificationQuestion:\n    \"\"\"\n    Represents a clarification question from the pipeline\n    \u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u304b\u3089\u306e\u660e\u78ba\u5316\u8cea\u554f\u3092\u8868\u73fe\u3059\u308b\u30af\u30e9\u30b9\n\n    Attributes:\n        question: The clarification question text / \u660e\u78ba\u5316\u8cea\u554f\u30c6\u30ad\u30b9\u30c8\n        turn: Current turn number / \u73fe\u5728\u306e\u30bf\u30fc\u30f3\u756a\u53f7\n        remaining_turns: Remaining turns / \u6b8b\u308a\u30bf\u30fc\u30f3\u6570\n    \"\"\"\n    question: str  # The clarification question text / \u660e\u78ba\u5316\u8cea\u554f\u30c6\u30ad\u30b9\u30c8\n    turn: int  # Current turn number / \u73fe\u5728\u306e\u30bf\u30fc\u30f3\u756a\u53f7\n    remaining_turns: int  # Remaining turns / \u6b8b\u308a\u30bf\u30fc\u30f3\u6570\n\n    def __str__(self) -&gt; str:\n        \"\"\"\n        String representation of the clarification question\n        \u660e\u78ba\u5316\u8cea\u554f\u306e\u6587\u5b57\u5217\u8868\u73fe\n\n        Returns:\n            str: Formatted question with turn info / \u30bf\u30fc\u30f3\u60c5\u5831\u4ed8\u304d\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u6e08\u307f\u8cea\u554f\n        \"\"\"\n        return f\"[\u30bf\u30fc\u30f3 {self.turn}/{self.turn + self.remaining_turns}] {self.question}\"\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.ClarificationQuestion.__str__","title":"<code>__str__()</code>","text":"<p>String representation of the clarification question \u660e\u78ba\u5316\u8cea\u554f\u306e\u6587\u5b57\u5217\u8868\u73fe</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Formatted question with turn info / \u30bf\u30fc\u30f3\u60c5\u5831\u4ed8\u304d\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u6e08\u307f\u8cea\u554f</p> Source code in <code>src\\agents_sdk_models\\clearify_pipeline.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"\n    String representation of the clarification question\n    \u660e\u78ba\u5316\u8cea\u554f\u306e\u6587\u5b57\u5217\u8868\u73fe\n\n    Returns:\n        str: Formatted question with turn info / \u30bf\u30fc\u30f3\u60c5\u5831\u4ed8\u304d\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u6e08\u307f\u8cea\u554f\n    \"\"\"\n    return f\"[\u30bf\u30fc\u30f3 {self.turn}/{self.turn + self.remaining_turns}] {self.question}\"\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.ClaudeModel","title":"<code>ClaudeModel</code>","text":"<p>               Bases: <code>OpenAIChatCompletionsModel</code></p> <p>Anthropic Claude model implementation that extends OpenAI's chat completions model OpenAI\u306e\u30c1\u30e3\u30c3\u30c8\u88dc\u5b8c\u30e2\u30c7\u30eb\u3092\u62e1\u5f35\u3057\u305fAnthropic Claude\u30e2\u30c7\u30eb\u306e\u5b9f\u88c5</p> Source code in <code>src\\agents_sdk_models\\anthropic.py</code> <pre><code>class ClaudeModel(OpenAIChatCompletionsModel):\n    \"\"\"\n    Anthropic Claude model implementation that extends OpenAI's chat completions model\n    OpenAI\u306e\u30c1\u30e3\u30c3\u30c8\u88dc\u5b8c\u30e2\u30c7\u30eb\u3092\u62e1\u5f35\u3057\u305fAnthropic Claude\u30e2\u30c7\u30eb\u306e\u5b9f\u88c5\n    \"\"\"\n\n    def __init__(\n        self,\n        model: str = \"claude-3-5-sonnet-latest\",\n        temperature: float = 0.3,\n        api_key: str = None,\n        base_url: str = \"https://api.anthropic.com/v1/\",\n        thinking: bool = False,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the Anthropic Claude model with OpenAI compatible interface\n        OpenAI\u4e92\u63db\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30fc\u30b9\u3067Anthropic Claude\u30e2\u30c7\u30eb\u3092\u521d\u671f\u5316\u3059\u308b\n\n        Args:\n            model (str): Name of the Claude model to use (e.g. \"claude-3-5-sonnet-latest\")\n                \u4f7f\u7528\u3059\u308bClaude\u30e2\u30c7\u30eb\u306e\u540d\u524d\uff08\u4f8b\uff1a\"claude-3-5-sonnet-latest\"\uff09\n            temperature (float): Sampling temperature between 0 and 1\n                \u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u6e29\u5ea6\uff080\u304b\u30891\u306e\u9593\uff09\n            api_key (str): Anthropic API key\n                Anthropic API\u30ad\u30fc\n            base_url (str): Base URL for the Anthropic OpenAI compatibility API\n                Anthropic OpenAI\u4e92\u63dbAPI\u306e\u30d9\u30fc\u30b9URL\n            thinking (bool): Enable extended thinking for complex reasoning\n                \u8907\u96d1\u306a\u63a8\u8ad6\u306e\u305f\u3081\u306e\u62e1\u5f35\u601d\u8003\u3092\u6709\u52b9\u306b\u3059\u308b\n            **kwargs: Additional arguments to pass to the OpenAI API\n                OpenAI API\u306b\u6e21\u3059\u8ffd\u52a0\u306e\u5f15\u6570\n        \"\"\"\n        # get_llm\u7d4c\u7531\u3067 base_url \u304c None \u306e\u5834\u5408\u306f\u30c7\u30d5\u30a9\u30eb\u30c8\u306e URL \u3092\u8a2d\u5b9a\n        if base_url == None:\n            base_url = \"https://api.anthropic.com/v1/\"\n\n        # api_key \u304c None \u306e\u5834\u5408\u306f\u74b0\u5883\u5909\u6570\u304b\u3089\u53d6\u5f97\n        if api_key is None:\n            api_key = os.environ.get(\"ANTHROPIC_API_KEY\")\n            if api_key is None:\n                raise ValueError(\"Anthropic API key is required. Get one from https://console.anthropic.com/\")\n\n        # Create AsyncOpenAI client with Anthropic base URL\n        # Anthropic\u306e\u30d9\u30fc\u30b9URL\u3067AsyncOpenAI\u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u3092\u4f5c\u6210\n        openai_client = AsyncOpenAI(base_url=base_url, api_key=api_key)\n\n        # Store the AsyncOpenAI client on the instance for direct access\n        # \u30c6\u30b9\u30c8\u3067\u53c2\u7167\u3067\u304d\u308b\u3088\u3046 AsyncOpenAI \u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u3092\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u306b\u4fdd\u5b58\u3059\u308b\n        self.openai_client = openai_client\n\n        # Store parameters for later use in API calls\n        # \u5f8c\u3067API\u30b3\u30fc\u30eb\u3067\u4f7f\u7528\u3059\u308b\u305f\u3081\u306b\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u4fdd\u5b58\n        self.temperature = temperature\n        self.thinking = thinking\n        self.kwargs = kwargs\n\n        # Initialize the parent class with our custom client\n        # \u30ab\u30b9\u30bf\u30e0\u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u3067\u89aa\u30af\u30e9\u30b9\u3092\u521d\u671f\u5316\n        super().__init__(\n            model=model,\n            openai_client=openai_client\n        )\n\n    # Override methods that make API calls to include our parameters\n    # API\u30b3\u30fc\u30eb\u3092\u884c\u3046\u30e1\u30bd\u30c3\u30c9\u3092\u30aa\u30fc\u30d0\u30fc\u30e9\u30a4\u30c9\u3057\u3066\u3001\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u542b\u3081\u308b\n    async def _create_chat_completion(self, *args, **kwargs):\n        \"\"\"Override to include temperature, thinking and other parameters\"\"\"\n        kwargs[\"temperature\"] = self.temperature\n\n        # Add thinking parameter if enabled\n        # \u62e1\u5f35\u601d\u8003\u304c\u6709\u52b9\u306a\u5834\u5408\u306fthinking\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u8ffd\u52a0\n        if self.thinking:\n            kwargs[\"thinking\"] = True\n\n        # Add any other custom parameters\n        # \u305d\u306e\u4ed6\u306e\u30ab\u30b9\u30bf\u30e0\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u8ffd\u52a0\n        kwargs.update(self.kwargs)\n\n        return await super()._create_chat_completion(*args, **kwargs)\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.ClaudeModel.__init__","title":"<code>__init__(model='claude-3-5-sonnet-latest', temperature=0.3, api_key=None, base_url='https://api.anthropic.com/v1/', thinking=False, **kwargs)</code>","text":"<p>Initialize the Anthropic Claude model with OpenAI compatible interface OpenAI\u4e92\u63db\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30fc\u30b9\u3067Anthropic Claude\u30e2\u30c7\u30eb\u3092\u521d\u671f\u5316\u3059\u308b</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Name of the Claude model to use (e.g. \"claude-3-5-sonnet-latest\") \u4f7f\u7528\u3059\u308bClaude\u30e2\u30c7\u30eb\u306e\u540d\u524d\uff08\u4f8b\uff1a\"claude-3-5-sonnet-latest\"\uff09</p> <code>'claude-3-5-sonnet-latest'</code> <code>temperature</code> <code>float</code> <p>Sampling temperature between 0 and 1 \u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u6e29\u5ea6\uff080\u304b\u30891\u306e\u9593\uff09</p> <code>0.3</code> <code>api_key</code> <code>str</code> <p>Anthropic API key Anthropic API\u30ad\u30fc</p> <code>None</code> <code>base_url</code> <code>str</code> <p>Base URL for the Anthropic OpenAI compatibility API Anthropic OpenAI\u4e92\u63dbAPI\u306e\u30d9\u30fc\u30b9URL</p> <code>'https://api.anthropic.com/v1/'</code> <code>thinking</code> <code>bool</code> <p>Enable extended thinking for complex reasoning \u8907\u96d1\u306a\u63a8\u8ad6\u306e\u305f\u3081\u306e\u62e1\u5f35\u601d\u8003\u3092\u6709\u52b9\u306b\u3059\u308b</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments to pass to the OpenAI API OpenAI API\u306b\u6e21\u3059\u8ffd\u52a0\u306e\u5f15\u6570</p> <code>{}</code> Source code in <code>src\\agents_sdk_models\\anthropic.py</code> <pre><code>def __init__(\n    self,\n    model: str = \"claude-3-5-sonnet-latest\",\n    temperature: float = 0.3,\n    api_key: str = None,\n    base_url: str = \"https://api.anthropic.com/v1/\",\n    thinking: bool = False,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initialize the Anthropic Claude model with OpenAI compatible interface\n    OpenAI\u4e92\u63db\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30fc\u30b9\u3067Anthropic Claude\u30e2\u30c7\u30eb\u3092\u521d\u671f\u5316\u3059\u308b\n\n    Args:\n        model (str): Name of the Claude model to use (e.g. \"claude-3-5-sonnet-latest\")\n            \u4f7f\u7528\u3059\u308bClaude\u30e2\u30c7\u30eb\u306e\u540d\u524d\uff08\u4f8b\uff1a\"claude-3-5-sonnet-latest\"\uff09\n        temperature (float): Sampling temperature between 0 and 1\n            \u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u6e29\u5ea6\uff080\u304b\u30891\u306e\u9593\uff09\n        api_key (str): Anthropic API key\n            Anthropic API\u30ad\u30fc\n        base_url (str): Base URL for the Anthropic OpenAI compatibility API\n            Anthropic OpenAI\u4e92\u63dbAPI\u306e\u30d9\u30fc\u30b9URL\n        thinking (bool): Enable extended thinking for complex reasoning\n            \u8907\u96d1\u306a\u63a8\u8ad6\u306e\u305f\u3081\u306e\u62e1\u5f35\u601d\u8003\u3092\u6709\u52b9\u306b\u3059\u308b\n        **kwargs: Additional arguments to pass to the OpenAI API\n            OpenAI API\u306b\u6e21\u3059\u8ffd\u52a0\u306e\u5f15\u6570\n    \"\"\"\n    # get_llm\u7d4c\u7531\u3067 base_url \u304c None \u306e\u5834\u5408\u306f\u30c7\u30d5\u30a9\u30eb\u30c8\u306e URL \u3092\u8a2d\u5b9a\n    if base_url == None:\n        base_url = \"https://api.anthropic.com/v1/\"\n\n    # api_key \u304c None \u306e\u5834\u5408\u306f\u74b0\u5883\u5909\u6570\u304b\u3089\u53d6\u5f97\n    if api_key is None:\n        api_key = os.environ.get(\"ANTHROPIC_API_KEY\")\n        if api_key is None:\n            raise ValueError(\"Anthropic API key is required. Get one from https://console.anthropic.com/\")\n\n    # Create AsyncOpenAI client with Anthropic base URL\n    # Anthropic\u306e\u30d9\u30fc\u30b9URL\u3067AsyncOpenAI\u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u3092\u4f5c\u6210\n    openai_client = AsyncOpenAI(base_url=base_url, api_key=api_key)\n\n    # Store the AsyncOpenAI client on the instance for direct access\n    # \u30c6\u30b9\u30c8\u3067\u53c2\u7167\u3067\u304d\u308b\u3088\u3046 AsyncOpenAI \u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u3092\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u306b\u4fdd\u5b58\u3059\u308b\n    self.openai_client = openai_client\n\n    # Store parameters for later use in API calls\n    # \u5f8c\u3067API\u30b3\u30fc\u30eb\u3067\u4f7f\u7528\u3059\u308b\u305f\u3081\u306b\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u4fdd\u5b58\n    self.temperature = temperature\n    self.thinking = thinking\n    self.kwargs = kwargs\n\n    # Initialize the parent class with our custom client\n    # \u30ab\u30b9\u30bf\u30e0\u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u3067\u89aa\u30af\u30e9\u30b9\u3092\u521d\u671f\u5316\n    super().__init__(\n        model=model,\n        openai_client=openai_client\n    )\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.Clearify","title":"<code>Clearify</code>","text":"<p>               Bases: <code>ClearifyBase</code></p> <p>Default clarification output with string user requirement \u6587\u5b57\u5217\u30e6\u30fc\u30b6\u30fc\u8981\u6c42\u3092\u6301\u3064\u30c7\u30d5\u30a9\u30eb\u30c8\u660e\u78ba\u5316\u51fa\u529b</p> <p>Attributes:</p> Name Type Description <code>clearity</code> <code>bool</code> <p>True if requirements are confirmed / \u8981\u4ef6\u304c\u78ba\u5b9a\u3057\u305f\u5834\u5408True</p> <code>user_requirement</code> <code>Optional[str]</code> <p>Confirmed user requirement as string / \u6587\u5b57\u5217\u3068\u3057\u3066\u78ba\u5b9a\u3057\u305f\u30e6\u30fc\u30b6\u30fc\u8981\u6c42</p> Source code in <code>src\\agents_sdk_models\\clearify_pipeline.py</code> <pre><code>class Clearify(ClearifyBase):\n    \"\"\"\n    Default clarification output with string user requirement\n    \u6587\u5b57\u5217\u30e6\u30fc\u30b6\u30fc\u8981\u6c42\u3092\u6301\u3064\u30c7\u30d5\u30a9\u30eb\u30c8\u660e\u78ba\u5316\u51fa\u529b\n\n    Attributes:\n        clearity: True if requirements are confirmed / \u8981\u4ef6\u304c\u78ba\u5b9a\u3057\u305f\u5834\u5408True\n        user_requirement: Confirmed user requirement as string / \u6587\u5b57\u5217\u3068\u3057\u3066\u78ba\u5b9a\u3057\u305f\u30e6\u30fc\u30b6\u30fc\u8981\u6c42\n    \"\"\"\n    user_requirement: Optional[str] = None  # Confirmed user requirement as string / \u6587\u5b57\u5217\u3068\u3057\u3066\u78ba\u5b9a\u3057\u305f\u30e6\u30fc\u30b6\u30fc\u8981\u6c42\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.ClearifyBase","title":"<code>ClearifyBase</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base class for requirement clarification output \u8981\u4ef6\u660e\u78ba\u5316\u51fa\u529b\u306e\u30d9\u30fc\u30b9\u30af\u30e9\u30b9</p> <p>Attributes:</p> Name Type Description <code>clearity</code> <code>bool</code> <p>True if requirements are confirmed / \u8981\u4ef6\u304c\u78ba\u5b9a\u3057\u305f\u5834\u5408True</p> Source code in <code>src\\agents_sdk_models\\clearify_pipeline.py</code> <pre><code>class ClearifyBase(BaseModel):\n    \"\"\"\n    Base class for requirement clarification output\n    \u8981\u4ef6\u660e\u78ba\u5316\u51fa\u529b\u306e\u30d9\u30fc\u30b9\u30af\u30e9\u30b9\n\n    Attributes:\n        clearity: True if requirements are confirmed / \u8981\u4ef6\u304c\u78ba\u5b9a\u3057\u305f\u5834\u5408True\n    \"\"\"\n    clearity: bool  # True if requirements are confirmed / \u8981\u4ef6\u304c\u78ba\u5b9a\u3057\u305f\u5834\u5408True\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.ClearifyPipeline","title":"<code>ClearifyPipeline</code>","text":"<p>               Bases: <code>AgentPipeline</code></p> <p>ClearifyPipeline class for requirements clarification using OpenAI Agents SDK OpenAI Agents SDK\u3092\u4f7f\u7528\u3057\u305f\u8981\u4ef6\u660e\u78ba\u5316\u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u30af\u30e9\u30b9</p> <p>This class extends AgentPipeline to handle: \u3053\u306e\u30af\u30e9\u30b9\u306fAgentPipeline\u3092\u62e1\u5f35\u3057\u3066\u4ee5\u4e0b\u3092\u51e6\u7406\u3057\u307e\u3059\uff1a - Iterative requirement clarification / \u53cd\u5fa9\u7684\u306a\u8981\u4ef6\u660e\u78ba\u5316 - Type-safe output wrapping / \u578b\u5b89\u5168\u306a\u51fa\u529b\u30e9\u30c3\u30d4\u30f3\u30b0 - Maximum turn control / \u6700\u5927\u30bf\u30fc\u30f3\u6570\u5236\u5fa1 - Structured requirement extraction / \u69cb\u9020\u5316\u3055\u308c\u305f\u8981\u6c42\u62bd\u51fa</p> Source code in <code>src\\agents_sdk_models\\clearify_pipeline.py</code> <pre><code>class ClearifyPipeline(AgentPipeline):\n    \"\"\"\n    ClearifyPipeline class for requirements clarification using OpenAI Agents SDK\n    OpenAI Agents SDK\u3092\u4f7f\u7528\u3057\u305f\u8981\u4ef6\u660e\u78ba\u5316\u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u30af\u30e9\u30b9\n\n    This class extends AgentPipeline to handle:\n    \u3053\u306e\u30af\u30e9\u30b9\u306fAgentPipeline\u3092\u62e1\u5f35\u3057\u3066\u4ee5\u4e0b\u3092\u51e6\u7406\u3057\u307e\u3059\uff1a\n    - Iterative requirement clarification / \u53cd\u5fa9\u7684\u306a\u8981\u4ef6\u660e\u78ba\u5316\n    - Type-safe output wrapping / \u578b\u5b89\u5168\u306a\u51fa\u529b\u30e9\u30c3\u30d4\u30f3\u30b0\n    - Maximum turn control / \u6700\u5927\u30bf\u30fc\u30f3\u6570\u5236\u5fa1\n    - Structured requirement extraction / \u69cb\u9020\u5316\u3055\u308c\u305f\u8981\u6c42\u62bd\u51fa\n    \"\"\"\n\n    def __init__(\n        self,\n        name: str,\n        generation_instructions: str,\n        output_data: Optional[Type[Any]] = None,\n        clerify_max_turns: int = 20,\n        evaluation_instructions: Optional[str] = None,\n        **kwargs\n    ) -&gt; None:\n        \"\"\"\n        Initialize the ClearifyPipeline with configuration parameters\n        \u8a2d\u5b9a\u30d1\u30e9\u30e1\u30fc\u30bf\u3067ClearifyPipeline\u3092\u521d\u671f\u5316\u3059\u308b\n\n        Args:\n            name: Pipeline name / \u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u540d\n            generation_instructions: System prompt for generation / \u751f\u6210\u7528\u30b7\u30b9\u30c6\u30e0\u30d7\u30ed\u30f3\u30d7\u30c8\n            output_data: Output data model type / \u51fa\u529b\u30c7\u30fc\u30bf\u30e2\u30c7\u30eb\u578b\n            clerify_max_turns: Maximum number of clarification turns / \u6700\u5927\u660e\u78ba\u5316\u30bf\u30fc\u30f3\u6570\n            evaluation_instructions: System prompt for evaluation / \u8a55\u4fa1\u7528\u30b7\u30b9\u30c6\u30e0\u30d7\u30ed\u30f3\u30d7\u30c8\n            **kwargs: Additional arguments for AgentPipeline / AgentPipeline\u7528\u8ffd\u52a0\u5f15\u6570\n        \"\"\"\n\n        # English: Store original output data type before wrapping\n        # \u65e5\u672c\u8a9e: \u30e9\u30c3\u30d4\u30f3\u30b0\u524d\u306e\u5143\u306e\u51fa\u529b\u30c7\u30fc\u30bf\u578b\u3092\u4fdd\u5b58\n        self.original_output_data = output_data\n        self.clerify_max_turns = clerify_max_turns\n        self._turn_count = 0\n\n        # English: Create wrapped output model based on provided type\n        # \u65e5\u672c\u8a9e: \u63d0\u4f9b\u3055\u308c\u305f\u578b\u306b\u57fa\u3065\u3044\u3066\u30e9\u30c3\u30d7\u3055\u308c\u305f\u51fa\u529b\u30e2\u30c7\u30eb\u3092\u4f5c\u6210\n        if output_data is not None:\n            # English: For typed output, create generic wrapper\n            # \u65e5\u672c\u8a9e: \u578b\u4ed8\u304d\u51fa\u529b\u306e\u5834\u5408\u3001\u30b8\u30a7\u30cd\u30ea\u30c3\u30af\u30e9\u30c3\u30d1\u30fc\u3092\u4f5c\u6210\n            wrapped_output_model = self._create_wrapped_model(output_data)\n        else:\n            # English: For untyped output, use default string wrapper\n            # \u65e5\u672c\u8a9e: \u578b\u306a\u3057\u51fa\u529b\u306e\u5834\u5408\u3001\u30c7\u30d5\u30a9\u30eb\u30c8\u306e\u6587\u5b57\u5217\u30e9\u30c3\u30d1\u30fc\u3092\u4f7f\u7528\n            wrapped_output_model = Clearify\n\n        # English: Enhanced generation instructions for clarification\n        # \u65e5\u672c\u8a9e: \u660e\u78ba\u5316\u7528\u306e\u62e1\u5f35\u751f\u6210\u6307\u793a\n        enhanced_instructions = self._build_clarification_instructions(\n            generation_instructions, \n            output_data\n        )\n\n        # English: Initialize parent with wrapped output model\n        # \u65e5\u672c\u8a9e: \u30e9\u30c3\u30d7\u3055\u308c\u305f\u51fa\u529b\u30e2\u30c7\u30eb\u3067\u89aa\u30af\u30e9\u30b9\u3092\u521d\u671f\u5316\n        super().__init__(\n            name=name,\n            generation_instructions=enhanced_instructions,\n            evaluation_instructions=evaluation_instructions,\n            output_model=wrapped_output_model,\n            **kwargs\n        )\n\n    def _create_wrapped_model(self, output_data_type: Type[Any]) -&gt; Type[BaseModel]:\n        \"\"\"\n        Create a wrapped output model for the given type\n        \u6307\u5b9a\u3055\u308c\u305f\u578b\u7528\u306e\u30e9\u30c3\u30d7\u3055\u308c\u305f\u51fa\u529b\u30e2\u30c7\u30eb\u3092\u4f5c\u6210\u3059\u308b\n\n        Args:\n            output_data_type: Original output data type / \u5143\u306e\u51fa\u529b\u30c7\u30fc\u30bf\u578b\n\n        Returns:\n            Type[BaseModel]: Wrapped model type / \u30e9\u30c3\u30d7\u3055\u308c\u305f\u30e2\u30c7\u30eb\u578b\n        \"\"\"\n        # English: Create dynamic Pydantic model that wraps the original type\n        # \u65e5\u672c\u8a9e: \u5143\u306e\u578b\u3092\u30e9\u30c3\u30d7\u3059\u308b\u52d5\u7684Pydantic\u30e2\u30c7\u30eb\u3092\u4f5c\u6210\n\n        class WrappedClearify(BaseModel):\n            clearity: bool  # True if requirements are confirmed / \u8981\u4ef6\u304c\u78ba\u5b9a\u3057\u305f\u5834\u5408True\n            user_requirement: Optional[output_data_type] = None  # Confirmed user requirement / \u78ba\u5b9a\u3057\u305f\u30e6\u30fc\u30b6\u30fc\u8981\u6c42\n\n        return WrappedClearify\n\n    def _build_clarification_instructions(\n        self, \n        base_instructions: str, \n        output_data_type: Optional[Type[Any]]\n    ) -&gt; str:\n        \"\"\"\n        Build enhanced instructions for clarification process\n        \u660e\u78ba\u5316\u30d7\u30ed\u30bb\u30b9\u7528\u306e\u62e1\u5f35\u6307\u793a\u3092\u69cb\u7bc9\u3059\u308b\n\n        Args:\n            base_instructions: Base generation instructions / \u30d9\u30fc\u30b9\u751f\u6210\u6307\u793a\n            output_data_type: Output data type for schema reference / \u30b9\u30ad\u30fc\u30de\u53c2\u7167\u7528\u51fa\u529b\u30c7\u30fc\u30bf\u578b\n\n        Returns:\n            str: Enhanced instructions / \u62e1\u5f35\u6307\u793a\n        \"\"\"\n        schema_info = \"\"\n        if output_data_type is not None:\n            try:\n                # English: Try to get schema information if available\n                # \u65e5\u672c\u8a9e: \u5229\u7528\u53ef\u80fd\u306a\u5834\u5408\u306f\u30b9\u30ad\u30fc\u30de\u60c5\u5831\u3092\u53d6\u5f97\u3092\u8a66\u884c\n                if hasattr(output_data_type, 'model_json_schema'):\n                    schema = output_data_type.model_json_schema()\n                    schema_info = f\"\\n\\n\u5fc5\u8981\u306a\u51fa\u529b\u5f62\u5f0f\u306e\u30b9\u30ad\u30fc\u30de:\\n{json.dumps(schema, indent=2, ensure_ascii=False)}\"\n                elif hasattr(output_data_type, '__annotations__'):\n                    annotations = output_data_type.__annotations__\n                    schema_info = f\"\\n\\n\u5fc5\u8981\u306a\u30d5\u30a3\u30fc\u30eb\u30c9: {list(annotations.keys())}\"\n            except Exception:\n                pass\n\n        enhanced_instructions = f\"\"\"{base_instructions}\n\n\u3042\u306a\u305f\u306f\u8981\u4ef6\u660e\u78ba\u5316\u306e\u5c02\u9580\u5bb6\u3067\u3059\u3002\u4ee5\u4e0b\u306e\u624b\u9806\u306b\u5f93\u3063\u3066\u304f\u3060\u3055\u3044\uff1a\n\n1. \u30e6\u30fc\u30b6\u30fc\u306e\u8981\u6c42\u3092\u7406\u89e3\u3057\u3001\u4e0d\u660e\u78ba\u306a\u70b9\u3084\u4e0d\u8db3\u3057\u3066\u3044\u308b\u60c5\u5831\u3092\u7279\u5b9a\u3059\u308b\n2. \u3088\u308a\u826f\u3044\u7d50\u679c\u306e\u305f\u3081\u306b\u5fc5\u8981\u306a\u8ffd\u52a0\u60c5\u5831\u3092\u8cea\u554f\u3059\u308b\n3. \u8981\u4ef6\u304c\u5341\u5206\u306b\u660e\u78ba\u306b\u306a\u3063\u305f\u5834\u5408\u306f\u3001clearity\u3092true\u306b\u8a2d\u5b9a\u3059\u308b\n4. \u8981\u4ef6\u304c\u4e0d\u5341\u5206\u306a\u5834\u5408\u306f\u3001clearity\u3092false\u306b\u8a2d\u5b9a\u3057\u3001\u8ffd\u52a0\u306e\u8cea\u554f\u3092\u884c\u3046\n\n\u51fa\u529b\u5f62\u5f0f\uff1a\n- clearity: \u8981\u4ef6\u304c\u660e\u78ba\u3067\u5b8c\u5168\u306a\u5834\u5408\u306ftrue\u3001\u8ffd\u52a0\u60c5\u5831\u304c\u5fc5\u8981\u306a\u5834\u5408\u306ffalse\n- user_requirement: clearity\u304ctrue\u306e\u5834\u5408\u306e\u307f\u3001\u78ba\u5b9a\u3057\u305f\u8981\u4ef6\u3092\u8a2d\u5b9a\n\n{schema_info}\n\n\u6700\u5927{self.clerify_max_turns}\u56de\u306e\u3084\u308a\u53d6\u308a\u3067\u8981\u4ef6\u3092\u660e\u78ba\u5316\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\"\"\"\n\n        return enhanced_instructions\n\n    def run(self, user_input: str) -&gt; Any:\n        \"\"\"\n        Run the clarification pipeline with user input\n        \u30e6\u30fc\u30b6\u30fc\u5165\u529b\u3067\u660e\u78ba\u5316\u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u3092\u5b9f\u884c\u3059\u308b\n\n        Args:\n            user_input: User input text / \u30e6\u30fc\u30b6\u30fc\u5165\u529b\u30c6\u30ad\u30b9\u30c8\n\n        Returns:\n            Any: Final clarified requirement, clarification question, or None if failed / \u6700\u7d42\u7684\u306a\u660e\u78ba\u5316\u3055\u308c\u305f\u8981\u6c42\u3001\u660e\u78ba\u5316\u8cea\u554f\u3001\u307e\u305f\u306f\u5931\u6557\u6642\u306fNone\n        \"\"\"\n        # English: Reset turn count for new session\n        # \u65e5\u672c\u8a9e: \u65b0\u3057\u3044\u30bb\u30c3\u30b7\u30e7\u30f3\u7528\u306b\u30bf\u30fc\u30f3\u30ab\u30a6\u30f3\u30c8\u3092\u30ea\u30bb\u30c3\u30c8\n        self._turn_count = 0\n        return self._process_input(user_input)\n\n    def continue_clarification(self, user_response: str) -&gt; Any:\n        \"\"\"\n        Continue clarification with user response to previous question\n        \u524d\u306e\u8cea\u554f\u306b\u5bfe\u3059\u308b\u30e6\u30fc\u30b6\u30fc\u56de\u7b54\u3067\u660e\u78ba\u5316\u3092\u7d99\u7d9a\u3059\u308b\n\n        Args:\n            user_response: User response to clarification question / \u660e\u78ba\u5316\u8cea\u554f\u306b\u5bfe\u3059\u308b\u30e6\u30fc\u30b6\u30fc\u56de\u7b54\n\n        Returns:\n            Any: Final clarified requirement, next clarification question, or None if failed / \u6700\u7d42\u7684\u306a\u660e\u78ba\u5316\u3055\u308c\u305f\u8981\u6c42\u3001\u6b21\u306e\u660e\u78ba\u5316\u8cea\u554f\u3001\u307e\u305f\u306f\u5931\u6557\u6642\u306fNone\n        \"\"\"\n        if self._turn_count &gt;= self.clerify_max_turns:\n            # English: Maximum turns reached, cannot continue\n            # \u65e5\u672c\u8a9e: \u6700\u5927\u30bf\u30fc\u30f3\u6570\u306b\u9054\u3057\u305f\u3001\u7d99\u7d9a\u3067\u304d\u306a\u3044\n            return None\n\n        return self._process_input(user_response)\n\n    def _process_input(self, user_input: str) -&gt; Any:\n        \"\"\"\n        Process user input and return result or next question\n        \u30e6\u30fc\u30b6\u30fc\u5165\u529b\u3092\u51e6\u7406\u3057\u3001\u7d50\u679c\u307e\u305f\u306f\u6b21\u306e\u8cea\u554f\u3092\u8fd4\u3059\n\n        Args:\n            user_input: User input text / \u30e6\u30fc\u30b6\u30fc\u5165\u529b\u30c6\u30ad\u30b9\u30c8\n\n        Returns:\n            Any: Final clarified requirement, clarification question, or None if failed / \u6700\u7d42\u7684\u306a\u660e\u78ba\u5316\u3055\u308c\u305f\u8981\u6c42\u3001\u660e\u78ba\u5316\u8cea\u554f\u3001\u307e\u305f\u306f\u5931\u6557\u6642\u306fNone\n        \"\"\"\n        # English: Build context with conversation history\n        # \u65e5\u672c\u8a9e: \u4f1a\u8a71\u5c65\u6b74\u3092\u542b\u3080\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u3092\u69cb\u7bc9\n        if self._turn_count &gt; 0:\n            # English: For subsequent turns, include conversation history\n            # \u65e5\u672c\u8a9e: 2\u56de\u76ee\u4ee5\u964d\u306e\u30bf\u30fc\u30f3\u3067\u306f\u4f1a\u8a71\u5c65\u6b74\u3092\u542b\u3081\u308b\n            conversation_context = self._build_conversation_context()\n            full_input = f\"{conversation_context}\\n\\n\u30e6\u30fc\u30b6\u30fc: {user_input}\"\n        else:\n            full_input = user_input\n\n        # English: Run parent pipeline\n        # \u65e5\u672c\u8a9e: \u89aa\u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u3092\u5b9f\u884c\n        result = super().run(full_input)\n\n        if result is None:\n            # English: Pipeline failed\n            # \u65e5\u672c\u8a9e: \u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u5931\u6557\n            return None\n\n        self._turn_count += 1\n\n        # English: Store this interaction in history\n        # \u65e5\u672c\u8a9e: \u3053\u306e\u5bfe\u8a71\u3092\u5c65\u6b74\u306b\u4fdd\u5b58\n        self._store_interaction(user_input, result)\n\n        # English: Check if clarification is complete\n        # \u65e5\u672c\u8a9e: \u660e\u78ba\u5316\u304c\u5b8c\u4e86\u3057\u3066\u3044\u308b\u304b\u30c1\u30a7\u30c3\u30af\n        if hasattr(result, 'clearity') and result.clearity:\n            # English: Requirements are clear, extract and return final result\n            # \u65e5\u672c\u8a9e: \u8981\u4ef6\u304c\u660e\u78ba\u3001\u6700\u7d42\u7d50\u679c\u3092\u62bd\u51fa\u3057\u3066\u8fd4\u3059\n            if hasattr(result, 'user_requirement') and result.user_requirement:\n                return result.user_requirement\n            else:\n                return result\n\n        # English: Requirements not clear yet, return the clarification question\n        # \u65e5\u672c\u8a9e: \u8981\u4ef6\u304c\u307e\u3060\u660e\u78ba\u3067\u306a\u3044\u3001\u660e\u78ba\u5316\u8cea\u554f\u3092\u8fd4\u3059\n        if hasattr(result, 'user_requirement') and result.user_requirement:\n            # English: Return the clarification question/request for more info\n            # \u65e5\u672c\u8a9e: \u660e\u78ba\u5316\u8cea\u554f/\u8ffd\u52a0\u60c5\u5831\u306e\u8981\u6c42\u3092\u8fd4\u3059\n            return ClarificationQuestion(\n                question=str(result.user_requirement),\n                turn=self._turn_count,\n                remaining_turns=self.remaining_turns\n            )\n\n        # English: No clear question, return the raw result\n        # \u65e5\u672c\u8a9e: \u660e\u78ba\u306a\u8cea\u554f\u304c\u306a\u3044\u3001\u751f\u306e\u7d50\u679c\u3092\u8fd4\u3059\n        return result\n\n    def _build_conversation_context(self) -&gt; str:\n        \"\"\"\n        Build conversation context from history\n        \u5c65\u6b74\u304b\u3089\u4f1a\u8a71\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u3092\u69cb\u7bc9\u3059\u308b\n\n        Returns:\n            str: Formatted conversation context / \u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u6e08\u307f\u4f1a\u8a71\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\n        \"\"\"\n        if not hasattr(self, '_conversation_history'):\n            return \"\"\n\n        context_lines = []\n        for interaction in self._conversation_history:\n            context_lines.append(f\"\u30e6\u30fc\u30b6\u30fc: {interaction['user_input']}\")\n            context_lines.append(f\"AI: {interaction['ai_response']}\")\n\n        return \"\\n\".join(context_lines)\n\n    def _store_interaction(self, user_input: str, ai_result: Any) -&gt; None:\n        \"\"\"\n        Store interaction in conversation history\n        \u4f1a\u8a71\u5c65\u6b74\u306b\u5bfe\u8a71\u3092\u4fdd\u5b58\u3059\u308b\n\n        Args:\n            user_input: User input / \u30e6\u30fc\u30b6\u30fc\u5165\u529b\n            ai_result: AI result / AI\u7d50\u679c\n        \"\"\"\n        if not hasattr(self, '_conversation_history'):\n            self._conversation_history = []\n\n        # English: Extract AI response text\n        # \u65e5\u672c\u8a9e: AI\u5fdc\u7b54\u30c6\u30ad\u30b9\u30c8\u3092\u62bd\u51fa\n        if hasattr(ai_result, 'user_requirement') and ai_result.user_requirement:\n            ai_response = str(ai_result.user_requirement)\n        else:\n            ai_response = str(ai_result)\n\n        self._conversation_history.append({\n            'user_input': user_input,\n            'ai_response': ai_response,\n            'turn': self._turn_count\n        })\n\n        # English: Keep only recent history to avoid context overflow\n        # \u65e5\u672c\u8a9e: \u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u30aa\u30fc\u30d0\u30fc\u30d5\u30ed\u30fc\u3092\u907f\u3051\u308b\u305f\u3081\u6700\u8fd1\u306e\u5c65\u6b74\u306e\u307f\u4fdd\u6301\n        max_history = 10\n        if len(self._conversation_history) &gt; max_history:\n            self._conversation_history = self._conversation_history[-max_history:]\n\n    def reset_turns(self) -&gt; None:\n        \"\"\"\n        Reset the turn counter for a new clarification session\n        \u65b0\u3057\u3044\u660e\u78ba\u5316\u30bb\u30c3\u30b7\u30e7\u30f3\u7528\u306b\u30bf\u30fc\u30f3\u30ab\u30a6\u30f3\u30bf\u30fc\u3092\u30ea\u30bb\u30c3\u30c8\u3059\u308b\n        \"\"\"\n        self._turn_count = 0\n        if hasattr(self, '_conversation_history'):\n            self._conversation_history = []\n\n    def reset_session(self) -&gt; None:\n        \"\"\"\n        Reset the entire session including conversation history\n        \u4f1a\u8a71\u5c65\u6b74\u3092\u542b\u3080\u30bb\u30c3\u30b7\u30e7\u30f3\u5168\u4f53\u3092\u30ea\u30bb\u30c3\u30c8\u3059\u308b\n        \"\"\"\n        self.reset_turns()\n        if hasattr(self, '_conversation_history'):\n            self._conversation_history = []\n\n    @property\n    def is_complete(self) -&gt; bool:\n        \"\"\"\n        Check if clarification process is complete (max turns reached)\n        \u660e\u78ba\u5316\u30d7\u30ed\u30bb\u30b9\u304c\u5b8c\u4e86\u3057\u3066\u3044\u308b\u304b\u30c1\u30a7\u30c3\u30af\uff08\u6700\u5927\u30bf\u30fc\u30f3\u6570\u306b\u9054\u3057\u305f\uff09\n\n        Returns:\n            bool: True if max turns reached / \u6700\u5927\u30bf\u30fc\u30f3\u6570\u306b\u9054\u3057\u305f\u5834\u5408True\n        \"\"\"\n        return self._turn_count &gt;= self.clerify_max_turns\n\n    @property\n    def conversation_history(self) -&gt; List[Dict[str, Any]]:\n        \"\"\"\n        Get the conversation history\n        \u4f1a\u8a71\u5c65\u6b74\u3092\u53d6\u5f97\u3059\u308b\n\n        Returns:\n            List[Dict[str, Any]]: Conversation history / \u4f1a\u8a71\u5c65\u6b74\n        \"\"\"\n        if not hasattr(self, '_conversation_history'):\n            return []\n        return self._conversation_history.copy()\n\n    @property\n    def current_turn(self) -&gt; int:\n        \"\"\"\n        Get the current turn number\n        \u73fe\u5728\u306e\u30bf\u30fc\u30f3\u756a\u53f7\u3092\u53d6\u5f97\u3059\u308b\n\n        Returns:\n            int: Current turn number / \u73fe\u5728\u306e\u30bf\u30fc\u30f3\u756a\u53f7\n        \"\"\"\n        return self._turn_count\n\n    @property\n    def remaining_turns(self) -&gt; int:\n        \"\"\"\n        Get the remaining number of turns\n        \u6b8b\u308a\u306e\u30bf\u30fc\u30f3\u6570\u3092\u53d6\u5f97\u3059\u308b\n\n        Returns:\n            int: Remaining turns / \u6b8b\u308a\u30bf\u30fc\u30f3\u6570\n        \"\"\"\n        return max(0, self.clerify_max_turns - self._turn_count) \n</code></pre>"},{"location":"api_reference/#agents_sdk_models.ClearifyPipeline.conversation_history","title":"<code>conversation_history</code>  <code>property</code>","text":"<p>Get the conversation history \u4f1a\u8a71\u5c65\u6b74\u3092\u53d6\u5f97\u3059\u308b</p> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List[Dict[str, Any]]: Conversation history / \u4f1a\u8a71\u5c65\u6b74</p>"},{"location":"api_reference/#agents_sdk_models.ClearifyPipeline.current_turn","title":"<code>current_turn</code>  <code>property</code>","text":"<p>Get the current turn number \u73fe\u5728\u306e\u30bf\u30fc\u30f3\u756a\u53f7\u3092\u53d6\u5f97\u3059\u308b</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Current turn number / \u73fe\u5728\u306e\u30bf\u30fc\u30f3\u756a\u53f7</p>"},{"location":"api_reference/#agents_sdk_models.ClearifyPipeline.is_complete","title":"<code>is_complete</code>  <code>property</code>","text":"<p>Check if clarification process is complete (max turns reached) \u660e\u78ba\u5316\u30d7\u30ed\u30bb\u30b9\u304c\u5b8c\u4e86\u3057\u3066\u3044\u308b\u304b\u30c1\u30a7\u30c3\u30af\uff08\u6700\u5927\u30bf\u30fc\u30f3\u6570\u306b\u9054\u3057\u305f\uff09</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if max turns reached / \u6700\u5927\u30bf\u30fc\u30f3\u6570\u306b\u9054\u3057\u305f\u5834\u5408True</p>"},{"location":"api_reference/#agents_sdk_models.ClearifyPipeline.remaining_turns","title":"<code>remaining_turns</code>  <code>property</code>","text":"<p>Get the remaining number of turns \u6b8b\u308a\u306e\u30bf\u30fc\u30f3\u6570\u3092\u53d6\u5f97\u3059\u308b</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Remaining turns / \u6b8b\u308a\u30bf\u30fc\u30f3\u6570</p>"},{"location":"api_reference/#agents_sdk_models.ClearifyPipeline.__init__","title":"<code>__init__(name, generation_instructions, output_data=None, clerify_max_turns=20, evaluation_instructions=None, **kwargs)</code>","text":"<p>Initialize the ClearifyPipeline with configuration parameters \u8a2d\u5b9a\u30d1\u30e9\u30e1\u30fc\u30bf\u3067ClearifyPipeline\u3092\u521d\u671f\u5316\u3059\u308b</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Pipeline name / \u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u540d</p> required <code>generation_instructions</code> <code>str</code> <p>System prompt for generation / \u751f\u6210\u7528\u30b7\u30b9\u30c6\u30e0\u30d7\u30ed\u30f3\u30d7\u30c8</p> required <code>output_data</code> <code>Optional[Type[Any]]</code> <p>Output data model type / \u51fa\u529b\u30c7\u30fc\u30bf\u30e2\u30c7\u30eb\u578b</p> <code>None</code> <code>clerify_max_turns</code> <code>int</code> <p>Maximum number of clarification turns / \u6700\u5927\u660e\u78ba\u5316\u30bf\u30fc\u30f3\u6570</p> <code>20</code> <code>evaluation_instructions</code> <code>Optional[str]</code> <p>System prompt for evaluation / \u8a55\u4fa1\u7528\u30b7\u30b9\u30c6\u30e0\u30d7\u30ed\u30f3\u30d7\u30c8</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments for AgentPipeline / AgentPipeline\u7528\u8ffd\u52a0\u5f15\u6570</p> <code>{}</code> Source code in <code>src\\agents_sdk_models\\clearify_pipeline.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    generation_instructions: str,\n    output_data: Optional[Type[Any]] = None,\n    clerify_max_turns: int = 20,\n    evaluation_instructions: Optional[str] = None,\n    **kwargs\n) -&gt; None:\n    \"\"\"\n    Initialize the ClearifyPipeline with configuration parameters\n    \u8a2d\u5b9a\u30d1\u30e9\u30e1\u30fc\u30bf\u3067ClearifyPipeline\u3092\u521d\u671f\u5316\u3059\u308b\n\n    Args:\n        name: Pipeline name / \u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u540d\n        generation_instructions: System prompt for generation / \u751f\u6210\u7528\u30b7\u30b9\u30c6\u30e0\u30d7\u30ed\u30f3\u30d7\u30c8\n        output_data: Output data model type / \u51fa\u529b\u30c7\u30fc\u30bf\u30e2\u30c7\u30eb\u578b\n        clerify_max_turns: Maximum number of clarification turns / \u6700\u5927\u660e\u78ba\u5316\u30bf\u30fc\u30f3\u6570\n        evaluation_instructions: System prompt for evaluation / \u8a55\u4fa1\u7528\u30b7\u30b9\u30c6\u30e0\u30d7\u30ed\u30f3\u30d7\u30c8\n        **kwargs: Additional arguments for AgentPipeline / AgentPipeline\u7528\u8ffd\u52a0\u5f15\u6570\n    \"\"\"\n\n    # English: Store original output data type before wrapping\n    # \u65e5\u672c\u8a9e: \u30e9\u30c3\u30d4\u30f3\u30b0\u524d\u306e\u5143\u306e\u51fa\u529b\u30c7\u30fc\u30bf\u578b\u3092\u4fdd\u5b58\n    self.original_output_data = output_data\n    self.clerify_max_turns = clerify_max_turns\n    self._turn_count = 0\n\n    # English: Create wrapped output model based on provided type\n    # \u65e5\u672c\u8a9e: \u63d0\u4f9b\u3055\u308c\u305f\u578b\u306b\u57fa\u3065\u3044\u3066\u30e9\u30c3\u30d7\u3055\u308c\u305f\u51fa\u529b\u30e2\u30c7\u30eb\u3092\u4f5c\u6210\n    if output_data is not None:\n        # English: For typed output, create generic wrapper\n        # \u65e5\u672c\u8a9e: \u578b\u4ed8\u304d\u51fa\u529b\u306e\u5834\u5408\u3001\u30b8\u30a7\u30cd\u30ea\u30c3\u30af\u30e9\u30c3\u30d1\u30fc\u3092\u4f5c\u6210\n        wrapped_output_model = self._create_wrapped_model(output_data)\n    else:\n        # English: For untyped output, use default string wrapper\n        # \u65e5\u672c\u8a9e: \u578b\u306a\u3057\u51fa\u529b\u306e\u5834\u5408\u3001\u30c7\u30d5\u30a9\u30eb\u30c8\u306e\u6587\u5b57\u5217\u30e9\u30c3\u30d1\u30fc\u3092\u4f7f\u7528\n        wrapped_output_model = Clearify\n\n    # English: Enhanced generation instructions for clarification\n    # \u65e5\u672c\u8a9e: \u660e\u78ba\u5316\u7528\u306e\u62e1\u5f35\u751f\u6210\u6307\u793a\n    enhanced_instructions = self._build_clarification_instructions(\n        generation_instructions, \n        output_data\n    )\n\n    # English: Initialize parent with wrapped output model\n    # \u65e5\u672c\u8a9e: \u30e9\u30c3\u30d7\u3055\u308c\u305f\u51fa\u529b\u30e2\u30c7\u30eb\u3067\u89aa\u30af\u30e9\u30b9\u3092\u521d\u671f\u5316\n    super().__init__(\n        name=name,\n        generation_instructions=enhanced_instructions,\n        evaluation_instructions=evaluation_instructions,\n        output_model=wrapped_output_model,\n        **kwargs\n    )\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.ClearifyPipeline.continue_clarification","title":"<code>continue_clarification(user_response)</code>","text":"<p>Continue clarification with user response to previous question \u524d\u306e\u8cea\u554f\u306b\u5bfe\u3059\u308b\u30e6\u30fc\u30b6\u30fc\u56de\u7b54\u3067\u660e\u78ba\u5316\u3092\u7d99\u7d9a\u3059\u308b</p> <p>Parameters:</p> Name Type Description Default <code>user_response</code> <code>str</code> <p>User response to clarification question / \u660e\u78ba\u5316\u8cea\u554f\u306b\u5bfe\u3059\u308b\u30e6\u30fc\u30b6\u30fc\u56de\u7b54</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>Final clarified requirement, next clarification question, or None if failed / \u6700\u7d42\u7684\u306a\u660e\u78ba\u5316\u3055\u308c\u305f\u8981\u6c42\u3001\u6b21\u306e\u660e\u78ba\u5316\u8cea\u554f\u3001\u307e\u305f\u306f\u5931\u6557\u6642\u306fNone</p> Source code in <code>src\\agents_sdk_models\\clearify_pipeline.py</code> <pre><code>def continue_clarification(self, user_response: str) -&gt; Any:\n    \"\"\"\n    Continue clarification with user response to previous question\n    \u524d\u306e\u8cea\u554f\u306b\u5bfe\u3059\u308b\u30e6\u30fc\u30b6\u30fc\u56de\u7b54\u3067\u660e\u78ba\u5316\u3092\u7d99\u7d9a\u3059\u308b\n\n    Args:\n        user_response: User response to clarification question / \u660e\u78ba\u5316\u8cea\u554f\u306b\u5bfe\u3059\u308b\u30e6\u30fc\u30b6\u30fc\u56de\u7b54\n\n    Returns:\n        Any: Final clarified requirement, next clarification question, or None if failed / \u6700\u7d42\u7684\u306a\u660e\u78ba\u5316\u3055\u308c\u305f\u8981\u6c42\u3001\u6b21\u306e\u660e\u78ba\u5316\u8cea\u554f\u3001\u307e\u305f\u306f\u5931\u6557\u6642\u306fNone\n    \"\"\"\n    if self._turn_count &gt;= self.clerify_max_turns:\n        # English: Maximum turns reached, cannot continue\n        # \u65e5\u672c\u8a9e: \u6700\u5927\u30bf\u30fc\u30f3\u6570\u306b\u9054\u3057\u305f\u3001\u7d99\u7d9a\u3067\u304d\u306a\u3044\n        return None\n\n    return self._process_input(user_response)\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.ClearifyPipeline.reset_session","title":"<code>reset_session()</code>","text":"<p>Reset the entire session including conversation history \u4f1a\u8a71\u5c65\u6b74\u3092\u542b\u3080\u30bb\u30c3\u30b7\u30e7\u30f3\u5168\u4f53\u3092\u30ea\u30bb\u30c3\u30c8\u3059\u308b</p> Source code in <code>src\\agents_sdk_models\\clearify_pipeline.py</code> <pre><code>def reset_session(self) -&gt; None:\n    \"\"\"\n    Reset the entire session including conversation history\n    \u4f1a\u8a71\u5c65\u6b74\u3092\u542b\u3080\u30bb\u30c3\u30b7\u30e7\u30f3\u5168\u4f53\u3092\u30ea\u30bb\u30c3\u30c8\u3059\u308b\n    \"\"\"\n    self.reset_turns()\n    if hasattr(self, '_conversation_history'):\n        self._conversation_history = []\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.ClearifyPipeline.reset_turns","title":"<code>reset_turns()</code>","text":"<p>Reset the turn counter for a new clarification session \u65b0\u3057\u3044\u660e\u78ba\u5316\u30bb\u30c3\u30b7\u30e7\u30f3\u7528\u306b\u30bf\u30fc\u30f3\u30ab\u30a6\u30f3\u30bf\u30fc\u3092\u30ea\u30bb\u30c3\u30c8\u3059\u308b</p> Source code in <code>src\\agents_sdk_models\\clearify_pipeline.py</code> <pre><code>def reset_turns(self) -&gt; None:\n    \"\"\"\n    Reset the turn counter for a new clarification session\n    \u65b0\u3057\u3044\u660e\u78ba\u5316\u30bb\u30c3\u30b7\u30e7\u30f3\u7528\u306b\u30bf\u30fc\u30f3\u30ab\u30a6\u30f3\u30bf\u30fc\u3092\u30ea\u30bb\u30c3\u30c8\u3059\u308b\n    \"\"\"\n    self._turn_count = 0\n    if hasattr(self, '_conversation_history'):\n        self._conversation_history = []\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.ClearifyPipeline.run","title":"<code>run(user_input)</code>","text":"<p>Run the clarification pipeline with user input \u30e6\u30fc\u30b6\u30fc\u5165\u529b\u3067\u660e\u78ba\u5316\u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u3092\u5b9f\u884c\u3059\u308b</p> <p>Parameters:</p> Name Type Description Default <code>user_input</code> <code>str</code> <p>User input text / \u30e6\u30fc\u30b6\u30fc\u5165\u529b\u30c6\u30ad\u30b9\u30c8</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>Final clarified requirement, clarification question, or None if failed / \u6700\u7d42\u7684\u306a\u660e\u78ba\u5316\u3055\u308c\u305f\u8981\u6c42\u3001\u660e\u78ba\u5316\u8cea\u554f\u3001\u307e\u305f\u306f\u5931\u6557\u6642\u306fNone</p> Source code in <code>src\\agents_sdk_models\\clearify_pipeline.py</code> <pre><code>def run(self, user_input: str) -&gt; Any:\n    \"\"\"\n    Run the clarification pipeline with user input\n    \u30e6\u30fc\u30b6\u30fc\u5165\u529b\u3067\u660e\u78ba\u5316\u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u3092\u5b9f\u884c\u3059\u308b\n\n    Args:\n        user_input: User input text / \u30e6\u30fc\u30b6\u30fc\u5165\u529b\u30c6\u30ad\u30b9\u30c8\n\n    Returns:\n        Any: Final clarified requirement, clarification question, or None if failed / \u6700\u7d42\u7684\u306a\u660e\u78ba\u5316\u3055\u308c\u305f\u8981\u6c42\u3001\u660e\u78ba\u5316\u8cea\u554f\u3001\u307e\u305f\u306f\u5931\u6557\u6642\u306fNone\n    \"\"\"\n    # English: Reset turn count for new session\n    # \u65e5\u672c\u8a9e: \u65b0\u3057\u3044\u30bb\u30c3\u30b7\u30e7\u30f3\u7528\u306b\u30bf\u30fc\u30f3\u30ab\u30a6\u30f3\u30c8\u3092\u30ea\u30bb\u30c3\u30c8\n    self._turn_count = 0\n    return self._process_input(user_input)\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.ConditionStep","title":"<code>ConditionStep</code>","text":"<p>               Bases: <code>Step</code></p> <p>Step that performs conditional routing \u6761\u4ef6\u4ed8\u304d\u30eb\u30fc\u30c6\u30a3\u30f3\u30b0\u3092\u5b9f\u884c\u3059\u308b\u30b9\u30c6\u30c3\u30d7</p> <p>This step evaluates a condition and routes to different steps based on the result. \u3053\u306e\u30b9\u30c6\u30c3\u30d7\u306f\u6761\u4ef6\u3092\u8a55\u4fa1\u3057\u3001\u7d50\u679c\u306b\u57fa\u3065\u3044\u3066\u7570\u306a\u308b\u30b9\u30c6\u30c3\u30d7\u306b\u30eb\u30fc\u30c6\u30a3\u30f3\u30b0\u3057\u307e\u3059\u3002</p> Source code in <code>src\\agents_sdk_models\\step.py</code> <pre><code>class ConditionStep(Step):\n    \"\"\"\n    Step that performs conditional routing\n    \u6761\u4ef6\u4ed8\u304d\u30eb\u30fc\u30c6\u30a3\u30f3\u30b0\u3092\u5b9f\u884c\u3059\u308b\u30b9\u30c6\u30c3\u30d7\n\n    This step evaluates a condition and routes to different steps based on the result.\n    \u3053\u306e\u30b9\u30c6\u30c3\u30d7\u306f\u6761\u4ef6\u3092\u8a55\u4fa1\u3057\u3001\u7d50\u679c\u306b\u57fa\u3065\u3044\u3066\u7570\u306a\u308b\u30b9\u30c6\u30c3\u30d7\u306b\u30eb\u30fc\u30c6\u30a3\u30f3\u30b0\u3057\u307e\u3059\u3002\n    \"\"\"\n\n    def __init__(\n        self, \n        name: str, \n        condition: Callable[[Context], Union[bool, Awaitable[bool]]], \n        if_true: str, \n        if_false: str\n    ):\n        \"\"\"\n        Initialize condition step\n        \u6761\u4ef6\u30b9\u30c6\u30c3\u30d7\u3092\u521d\u671f\u5316\n\n        Args:\n            name: Step name / \u30b9\u30c6\u30c3\u30d7\u540d\n            condition: Condition function / \u6761\u4ef6\u95a2\u6570\n            if_true: Step to go if condition is True / \u6761\u4ef6\u304cTrue\u306e\u5834\u5408\u306e\u30b9\u30c6\u30c3\u30d7\n            if_false: Step to go if condition is False / \u6761\u4ef6\u304cFalse\u306e\u5834\u5408\u306e\u30b9\u30c6\u30c3\u30d7\n        \"\"\"\n        super().__init__(name)\n        self.condition = condition\n        self.if_true = if_true\n        self.if_false = if_false\n\n    async def run(self, user_input: Optional[str], ctx: Context) -&gt; Context:\n        \"\"\"\n        Execute condition step\n        \u6761\u4ef6\u30b9\u30c6\u30c3\u30d7\u3092\u5b9f\u884c\n\n        Args:\n            user_input: User input (not used) / \u30e6\u30fc\u30b6\u30fc\u5165\u529b\uff08\u4f7f\u7528\u3055\u308c\u306a\u3044\uff09\n            ctx: Current context / \u73fe\u5728\u306e\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\n\n        Returns:\n            Context: Updated context with routing / \u30eb\u30fc\u30c6\u30a3\u30f3\u30b0\u4ed8\u304d\u66f4\u65b0\u6e08\u307f\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\n        \"\"\"\n        ctx.update_step_info(self.name)\n\n        # Evaluate condition (may be async)\n        # \u6761\u4ef6\u3092\u8a55\u4fa1\uff08\u975e\u540c\u671f\u306e\u53ef\u80fd\u6027\u3042\u308a\uff09\n        try:\n            result = self.condition(ctx)\n            if asyncio.iscoroutine(result):\n                result = await result\n        except Exception as e:\n            # On error, go to false branch\n            # \u30a8\u30e9\u30fc\u6642\u306ffalse\u30d6\u30e9\u30f3\u30c1\u306b\u9032\u3080\n            ctx.add_system_message(f\"Condition evaluation error: {e}\")\n            result = False\n\n        # Route based on condition result\n        # \u6761\u4ef6\u7d50\u679c\u306b\u57fa\u3065\u3044\u3066\u30eb\u30fc\u30c6\u30a3\u30f3\u30b0\n        next_step = self.if_true if result else self.if_false\n        ctx.goto(next_step)\n\n        return ctx\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.ConditionStep.__init__","title":"<code>__init__(name, condition, if_true, if_false)</code>","text":"<p>Initialize condition step \u6761\u4ef6\u30b9\u30c6\u30c3\u30d7\u3092\u521d\u671f\u5316</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Step name / \u30b9\u30c6\u30c3\u30d7\u540d</p> required <code>condition</code> <code>Callable[[Context], Union[bool, Awaitable[bool]]]</code> <p>Condition function / \u6761\u4ef6\u95a2\u6570</p> required <code>if_true</code> <code>str</code> <p>Step to go if condition is True / \u6761\u4ef6\u304cTrue\u306e\u5834\u5408\u306e\u30b9\u30c6\u30c3\u30d7</p> required <code>if_false</code> <code>str</code> <p>Step to go if condition is False / \u6761\u4ef6\u304cFalse\u306e\u5834\u5408\u306e\u30b9\u30c6\u30c3\u30d7</p> required Source code in <code>src\\agents_sdk_models\\step.py</code> <pre><code>def __init__(\n    self, \n    name: str, \n    condition: Callable[[Context], Union[bool, Awaitable[bool]]], \n    if_true: str, \n    if_false: str\n):\n    \"\"\"\n    Initialize condition step\n    \u6761\u4ef6\u30b9\u30c6\u30c3\u30d7\u3092\u521d\u671f\u5316\n\n    Args:\n        name: Step name / \u30b9\u30c6\u30c3\u30d7\u540d\n        condition: Condition function / \u6761\u4ef6\u95a2\u6570\n        if_true: Step to go if condition is True / \u6761\u4ef6\u304cTrue\u306e\u5834\u5408\u306e\u30b9\u30c6\u30c3\u30d7\n        if_false: Step to go if condition is False / \u6761\u4ef6\u304cFalse\u306e\u5834\u5408\u306e\u30b9\u30c6\u30c3\u30d7\n    \"\"\"\n    super().__init__(name)\n    self.condition = condition\n    self.if_true = if_true\n    self.if_false = if_false\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.ConditionStep.run","title":"<code>run(user_input, ctx)</code>  <code>async</code>","text":"<p>Execute condition step \u6761\u4ef6\u30b9\u30c6\u30c3\u30d7\u3092\u5b9f\u884c</p> <p>Parameters:</p> Name Type Description Default <code>user_input</code> <code>Optional[str]</code> <p>User input (not used) / \u30e6\u30fc\u30b6\u30fc\u5165\u529b\uff08\u4f7f\u7528\u3055\u308c\u306a\u3044\uff09</p> required <code>ctx</code> <code>Context</code> <p>Current context / \u73fe\u5728\u306e\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8</p> required <p>Returns:</p> Name Type Description <code>Context</code> <code>Context</code> <p>Updated context with routing / \u30eb\u30fc\u30c6\u30a3\u30f3\u30b0\u4ed8\u304d\u66f4\u65b0\u6e08\u307f\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8</p> Source code in <code>src\\agents_sdk_models\\step.py</code> <pre><code>async def run(self, user_input: Optional[str], ctx: Context) -&gt; Context:\n    \"\"\"\n    Execute condition step\n    \u6761\u4ef6\u30b9\u30c6\u30c3\u30d7\u3092\u5b9f\u884c\n\n    Args:\n        user_input: User input (not used) / \u30e6\u30fc\u30b6\u30fc\u5165\u529b\uff08\u4f7f\u7528\u3055\u308c\u306a\u3044\uff09\n        ctx: Current context / \u73fe\u5728\u306e\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\n\n    Returns:\n        Context: Updated context with routing / \u30eb\u30fc\u30c6\u30a3\u30f3\u30b0\u4ed8\u304d\u66f4\u65b0\u6e08\u307f\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\n    \"\"\"\n    ctx.update_step_info(self.name)\n\n    # Evaluate condition (may be async)\n    # \u6761\u4ef6\u3092\u8a55\u4fa1\uff08\u975e\u540c\u671f\u306e\u53ef\u80fd\u6027\u3042\u308a\uff09\n    try:\n        result = self.condition(ctx)\n        if asyncio.iscoroutine(result):\n            result = await result\n    except Exception as e:\n        # On error, go to false branch\n        # \u30a8\u30e9\u30fc\u6642\u306ffalse\u30d6\u30e9\u30f3\u30c1\u306b\u9032\u3080\n        ctx.add_system_message(f\"Condition evaluation error: {e}\")\n        result = False\n\n    # Route based on condition result\n    # \u6761\u4ef6\u7d50\u679c\u306b\u57fa\u3065\u3044\u3066\u30eb\u30fc\u30c6\u30a3\u30f3\u30b0\n    next_step = self.if_true if result else self.if_false\n    ctx.goto(next_step)\n\n    return ctx\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.Context","title":"<code>Context</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Context class for Flow/Step workflow state management \u30d5\u30ed\u30fc/\u30b9\u30c6\u30c3\u30d7\u30ef\u30fc\u30af\u30d5\u30ed\u30fc\u72b6\u614b\u7ba1\u7406\u7528\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u30af\u30e9\u30b9</p> <p>This class provides: \u3053\u306e\u30af\u30e9\u30b9\u306f\u4ee5\u4e0b\u3092\u63d0\u4f9b\u3057\u307e\u3059\uff1a - Type-safe shared state / \u578b\u5b89\u5168\u306a\u5171\u6709\u72b6\u614b - Conversation history management / \u4f1a\u8a71\u5c65\u6b74\u7ba1\u7406 - Step routing control / \u30b9\u30c6\u30c3\u30d7\u30eb\u30fc\u30c6\u30a3\u30f3\u30b0\u5236\u5fa1 - LangChain LCEL compatibility / LangChain LCEL\u4e92\u63db\u6027 - User input/output coordination / \u30e6\u30fc\u30b6\u30fc\u5165\u51fa\u529b\u8abf\u6574</p> Source code in <code>src\\agents_sdk_models\\context.py</code> <pre><code>class Context(BaseModel):\n    \"\"\"\n    Context class for Flow/Step workflow state management\n    \u30d5\u30ed\u30fc/\u30b9\u30c6\u30c3\u30d7\u30ef\u30fc\u30af\u30d5\u30ed\u30fc\u72b6\u614b\u7ba1\u7406\u7528\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u30af\u30e9\u30b9\n\n    This class provides:\n    \u3053\u306e\u30af\u30e9\u30b9\u306f\u4ee5\u4e0b\u3092\u63d0\u4f9b\u3057\u307e\u3059\uff1a\n    - Type-safe shared state / \u578b\u5b89\u5168\u306a\u5171\u6709\u72b6\u614b\n    - Conversation history management / \u4f1a\u8a71\u5c65\u6b74\u7ba1\u7406\n    - Step routing control / \u30b9\u30c6\u30c3\u30d7\u30eb\u30fc\u30c6\u30a3\u30f3\u30b0\u5236\u5fa1\n    - LangChain LCEL compatibility / LangChain LCEL\u4e92\u63db\u6027\n    - User input/output coordination / \u30e6\u30fc\u30b6\u30fc\u5165\u51fa\u529b\u8abf\u6574\n    \"\"\"\n\n    # Core state / \u30b3\u30a2\u72b6\u614b\n    last_user_input: Optional[str] = None  # Most recent user input / \u76f4\u8fd1\u306e\u30e6\u30fc\u30b6\u30fc\u5165\u529b\n    messages: List[Message] = Field(default_factory=list)  # Conversation history / \u4f1a\u8a71\u5c65\u6b74\n\n    # External data / \u5916\u90e8\u30c7\u30fc\u30bf\n    knowledge: Dict[str, Any] = Field(default_factory=dict)  # External knowledge (RAG, etc.) / \u5916\u90e8\u77e5\u8b58\uff08RAG\u306a\u3069\uff09\n    prev_outputs: Dict[str, Any] = Field(default_factory=dict)  # Previous step outputs / \u524d\u30b9\u30c6\u30c3\u30d7\u306e\u51fa\u529b\n\n    # Flow control / \u30d5\u30ed\u30fc\u5236\u5fa1\n    next_label: Optional[str] = None  # Next step routing instruction / \u6b21\u30b9\u30c6\u30c3\u30d7\u306e\u30eb\u30fc\u30c6\u30a3\u30f3\u30b0\u6307\u793a\n    current_step: Optional[str] = None  # Current step name / \u73fe\u5728\u306e\u30b9\u30c6\u30c3\u30d7\u540d\n\n    # Results / \u7d50\u679c\n    artifacts: Dict[str, Any] = Field(default_factory=dict)  # Flow-wide artifacts / \u30d5\u30ed\u30fc\u5168\u4f53\u306e\u6210\u679c\u7269\n    shared_state: Dict[str, Any] = Field(default_factory=dict)  # Arbitrary shared values / \u4efb\u610f\u306e\u5171\u6709\u5024\n\n    # User interaction / \u30e6\u30fc\u30b6\u30fc\u5bfe\u8a71\n    awaiting_prompt: Optional[str] = None  # Prompt waiting for user input / \u30e6\u30fc\u30b6\u30fc\u5165\u529b\u5f85\u3061\u306e\u30d7\u30ed\u30f3\u30d7\u30c8\n    awaiting_user_input: bool = False  # Flag indicating waiting for user input / \u30e6\u30fc\u30b6\u30fc\u5165\u529b\u5f85\u3061\u30d5\u30e9\u30b0\n\n    # Execution metadata / \u5b9f\u884c\u30e1\u30bf\u30c7\u30fc\u30bf\n    trace_id: Optional[str] = None  # Trace ID for observability / \u30aa\u30d6\u30b6\u30fc\u30d0\u30d3\u30ea\u30c6\u30a3\u7528\u30c8\u30ec\u30fc\u30b9ID\n    start_time: datetime = Field(default_factory=datetime.now)  # Flow start time / \u30d5\u30ed\u30fc\u958b\u59cb\u6642\u523b\n    step_count: int = 0  # Number of steps executed / \u5b9f\u884c\u3055\u308c\u305f\u30b9\u30c6\u30c3\u30d7\u6570\n\n    # Internal async coordination (private attributes) / \u5185\u90e8\u975e\u540c\u671f\u8abf\u6574\uff08\u30d7\u30e9\u30a4\u30d9\u30fc\u30c8\u5c5e\u6027\uff09\n    _user_input_event: Optional[asyncio.Event] = PrivateAttr(default=None)\n    _awaiting_prompt_event: Optional[asyncio.Event] = PrivateAttr(default=None)\n\n    def __init__(self, **data):\n        \"\"\"\n        Initialize Context with async events\n        \u975e\u540c\u671f\u30a4\u30d9\u30f3\u30c8\u3067Context\u3092\u521d\u671f\u5316\n        \"\"\"\n        super().__init__(**data)\n        self._user_input_event = asyncio.Event()\n        self._awaiting_prompt_event = asyncio.Event()\n\n    def add_user_message(self, content: str, metadata: Optional[Dict[str, Any]] = None) -&gt; None:\n        \"\"\"\n        Add user message to conversation history\n        \u30e6\u30fc\u30b6\u30fc\u30e1\u30c3\u30bb\u30fc\u30b8\u3092\u4f1a\u8a71\u5c65\u6b74\u306b\u8ffd\u52a0\n\n        Args:\n            content: Message content / \u30e1\u30c3\u30bb\u30fc\u30b8\u5185\u5bb9\n            metadata: Additional metadata / \u8ffd\u52a0\u30e1\u30bf\u30c7\u30fc\u30bf\n        \"\"\"\n        message = Message(\n            role=\"user\",\n            content=content,\n            metadata=metadata or {}\n        )\n        self.messages.append(message)\n        self.last_user_input = content\n\n    def add_assistant_message(self, content: str, metadata: Optional[Dict[str, Any]] = None) -&gt; None:\n        \"\"\"\n        Add assistant message to conversation history\n        \u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u30e1\u30c3\u30bb\u30fc\u30b8\u3092\u4f1a\u8a71\u5c65\u6b74\u306b\u8ffd\u52a0\n\n        Args:\n            content: Message content / \u30e1\u30c3\u30bb\u30fc\u30b8\u5185\u5bb9\n            metadata: Additional metadata / \u8ffd\u52a0\u30e1\u30bf\u30c7\u30fc\u30bf\n        \"\"\"\n        message = Message(\n            role=\"assistant\",\n            content=content,\n            metadata=metadata or {}\n        )\n        self.messages.append(message)\n\n    def add_system_message(self, content: str, metadata: Optional[Dict[str, Any]] = None) -&gt; None:\n        \"\"\"\n        Add system message to conversation history\n        \u30b7\u30b9\u30c6\u30e0\u30e1\u30c3\u30bb\u30fc\u30b8\u3092\u4f1a\u8a71\u5c65\u6b74\u306b\u8ffd\u52a0\n\n        Args:\n            content: Message content / \u30e1\u30c3\u30bb\u30fc\u30b8\u5185\u5bb9\n            metadata: Additional metadata / \u8ffd\u52a0\u30e1\u30bf\u30c7\u30fc\u30bf\n        \"\"\"\n        message = Message(\n            role=\"system\",\n            content=content,\n            metadata=metadata or {}\n        )\n        self.messages.append(message)\n\n    def set_waiting_for_user_input(self, prompt: str) -&gt; None:\n        \"\"\"\n        Set context to wait for user input with a prompt\n        \u30d7\u30ed\u30f3\u30d7\u30c8\u3067\u30e6\u30fc\u30b6\u30fc\u5165\u529b\u5f85\u3061\u72b6\u614b\u306b\u8a2d\u5b9a\n\n        Args:\n            prompt: Prompt to display to user / \u30e6\u30fc\u30b6\u30fc\u306b\u8868\u793a\u3059\u308b\u30d7\u30ed\u30f3\u30d7\u30c8\n        \"\"\"\n        self.awaiting_prompt = prompt\n        self.awaiting_user_input = True\n        if self._awaiting_prompt_event:\n            self._awaiting_prompt_event.set()\n\n    def provide_user_input(self, user_input: str) -&gt; None:\n        \"\"\"\n        Provide user input and clear waiting state\n        \u30e6\u30fc\u30b6\u30fc\u5165\u529b\u3092\u63d0\u4f9b\u3057\u3001\u5f85\u3061\u72b6\u614b\u3092\u30af\u30ea\u30a2\n\n        Args:\n            user_input: User input text / \u30e6\u30fc\u30b6\u30fc\u5165\u529b\u30c6\u30ad\u30b9\u30c8\n        \"\"\"\n        self.add_user_message(user_input)\n        self.awaiting_prompt = None\n        self.awaiting_user_input = False\n        if self._user_input_event:\n            self._user_input_event.set()\n\n    def clear_prompt(self) -&gt; Optional[str]:\n        \"\"\"\n        Clear and return the current prompt\n        \u73fe\u5728\u306e\u30d7\u30ed\u30f3\u30d7\u30c8\u3092\u30af\u30ea\u30a2\u3057\u3066\u8fd4\u3059\n\n        Returns:\n            str | None: The prompt if one was waiting / \u5f85\u6a5f\u4e2d\u3060\u3063\u305f\u30d7\u30ed\u30f3\u30d7\u30c8\n        \"\"\"\n        prompt = self.awaiting_prompt\n        self.awaiting_prompt = None\n        if self._awaiting_prompt_event:\n            self._awaiting_prompt_event.clear()\n        return prompt\n\n    async def wait_for_user_input(self) -&gt; str:\n        \"\"\"\n        Async wait for user input\n        \u30e6\u30fc\u30b6\u30fc\u5165\u529b\u3092\u975e\u540c\u671f\u3067\u5f85\u6a5f\n\n        Returns:\n            str: User input / \u30e6\u30fc\u30b6\u30fc\u5165\u529b\n        \"\"\"\n        if self._user_input_event:\n            await self._user_input_event.wait()\n            self._user_input_event.clear()\n        return self.last_user_input or \"\"\n\n    async def wait_for_prompt_event(self) -&gt; str:\n        \"\"\"\n        Async wait for prompt event\n        \u30d7\u30ed\u30f3\u30d7\u30c8\u30a4\u30d9\u30f3\u30c8\u3092\u975e\u540c\u671f\u3067\u5f85\u6a5f\n\n        Returns:\n            str: Prompt waiting for user / \u30e6\u30fc\u30b6\u30fc\u5f85\u3061\u306e\u30d7\u30ed\u30f3\u30d7\u30c8\n        \"\"\"\n        if self._awaiting_prompt_event:\n            await self._awaiting_prompt_event.wait()\n        return self.awaiting_prompt or \"\"\n\n    def goto(self, label: str) -&gt; None:\n        \"\"\"\n        Set next step routing\n        \u6b21\u30b9\u30c6\u30c3\u30d7\u306e\u30eb\u30fc\u30c6\u30a3\u30f3\u30b0\u3092\u8a2d\u5b9a\n\n        Args:\n            label: Next step label / \u6b21\u30b9\u30c6\u30c3\u30d7\u306e\u30e9\u30d9\u30eb\n        \"\"\"\n        self.next_label = label\n\n    def finish(self) -&gt; None:\n        \"\"\"\n        Mark flow as finished\n        \u30d5\u30ed\u30fc\u3092\u5b8c\u4e86\u3068\u3057\u3066\u30de\u30fc\u30af\n        \"\"\"\n        self.next_label = None\n\n    def is_finished(self) -&gt; bool:\n        \"\"\"\n        Check if flow is finished\n        \u30d5\u30ed\u30fc\u304c\u5b8c\u4e86\u3057\u3066\u3044\u308b\u304b\u30c1\u30a7\u30c3\u30af\n\n        Returns:\n            bool: True if finished / \u5b8c\u4e86\u3057\u3066\u3044\u308b\u5834\u5408True\n        \"\"\"\n        return self.next_label is None\n\n    def as_dict(self) -&gt; Dict[str, Any]:\n        \"\"\"\n        Convert to dictionary for LangChain LCEL compatibility\n        LangChain LCEL\u4e92\u63db\u6027\u306e\u305f\u3081\u306b\u8f9e\u66f8\u306b\u5909\u63db\n\n        Returns:\n            Dict[str, Any]: Dictionary representation / \u8f9e\u66f8\u8868\u73fe\n        \"\"\"\n        data = self.dict()\n        # Convert messages to LangChain format\n        # \u30e1\u30c3\u30bb\u30fc\u30b8\u3092LangChain\u5f62\u5f0f\u306b\u5909\u63db\n        data[\"history\"] = [\n            {\"role\": msg.role, \"content\": msg.content, \"metadata\": msg.metadata}\n            for msg in self.messages\n        ]\n        data.pop(\"messages\", None)\n        return data\n\n    @classmethod\n    def from_dict(cls, data: Dict[str, Any]) -&gt; \"Context\":\n        \"\"\"\n        Create Context from dictionary (LangChain LCEL compatibility)\n        \u8f9e\u66f8\u304b\u3089Context\u3092\u4f5c\u6210\uff08LangChain LCEL\u4e92\u63db\u6027\uff09\n\n        Args:\n            data: Dictionary data / \u8f9e\u66f8\u30c7\u30fc\u30bf\n\n        Returns:\n            Context: New context instance / \u65b0\u3057\u3044\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\n        \"\"\"\n        data = data.copy()\n        # Convert history to messages\n        # \u5c65\u6b74\u3092\u30e1\u30c3\u30bb\u30fc\u30b8\u306b\u5909\u63db\n        history = data.pop(\"history\", [])\n        messages = []\n        for msg_data in history:\n            if isinstance(msg_data, dict):\n                messages.append(Message(\n                    role=msg_data.get(\"role\", \"user\"),\n                    content=msg_data.get(\"content\", \"\"),\n                    metadata=msg_data.get(\"metadata\", {})\n                ))\n        data[\"messages\"] = messages\n        return cls(**data)\n\n    def get_conversation_text(self, include_system: bool = False) -&gt; str:\n        \"\"\"\n        Get conversation as formatted text\n        \u4f1a\u8a71\u3092\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u6e08\u307f\u30c6\u30ad\u30b9\u30c8\u3068\u3057\u3066\u53d6\u5f97\n\n        Args:\n            include_system: Include system messages / \u30b7\u30b9\u30c6\u30e0\u30e1\u30c3\u30bb\u30fc\u30b8\u3092\u542b\u3081\u308b\n\n        Returns:\n            str: Formatted conversation / \u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u6e08\u307f\u4f1a\u8a71\n        \"\"\"\n        lines = []\n        for msg in self.messages:\n            if not include_system and msg.role == \"system\":\n                continue\n            role_label = {\"user\": \"\ud83d\udc64\", \"assistant\": \"\ud83e\udd16\", \"system\": \"\u2699\ufe0f\"}.get(msg.role, msg.role)\n            lines.append(f\"{role_label} {msg.content}\")\n        return \"\\n\".join(lines)\n\n    def get_last_messages(self, n: int = 10) -&gt; List[Message]:\n        \"\"\"\n        Get last N messages\n        \u6700\u5f8c\u306eN\u30e1\u30c3\u30bb\u30fc\u30b8\u3092\u53d6\u5f97\n\n        Args:\n            n: Number of messages / \u30e1\u30c3\u30bb\u30fc\u30b8\u6570\n\n        Returns:\n            List[Message]: Last N messages / \u6700\u5f8c\u306eN\u30e1\u30c3\u30bb\u30fc\u30b8\n        \"\"\"\n        return self.messages[-n:] if len(self.messages) &gt; n else self.messages.copy()\n\n    def update_step_info(self, step_name: str) -&gt; None:\n        \"\"\"\n        Update current step information\n        \u73fe\u5728\u306e\u30b9\u30c6\u30c3\u30d7\u60c5\u5831\u3092\u66f4\u65b0\n\n        Args:\n            step_name: Current step name / \u73fe\u5728\u306e\u30b9\u30c6\u30c3\u30d7\u540d\n        \"\"\"\n        self.current_step = step_name\n        self.step_count += 1\n\n    def set_artifact(self, key: str, value: Any) -&gt; None:\n        \"\"\"\n        Set artifact value\n        \u6210\u679c\u7269\u306e\u5024\u3092\u8a2d\u5b9a\n\n        Args:\n            key: Artifact key / \u6210\u679c\u7269\u30ad\u30fc\n            value: Artifact value / \u6210\u679c\u7269\u5024\n        \"\"\"\n        self.artifacts[key] = value\n\n    def get_artifact(self, key: str, default: Any = None) -&gt; Any:\n        \"\"\"\n        Get artifact value\n        \u6210\u679c\u7269\u306e\u5024\u3092\u53d6\u5f97\n\n        Args:\n            key: Artifact key / \u6210\u679c\u7269\u30ad\u30fc\n            default: Default value if not found / \u898b\u3064\u304b\u3089\u306a\u3044\u5834\u5408\u306e\u30c7\u30d5\u30a9\u30eb\u30c8\u5024\n\n        Returns:\n            Any: Artifact value / \u6210\u679c\u7269\u5024\n        \"\"\"\n        return self.artifacts.get(key, default)\n\n    class Config:\n        # Allow arbitrary types for flexibility\n        # \u67d4\u8edf\u6027\u306e\u305f\u3081\u306b\u4efb\u610f\u306e\u578b\u3092\u8a31\u53ef\n        arbitrary_types_allowed = True \n</code></pre>"},{"location":"api_reference/#agents_sdk_models.Context.__init__","title":"<code>__init__(**data)</code>","text":"<p>Initialize Context with async events \u975e\u540c\u671f\u30a4\u30d9\u30f3\u30c8\u3067Context\u3092\u521d\u671f\u5316</p> Source code in <code>src\\agents_sdk_models\\context.py</code> <pre><code>def __init__(self, **data):\n    \"\"\"\n    Initialize Context with async events\n    \u975e\u540c\u671f\u30a4\u30d9\u30f3\u30c8\u3067Context\u3092\u521d\u671f\u5316\n    \"\"\"\n    super().__init__(**data)\n    self._user_input_event = asyncio.Event()\n    self._awaiting_prompt_event = asyncio.Event()\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.Context.add_assistant_message","title":"<code>add_assistant_message(content, metadata=None)</code>","text":"<p>Add assistant message to conversation history \u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u30e1\u30c3\u30bb\u30fc\u30b8\u3092\u4f1a\u8a71\u5c65\u6b74\u306b\u8ffd\u52a0</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>Message content / \u30e1\u30c3\u30bb\u30fc\u30b8\u5185\u5bb9</p> required <code>metadata</code> <code>Optional[Dict[str, Any]]</code> <p>Additional metadata / \u8ffd\u52a0\u30e1\u30bf\u30c7\u30fc\u30bf</p> <code>None</code> Source code in <code>src\\agents_sdk_models\\context.py</code> <pre><code>def add_assistant_message(self, content: str, metadata: Optional[Dict[str, Any]] = None) -&gt; None:\n    \"\"\"\n    Add assistant message to conversation history\n    \u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u30e1\u30c3\u30bb\u30fc\u30b8\u3092\u4f1a\u8a71\u5c65\u6b74\u306b\u8ffd\u52a0\n\n    Args:\n        content: Message content / \u30e1\u30c3\u30bb\u30fc\u30b8\u5185\u5bb9\n        metadata: Additional metadata / \u8ffd\u52a0\u30e1\u30bf\u30c7\u30fc\u30bf\n    \"\"\"\n    message = Message(\n        role=\"assistant\",\n        content=content,\n        metadata=metadata or {}\n    )\n    self.messages.append(message)\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.Context.add_system_message","title":"<code>add_system_message(content, metadata=None)</code>","text":"<p>Add system message to conversation history \u30b7\u30b9\u30c6\u30e0\u30e1\u30c3\u30bb\u30fc\u30b8\u3092\u4f1a\u8a71\u5c65\u6b74\u306b\u8ffd\u52a0</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>Message content / \u30e1\u30c3\u30bb\u30fc\u30b8\u5185\u5bb9</p> required <code>metadata</code> <code>Optional[Dict[str, Any]]</code> <p>Additional metadata / \u8ffd\u52a0\u30e1\u30bf\u30c7\u30fc\u30bf</p> <code>None</code> Source code in <code>src\\agents_sdk_models\\context.py</code> <pre><code>def add_system_message(self, content: str, metadata: Optional[Dict[str, Any]] = None) -&gt; None:\n    \"\"\"\n    Add system message to conversation history\n    \u30b7\u30b9\u30c6\u30e0\u30e1\u30c3\u30bb\u30fc\u30b8\u3092\u4f1a\u8a71\u5c65\u6b74\u306b\u8ffd\u52a0\n\n    Args:\n        content: Message content / \u30e1\u30c3\u30bb\u30fc\u30b8\u5185\u5bb9\n        metadata: Additional metadata / \u8ffd\u52a0\u30e1\u30bf\u30c7\u30fc\u30bf\n    \"\"\"\n    message = Message(\n        role=\"system\",\n        content=content,\n        metadata=metadata or {}\n    )\n    self.messages.append(message)\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.Context.add_user_message","title":"<code>add_user_message(content, metadata=None)</code>","text":"<p>Add user message to conversation history \u30e6\u30fc\u30b6\u30fc\u30e1\u30c3\u30bb\u30fc\u30b8\u3092\u4f1a\u8a71\u5c65\u6b74\u306b\u8ffd\u52a0</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>Message content / \u30e1\u30c3\u30bb\u30fc\u30b8\u5185\u5bb9</p> required <code>metadata</code> <code>Optional[Dict[str, Any]]</code> <p>Additional metadata / \u8ffd\u52a0\u30e1\u30bf\u30c7\u30fc\u30bf</p> <code>None</code> Source code in <code>src\\agents_sdk_models\\context.py</code> <pre><code>def add_user_message(self, content: str, metadata: Optional[Dict[str, Any]] = None) -&gt; None:\n    \"\"\"\n    Add user message to conversation history\n    \u30e6\u30fc\u30b6\u30fc\u30e1\u30c3\u30bb\u30fc\u30b8\u3092\u4f1a\u8a71\u5c65\u6b74\u306b\u8ffd\u52a0\n\n    Args:\n        content: Message content / \u30e1\u30c3\u30bb\u30fc\u30b8\u5185\u5bb9\n        metadata: Additional metadata / \u8ffd\u52a0\u30e1\u30bf\u30c7\u30fc\u30bf\n    \"\"\"\n    message = Message(\n        role=\"user\",\n        content=content,\n        metadata=metadata or {}\n    )\n    self.messages.append(message)\n    self.last_user_input = content\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.Context.as_dict","title":"<code>as_dict()</code>","text":"<p>Convert to dictionary for LangChain LCEL compatibility LangChain LCEL\u4e92\u63db\u6027\u306e\u305f\u3081\u306b\u8f9e\u66f8\u306b\u5909\u63db</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: Dictionary representation / \u8f9e\u66f8\u8868\u73fe</p> Source code in <code>src\\agents_sdk_models\\context.py</code> <pre><code>def as_dict(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Convert to dictionary for LangChain LCEL compatibility\n    LangChain LCEL\u4e92\u63db\u6027\u306e\u305f\u3081\u306b\u8f9e\u66f8\u306b\u5909\u63db\n\n    Returns:\n        Dict[str, Any]: Dictionary representation / \u8f9e\u66f8\u8868\u73fe\n    \"\"\"\n    data = self.dict()\n    # Convert messages to LangChain format\n    # \u30e1\u30c3\u30bb\u30fc\u30b8\u3092LangChain\u5f62\u5f0f\u306b\u5909\u63db\n    data[\"history\"] = [\n        {\"role\": msg.role, \"content\": msg.content, \"metadata\": msg.metadata}\n        for msg in self.messages\n    ]\n    data.pop(\"messages\", None)\n    return data\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.Context.clear_prompt","title":"<code>clear_prompt()</code>","text":"<p>Clear and return the current prompt \u73fe\u5728\u306e\u30d7\u30ed\u30f3\u30d7\u30c8\u3092\u30af\u30ea\u30a2\u3057\u3066\u8fd4\u3059</p> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>str | None: The prompt if one was waiting / \u5f85\u6a5f\u4e2d\u3060\u3063\u305f\u30d7\u30ed\u30f3\u30d7\u30c8</p> Source code in <code>src\\agents_sdk_models\\context.py</code> <pre><code>def clear_prompt(self) -&gt; Optional[str]:\n    \"\"\"\n    Clear and return the current prompt\n    \u73fe\u5728\u306e\u30d7\u30ed\u30f3\u30d7\u30c8\u3092\u30af\u30ea\u30a2\u3057\u3066\u8fd4\u3059\n\n    Returns:\n        str | None: The prompt if one was waiting / \u5f85\u6a5f\u4e2d\u3060\u3063\u305f\u30d7\u30ed\u30f3\u30d7\u30c8\n    \"\"\"\n    prompt = self.awaiting_prompt\n    self.awaiting_prompt = None\n    if self._awaiting_prompt_event:\n        self._awaiting_prompt_event.clear()\n    return prompt\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.Context.finish","title":"<code>finish()</code>","text":"<p>Mark flow as finished \u30d5\u30ed\u30fc\u3092\u5b8c\u4e86\u3068\u3057\u3066\u30de\u30fc\u30af</p> Source code in <code>src\\agents_sdk_models\\context.py</code> <pre><code>def finish(self) -&gt; None:\n    \"\"\"\n    Mark flow as finished\n    \u30d5\u30ed\u30fc\u3092\u5b8c\u4e86\u3068\u3057\u3066\u30de\u30fc\u30af\n    \"\"\"\n    self.next_label = None\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.Context.from_dict","title":"<code>from_dict(data)</code>  <code>classmethod</code>","text":"<p>Create Context from dictionary (LangChain LCEL compatibility) \u8f9e\u66f8\u304b\u3089Context\u3092\u4f5c\u6210\uff08LangChain LCEL\u4e92\u63db\u6027\uff09</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dict[str, Any]</code> <p>Dictionary data / \u8f9e\u66f8\u30c7\u30fc\u30bf</p> required <p>Returns:</p> Name Type Description <code>Context</code> <code>'Context'</code> <p>New context instance / \u65b0\u3057\u3044\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9</p> Source code in <code>src\\agents_sdk_models\\context.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: Dict[str, Any]) -&gt; \"Context\":\n    \"\"\"\n    Create Context from dictionary (LangChain LCEL compatibility)\n    \u8f9e\u66f8\u304b\u3089Context\u3092\u4f5c\u6210\uff08LangChain LCEL\u4e92\u63db\u6027\uff09\n\n    Args:\n        data: Dictionary data / \u8f9e\u66f8\u30c7\u30fc\u30bf\n\n    Returns:\n        Context: New context instance / \u65b0\u3057\u3044\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\n    \"\"\"\n    data = data.copy()\n    # Convert history to messages\n    # \u5c65\u6b74\u3092\u30e1\u30c3\u30bb\u30fc\u30b8\u306b\u5909\u63db\n    history = data.pop(\"history\", [])\n    messages = []\n    for msg_data in history:\n        if isinstance(msg_data, dict):\n            messages.append(Message(\n                role=msg_data.get(\"role\", \"user\"),\n                content=msg_data.get(\"content\", \"\"),\n                metadata=msg_data.get(\"metadata\", {})\n            ))\n    data[\"messages\"] = messages\n    return cls(**data)\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.Context.get_artifact","title":"<code>get_artifact(key, default=None)</code>","text":"<p>Get artifact value \u6210\u679c\u7269\u306e\u5024\u3092\u53d6\u5f97</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Artifact key / \u6210\u679c\u7269\u30ad\u30fc</p> required <code>default</code> <code>Any</code> <p>Default value if not found / \u898b\u3064\u304b\u3089\u306a\u3044\u5834\u5408\u306e\u30c7\u30d5\u30a9\u30eb\u30c8\u5024</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>Artifact value / \u6210\u679c\u7269\u5024</p> Source code in <code>src\\agents_sdk_models\\context.py</code> <pre><code>def get_artifact(self, key: str, default: Any = None) -&gt; Any:\n    \"\"\"\n    Get artifact value\n    \u6210\u679c\u7269\u306e\u5024\u3092\u53d6\u5f97\n\n    Args:\n        key: Artifact key / \u6210\u679c\u7269\u30ad\u30fc\n        default: Default value if not found / \u898b\u3064\u304b\u3089\u306a\u3044\u5834\u5408\u306e\u30c7\u30d5\u30a9\u30eb\u30c8\u5024\n\n    Returns:\n        Any: Artifact value / \u6210\u679c\u7269\u5024\n    \"\"\"\n    return self.artifacts.get(key, default)\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.Context.get_conversation_text","title":"<code>get_conversation_text(include_system=False)</code>","text":"<p>Get conversation as formatted text \u4f1a\u8a71\u3092\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u6e08\u307f\u30c6\u30ad\u30b9\u30c8\u3068\u3057\u3066\u53d6\u5f97</p> <p>Parameters:</p> Name Type Description Default <code>include_system</code> <code>bool</code> <p>Include system messages / \u30b7\u30b9\u30c6\u30e0\u30e1\u30c3\u30bb\u30fc\u30b8\u3092\u542b\u3081\u308b</p> <code>False</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Formatted conversation / \u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u6e08\u307f\u4f1a\u8a71</p> Source code in <code>src\\agents_sdk_models\\context.py</code> <pre><code>def get_conversation_text(self, include_system: bool = False) -&gt; str:\n    \"\"\"\n    Get conversation as formatted text\n    \u4f1a\u8a71\u3092\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u6e08\u307f\u30c6\u30ad\u30b9\u30c8\u3068\u3057\u3066\u53d6\u5f97\n\n    Args:\n        include_system: Include system messages / \u30b7\u30b9\u30c6\u30e0\u30e1\u30c3\u30bb\u30fc\u30b8\u3092\u542b\u3081\u308b\n\n    Returns:\n        str: Formatted conversation / \u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u6e08\u307f\u4f1a\u8a71\n    \"\"\"\n    lines = []\n    for msg in self.messages:\n        if not include_system and msg.role == \"system\":\n            continue\n        role_label = {\"user\": \"\ud83d\udc64\", \"assistant\": \"\ud83e\udd16\", \"system\": \"\u2699\ufe0f\"}.get(msg.role, msg.role)\n        lines.append(f\"{role_label} {msg.content}\")\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.Context.get_last_messages","title":"<code>get_last_messages(n=10)</code>","text":"<p>Get last N messages \u6700\u5f8c\u306eN\u30e1\u30c3\u30bb\u30fc\u30b8\u3092\u53d6\u5f97</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of messages / \u30e1\u30c3\u30bb\u30fc\u30b8\u6570</p> <code>10</code> <p>Returns:</p> Type Description <code>List[Message]</code> <p>List[Message]: Last N messages / \u6700\u5f8c\u306eN\u30e1\u30c3\u30bb\u30fc\u30b8</p> Source code in <code>src\\agents_sdk_models\\context.py</code> <pre><code>def get_last_messages(self, n: int = 10) -&gt; List[Message]:\n    \"\"\"\n    Get last N messages\n    \u6700\u5f8c\u306eN\u30e1\u30c3\u30bb\u30fc\u30b8\u3092\u53d6\u5f97\n\n    Args:\n        n: Number of messages / \u30e1\u30c3\u30bb\u30fc\u30b8\u6570\n\n    Returns:\n        List[Message]: Last N messages / \u6700\u5f8c\u306eN\u30e1\u30c3\u30bb\u30fc\u30b8\n    \"\"\"\n    return self.messages[-n:] if len(self.messages) &gt; n else self.messages.copy()\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.Context.goto","title":"<code>goto(label)</code>","text":"<p>Set next step routing \u6b21\u30b9\u30c6\u30c3\u30d7\u306e\u30eb\u30fc\u30c6\u30a3\u30f3\u30b0\u3092\u8a2d\u5b9a</p> <p>Parameters:</p> Name Type Description Default <code>label</code> <code>str</code> <p>Next step label / \u6b21\u30b9\u30c6\u30c3\u30d7\u306e\u30e9\u30d9\u30eb</p> required Source code in <code>src\\agents_sdk_models\\context.py</code> <pre><code>def goto(self, label: str) -&gt; None:\n    \"\"\"\n    Set next step routing\n    \u6b21\u30b9\u30c6\u30c3\u30d7\u306e\u30eb\u30fc\u30c6\u30a3\u30f3\u30b0\u3092\u8a2d\u5b9a\n\n    Args:\n        label: Next step label / \u6b21\u30b9\u30c6\u30c3\u30d7\u306e\u30e9\u30d9\u30eb\n    \"\"\"\n    self.next_label = label\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.Context.is_finished","title":"<code>is_finished()</code>","text":"<p>Check if flow is finished \u30d5\u30ed\u30fc\u304c\u5b8c\u4e86\u3057\u3066\u3044\u308b\u304b\u30c1\u30a7\u30c3\u30af</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if finished / \u5b8c\u4e86\u3057\u3066\u3044\u308b\u5834\u5408True</p> Source code in <code>src\\agents_sdk_models\\context.py</code> <pre><code>def is_finished(self) -&gt; bool:\n    \"\"\"\n    Check if flow is finished\n    \u30d5\u30ed\u30fc\u304c\u5b8c\u4e86\u3057\u3066\u3044\u308b\u304b\u30c1\u30a7\u30c3\u30af\n\n    Returns:\n        bool: True if finished / \u5b8c\u4e86\u3057\u3066\u3044\u308b\u5834\u5408True\n    \"\"\"\n    return self.next_label is None\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.Context.provide_user_input","title":"<code>provide_user_input(user_input)</code>","text":"<p>Provide user input and clear waiting state \u30e6\u30fc\u30b6\u30fc\u5165\u529b\u3092\u63d0\u4f9b\u3057\u3001\u5f85\u3061\u72b6\u614b\u3092\u30af\u30ea\u30a2</p> <p>Parameters:</p> Name Type Description Default <code>user_input</code> <code>str</code> <p>User input text / \u30e6\u30fc\u30b6\u30fc\u5165\u529b\u30c6\u30ad\u30b9\u30c8</p> required Source code in <code>src\\agents_sdk_models\\context.py</code> <pre><code>def provide_user_input(self, user_input: str) -&gt; None:\n    \"\"\"\n    Provide user input and clear waiting state\n    \u30e6\u30fc\u30b6\u30fc\u5165\u529b\u3092\u63d0\u4f9b\u3057\u3001\u5f85\u3061\u72b6\u614b\u3092\u30af\u30ea\u30a2\n\n    Args:\n        user_input: User input text / \u30e6\u30fc\u30b6\u30fc\u5165\u529b\u30c6\u30ad\u30b9\u30c8\n    \"\"\"\n    self.add_user_message(user_input)\n    self.awaiting_prompt = None\n    self.awaiting_user_input = False\n    if self._user_input_event:\n        self._user_input_event.set()\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.Context.set_artifact","title":"<code>set_artifact(key, value)</code>","text":"<p>Set artifact value \u6210\u679c\u7269\u306e\u5024\u3092\u8a2d\u5b9a</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Artifact key / \u6210\u679c\u7269\u30ad\u30fc</p> required <code>value</code> <code>Any</code> <p>Artifact value / \u6210\u679c\u7269\u5024</p> required Source code in <code>src\\agents_sdk_models\\context.py</code> <pre><code>def set_artifact(self, key: str, value: Any) -&gt; None:\n    \"\"\"\n    Set artifact value\n    \u6210\u679c\u7269\u306e\u5024\u3092\u8a2d\u5b9a\n\n    Args:\n        key: Artifact key / \u6210\u679c\u7269\u30ad\u30fc\n        value: Artifact value / \u6210\u679c\u7269\u5024\n    \"\"\"\n    self.artifacts[key] = value\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.Context.set_waiting_for_user_input","title":"<code>set_waiting_for_user_input(prompt)</code>","text":"<p>Set context to wait for user input with a prompt \u30d7\u30ed\u30f3\u30d7\u30c8\u3067\u30e6\u30fc\u30b6\u30fc\u5165\u529b\u5f85\u3061\u72b6\u614b\u306b\u8a2d\u5b9a</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>Prompt to display to user / \u30e6\u30fc\u30b6\u30fc\u306b\u8868\u793a\u3059\u308b\u30d7\u30ed\u30f3\u30d7\u30c8</p> required Source code in <code>src\\agents_sdk_models\\context.py</code> <pre><code>def set_waiting_for_user_input(self, prompt: str) -&gt; None:\n    \"\"\"\n    Set context to wait for user input with a prompt\n    \u30d7\u30ed\u30f3\u30d7\u30c8\u3067\u30e6\u30fc\u30b6\u30fc\u5165\u529b\u5f85\u3061\u72b6\u614b\u306b\u8a2d\u5b9a\n\n    Args:\n        prompt: Prompt to display to user / \u30e6\u30fc\u30b6\u30fc\u306b\u8868\u793a\u3059\u308b\u30d7\u30ed\u30f3\u30d7\u30c8\n    \"\"\"\n    self.awaiting_prompt = prompt\n    self.awaiting_user_input = True\n    if self._awaiting_prompt_event:\n        self._awaiting_prompt_event.set()\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.Context.update_step_info","title":"<code>update_step_info(step_name)</code>","text":"<p>Update current step information \u73fe\u5728\u306e\u30b9\u30c6\u30c3\u30d7\u60c5\u5831\u3092\u66f4\u65b0</p> <p>Parameters:</p> Name Type Description Default <code>step_name</code> <code>str</code> <p>Current step name / \u73fe\u5728\u306e\u30b9\u30c6\u30c3\u30d7\u540d</p> required Source code in <code>src\\agents_sdk_models\\context.py</code> <pre><code>def update_step_info(self, step_name: str) -&gt; None:\n    \"\"\"\n    Update current step information\n    \u73fe\u5728\u306e\u30b9\u30c6\u30c3\u30d7\u60c5\u5831\u3092\u66f4\u65b0\n\n    Args:\n        step_name: Current step name / \u73fe\u5728\u306e\u30b9\u30c6\u30c3\u30d7\u540d\n    \"\"\"\n    self.current_step = step_name\n    self.step_count += 1\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.Context.wait_for_prompt_event","title":"<code>wait_for_prompt_event()</code>  <code>async</code>","text":"<p>Async wait for prompt event \u30d7\u30ed\u30f3\u30d7\u30c8\u30a4\u30d9\u30f3\u30c8\u3092\u975e\u540c\u671f\u3067\u5f85\u6a5f</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Prompt waiting for user / \u30e6\u30fc\u30b6\u30fc\u5f85\u3061\u306e\u30d7\u30ed\u30f3\u30d7\u30c8</p> Source code in <code>src\\agents_sdk_models\\context.py</code> <pre><code>async def wait_for_prompt_event(self) -&gt; str:\n    \"\"\"\n    Async wait for prompt event\n    \u30d7\u30ed\u30f3\u30d7\u30c8\u30a4\u30d9\u30f3\u30c8\u3092\u975e\u540c\u671f\u3067\u5f85\u6a5f\n\n    Returns:\n        str: Prompt waiting for user / \u30e6\u30fc\u30b6\u30fc\u5f85\u3061\u306e\u30d7\u30ed\u30f3\u30d7\u30c8\n    \"\"\"\n    if self._awaiting_prompt_event:\n        await self._awaiting_prompt_event.wait()\n    return self.awaiting_prompt or \"\"\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.Context.wait_for_user_input","title":"<code>wait_for_user_input()</code>  <code>async</code>","text":"<p>Async wait for user input \u30e6\u30fc\u30b6\u30fc\u5165\u529b\u3092\u975e\u540c\u671f\u3067\u5f85\u6a5f</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>User input / \u30e6\u30fc\u30b6\u30fc\u5165\u529b</p> Source code in <code>src\\agents_sdk_models\\context.py</code> <pre><code>async def wait_for_user_input(self) -&gt; str:\n    \"\"\"\n    Async wait for user input\n    \u30e6\u30fc\u30b6\u30fc\u5165\u529b\u3092\u975e\u540c\u671f\u3067\u5f85\u6a5f\n\n    Returns:\n        str: User input / \u30e6\u30fc\u30b6\u30fc\u5165\u529b\n    \"\"\"\n    if self._user_input_event:\n        await self._user_input_event.wait()\n        self._user_input_event.clear()\n    return self.last_user_input or \"\"\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.DebugStep","title":"<code>DebugStep</code>","text":"<p>               Bases: <code>Step</code></p> <p>Step for debugging and logging \u30c7\u30d0\u30c3\u30b0\u3068\u30ed\u30b0\u7528\u30b9\u30c6\u30c3\u30d7</p> <p>This step prints or logs context information for debugging purposes. \u3053\u306e\u30b9\u30c6\u30c3\u30d7\u306f\u30c7\u30d0\u30c3\u30b0\u76ee\u7684\u3067\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u60c5\u5831\u3092\u5370\u5237\u307e\u305f\u306f\u30ed\u30b0\u51fa\u529b\u3057\u307e\u3059\u3002</p> Source code in <code>src\\agents_sdk_models\\step.py</code> <pre><code>class DebugStep(Step):\n    \"\"\"\n    Step for debugging and logging\n    \u30c7\u30d0\u30c3\u30b0\u3068\u30ed\u30b0\u7528\u30b9\u30c6\u30c3\u30d7\n\n    This step prints or logs context information for debugging purposes.\n    \u3053\u306e\u30b9\u30c6\u30c3\u30d7\u306f\u30c7\u30d0\u30c3\u30b0\u76ee\u7684\u3067\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u60c5\u5831\u3092\u5370\u5237\u307e\u305f\u306f\u30ed\u30b0\u51fa\u529b\u3057\u307e\u3059\u3002\n    \"\"\"\n\n    def __init__(self, name: str, message: str = \"\", print_context: bool = False, next_step: Optional[str] = None):\n        \"\"\"\n        Initialize debug step\n        \u30c7\u30d0\u30c3\u30b0\u30b9\u30c6\u30c3\u30d7\u3092\u521d\u671f\u5316\n\n        Args:\n            name: Step name / \u30b9\u30c6\u30c3\u30d7\u540d\n            message: Debug message / \u30c7\u30d0\u30c3\u30b0\u30e1\u30c3\u30bb\u30fc\u30b8\n            print_context: Whether to print full context / \u5b8c\u5168\u306a\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u3092\u5370\u5237\u3059\u308b\u304b\n            next_step: Next step / \u6b21\u30b9\u30c6\u30c3\u30d7\n        \"\"\"\n        super().__init__(name)\n        self.message = message\n        self.print_context = print_context\n        self.next_step = next_step\n\n    async def run(self, user_input: Optional[str], ctx: Context) -&gt; Context:\n        \"\"\"\n        Execute debug step\n        \u30c7\u30d0\u30c3\u30b0\u30b9\u30c6\u30c3\u30d7\u3092\u5b9f\u884c\n\n        Args:\n            user_input: User input / \u30e6\u30fc\u30b6\u30fc\u5165\u529b\n            ctx: Current context / \u73fe\u5728\u306e\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\n\n        Returns:\n            Context: Updated context / \u66f4\u65b0\u6e08\u307f\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\n        \"\"\"\n        ctx.update_step_info(self.name)\n\n        # Print debug information\n        # \u30c7\u30d0\u30c3\u30b0\u60c5\u5831\u3092\u5370\u5237\n        print(f\"\ud83d\udc1b DEBUG [{self.name}]: {self.message}\")\n        if user_input:\n            print(f\"   User Input: {user_input}\")\n        print(f\"   Step Count: {ctx.step_count}\")\n        print(f\"   Next Label: {ctx.next_label}\")\n\n        if self.print_context:\n            print(f\"   Context: {ctx.dict()}\")\n\n        # Add debug message to system messages\n        # \u30c7\u30d0\u30c3\u30b0\u30e1\u30c3\u30bb\u30fc\u30b8\u3092\u30b7\u30b9\u30c6\u30e0\u30e1\u30c3\u30bb\u30fc\u30b8\u306b\u8ffd\u52a0\n        ctx.add_system_message(f\"DEBUG {self.name}: {self.message}\")\n\n        # Set next step if specified\n        # \u6307\u5b9a\u3055\u308c\u3066\u3044\u308b\u5834\u5408\u306f\u6b21\u30b9\u30c6\u30c3\u30d7\u3092\u8a2d\u5b9a\n        if self.next_step:\n            ctx.goto(self.next_step)\n\n        return ctx\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.DebugStep.__init__","title":"<code>__init__(name, message='', print_context=False, next_step=None)</code>","text":"<p>Initialize debug step \u30c7\u30d0\u30c3\u30b0\u30b9\u30c6\u30c3\u30d7\u3092\u521d\u671f\u5316</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Step name / \u30b9\u30c6\u30c3\u30d7\u540d</p> required <code>message</code> <code>str</code> <p>Debug message / \u30c7\u30d0\u30c3\u30b0\u30e1\u30c3\u30bb\u30fc\u30b8</p> <code>''</code> <code>print_context</code> <code>bool</code> <p>Whether to print full context / \u5b8c\u5168\u306a\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u3092\u5370\u5237\u3059\u308b\u304b</p> <code>False</code> <code>next_step</code> <code>Optional[str]</code> <p>Next step / \u6b21\u30b9\u30c6\u30c3\u30d7</p> <code>None</code> Source code in <code>src\\agents_sdk_models\\step.py</code> <pre><code>def __init__(self, name: str, message: str = \"\", print_context: bool = False, next_step: Optional[str] = None):\n    \"\"\"\n    Initialize debug step\n    \u30c7\u30d0\u30c3\u30b0\u30b9\u30c6\u30c3\u30d7\u3092\u521d\u671f\u5316\n\n    Args:\n        name: Step name / \u30b9\u30c6\u30c3\u30d7\u540d\n        message: Debug message / \u30c7\u30d0\u30c3\u30b0\u30e1\u30c3\u30bb\u30fc\u30b8\n        print_context: Whether to print full context / \u5b8c\u5168\u306a\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u3092\u5370\u5237\u3059\u308b\u304b\n        next_step: Next step / \u6b21\u30b9\u30c6\u30c3\u30d7\n    \"\"\"\n    super().__init__(name)\n    self.message = message\n    self.print_context = print_context\n    self.next_step = next_step\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.DebugStep.run","title":"<code>run(user_input, ctx)</code>  <code>async</code>","text":"<p>Execute debug step \u30c7\u30d0\u30c3\u30b0\u30b9\u30c6\u30c3\u30d7\u3092\u5b9f\u884c</p> <p>Parameters:</p> Name Type Description Default <code>user_input</code> <code>Optional[str]</code> <p>User input / \u30e6\u30fc\u30b6\u30fc\u5165\u529b</p> required <code>ctx</code> <code>Context</code> <p>Current context / \u73fe\u5728\u306e\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8</p> required <p>Returns:</p> Name Type Description <code>Context</code> <code>Context</code> <p>Updated context / \u66f4\u65b0\u6e08\u307f\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8</p> Source code in <code>src\\agents_sdk_models\\step.py</code> <pre><code>async def run(self, user_input: Optional[str], ctx: Context) -&gt; Context:\n    \"\"\"\n    Execute debug step\n    \u30c7\u30d0\u30c3\u30b0\u30b9\u30c6\u30c3\u30d7\u3092\u5b9f\u884c\n\n    Args:\n        user_input: User input / \u30e6\u30fc\u30b6\u30fc\u5165\u529b\n        ctx: Current context / \u73fe\u5728\u306e\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\n\n    Returns:\n        Context: Updated context / \u66f4\u65b0\u6e08\u307f\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\n    \"\"\"\n    ctx.update_step_info(self.name)\n\n    # Print debug information\n    # \u30c7\u30d0\u30c3\u30b0\u60c5\u5831\u3092\u5370\u5237\n    print(f\"\ud83d\udc1b DEBUG [{self.name}]: {self.message}\")\n    if user_input:\n        print(f\"   User Input: {user_input}\")\n    print(f\"   Step Count: {ctx.step_count}\")\n    print(f\"   Next Label: {ctx.next_label}\")\n\n    if self.print_context:\n        print(f\"   Context: {ctx.dict()}\")\n\n    # Add debug message to system messages\n    # \u30c7\u30d0\u30c3\u30b0\u30e1\u30c3\u30bb\u30fc\u30b8\u3092\u30b7\u30b9\u30c6\u30e0\u30e1\u30c3\u30bb\u30fc\u30b8\u306b\u8ffd\u52a0\n    ctx.add_system_message(f\"DEBUG {self.name}: {self.message}\")\n\n    # Set next step if specified\n    # \u6307\u5b9a\u3055\u308c\u3066\u3044\u308b\u5834\u5408\u306f\u6b21\u30b9\u30c6\u30c3\u30d7\u3092\u8a2d\u5b9a\n    if self.next_step:\n        ctx.goto(self.next_step)\n\n    return ctx\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.EvaluationResult","title":"<code>EvaluationResult</code>  <code>dataclass</code>","text":"<p>Result of evaluation for generated content \u751f\u6210\u3055\u308c\u305f\u30b3\u30f3\u30c6\u30f3\u30c4\u306e\u8a55\u4fa1\u7d50\u679c\u3092\u4fdd\u6301\u3059\u308b\u30af\u30e9\u30b9</p> <p>Attributes:</p> Name Type Description <code>score</code> <code>int</code> <p>Evaluation score (0-100) / \u8a55\u4fa1\u30b9\u30b3\u30a2\uff080-100\uff09</p> <code>comment</code> <code>List[Comment]</code> <p>List of Comment instances containing importance and content / \u91cd\u8981\u5ea6\u3068\u5185\u5bb9\u3092\u6301\u3064Comment\u30af\u30e9\u30b9\u306e\u30ea\u30b9\u30c8</p> Source code in <code>src\\agents_sdk_models\\pipeline.py</code> <pre><code>@dataclass\nclass EvaluationResult:\n    \"\"\"\n    Result of evaluation for generated content\n    \u751f\u6210\u3055\u308c\u305f\u30b3\u30f3\u30c6\u30f3\u30c4\u306e\u8a55\u4fa1\u7d50\u679c\u3092\u4fdd\u6301\u3059\u308b\u30af\u30e9\u30b9\n\n    Attributes:\n        score: Evaluation score (0-100) / \u8a55\u4fa1\u30b9\u30b3\u30a2\uff080-100\uff09\n        comment: List of Comment instances containing importance and content / \u91cd\u8981\u5ea6\u3068\u5185\u5bb9\u3092\u6301\u3064Comment\u30af\u30e9\u30b9\u306e\u30ea\u30b9\u30c8\n    \"\"\"\n    score: int  # Evaluation score (0-100) / \u8a55\u4fa1\u30b9\u30b3\u30a2\uff080-100\uff09\n    comment: List[Comment]  # List of evaluation comments / \u8a55\u4fa1\u30b3\u30e1\u30f3\u30c8\u306e\u30ea\u30b9\u30c8\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.Flow","title":"<code>Flow</code>","text":"<p>Flow orchestration engine for Step-based workflows \u30b9\u30c6\u30c3\u30d7\u30d9\u30fc\u30b9\u30ef\u30fc\u30af\u30d5\u30ed\u30fc\u7528\u30d5\u30ed\u30fc\u30aa\u30fc\u30b1\u30b9\u30c8\u30ec\u30fc\u30b7\u30e7\u30f3\u30a8\u30f3\u30b8\u30f3</p> <p>This class provides: \u3053\u306e\u30af\u30e9\u30b9\u306f\u4ee5\u4e0b\u3092\u63d0\u4f9b\u3057\u307e\u3059\uff1a - Declarative step-based workflow definition / \u5ba3\u8a00\u7684\u30b9\u30c6\u30c3\u30d7\u30d9\u30fc\u30b9\u30ef\u30fc\u30af\u30d5\u30ed\u30fc\u5b9a\u7fa9 - Synchronous and asynchronous execution modes / \u540c\u671f\u30fb\u975e\u540c\u671f\u5b9f\u884c\u30e2\u30fc\u30c9 - User input coordination for interactive workflows / \u5bfe\u8a71\u7684\u30ef\u30fc\u30af\u30d5\u30ed\u30fc\u7528\u30e6\u30fc\u30b6\u30fc\u5165\u529b\u8abf\u6574 - Error handling and observability / \u30a8\u30e9\u30fc\u30cf\u30f3\u30c9\u30ea\u30f3\u30b0\u3068\u30aa\u30d6\u30b6\u30fc\u30d0\u30d3\u30ea\u30c6\u30a3</p> Source code in <code>src\\agents_sdk_models\\flow.py</code> <pre><code>class Flow:\n    \"\"\"\n    Flow orchestration engine for Step-based workflows\n    \u30b9\u30c6\u30c3\u30d7\u30d9\u30fc\u30b9\u30ef\u30fc\u30af\u30d5\u30ed\u30fc\u7528\u30d5\u30ed\u30fc\u30aa\u30fc\u30b1\u30b9\u30c8\u30ec\u30fc\u30b7\u30e7\u30f3\u30a8\u30f3\u30b8\u30f3\n\n    This class provides:\n    \u3053\u306e\u30af\u30e9\u30b9\u306f\u4ee5\u4e0b\u3092\u63d0\u4f9b\u3057\u307e\u3059\uff1a\n    - Declarative step-based workflow definition / \u5ba3\u8a00\u7684\u30b9\u30c6\u30c3\u30d7\u30d9\u30fc\u30b9\u30ef\u30fc\u30af\u30d5\u30ed\u30fc\u5b9a\u7fa9\n    - Synchronous and asynchronous execution modes / \u540c\u671f\u30fb\u975e\u540c\u671f\u5b9f\u884c\u30e2\u30fc\u30c9\n    - User input coordination for interactive workflows / \u5bfe\u8a71\u7684\u30ef\u30fc\u30af\u30d5\u30ed\u30fc\u7528\u30e6\u30fc\u30b6\u30fc\u5165\u529b\u8abf\u6574\n    - Error handling and observability / \u30a8\u30e9\u30fc\u30cf\u30f3\u30c9\u30ea\u30f3\u30b0\u3068\u30aa\u30d6\u30b6\u30fc\u30d0\u30d3\u30ea\u30c6\u30a3\n    \"\"\"\n\n    def __init__(\n        self, \n        start: Optional[str] = None, \n        steps: Optional[Union[Dict[str, Step], List[Step], Step]] = None, \n        context: Optional[Context] = None,\n        max_steps: int = 1000,\n        trace_id: Optional[str] = None\n    ):\n        \"\"\"\n        Initialize Flow with flexible step definitions\n        \u67d4\u8edf\u306a\u30b9\u30c6\u30c3\u30d7\u5b9a\u7fa9\u3067Flow\u3092\u521d\u671f\u5316\n\n        This constructor now supports three ways to define steps:\n        \u3053\u306e\u30b3\u30f3\u30b9\u30c8\u30e9\u30af\u30bf\u306f3\u3064\u306e\u65b9\u6cd5\u3067\u30b9\u30c6\u30c3\u30d7\u3092\u5b9a\u7fa9\u3067\u304d\u307e\u3059\uff1a\n        1. Traditional: start step name + Dict[str, Step]\n        2. Sequential: List[Step] (creates sequential workflow)\n        3. Single: Single Step (creates single-step workflow)\n\n        Args:\n            start: Start step label (optional for List/Single mode) / \u958b\u59cb\u30b9\u30c6\u30c3\u30d7\u30e9\u30d9\u30eb\uff08List/Single\u30e2\u30fc\u30c9\u3067\u306f\u7701\u7565\u53ef\uff09\n            steps: Step definitions - Dict[str, Step], List[Step], or Step / \u30b9\u30c6\u30c3\u30d7\u5b9a\u7fa9 - Dict[str, Step]\u3001List[Step]\u3001\u307e\u305f\u306fStep\n            context: Initial context (optional) / \u521d\u671f\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\uff08\u30aa\u30d7\u30b7\u30e7\u30f3\uff09\n            max_steps: Maximum number of steps to prevent infinite loops / \u7121\u9650\u30eb\u30fc\u30d7\u9632\u6b62\u306e\u305f\u3081\u306e\u6700\u5927\u30b9\u30c6\u30c3\u30d7\u6570\n            trace_id: Trace ID for observability / \u30aa\u30d6\u30b6\u30fc\u30d0\u30d3\u30ea\u30c6\u30a3\u7528\u30c8\u30ec\u30fc\u30b9ID\n        \"\"\"\n        # Handle flexible step definitions\n        # \u67d4\u8edf\u306a\u30b9\u30c6\u30c3\u30d7\u5b9a\u7fa9\u3092\u51e6\u7406\n        if isinstance(steps, dict):\n            # Traditional mode: Dict[str, Step]\n            # \u5f93\u6765\u30e2\u30fc\u30c9: Dict[str, Step]\n            if start is None:\n                raise ValueError(\"start parameter is required when steps is a dictionary\")\n            self.start = start\n            self.steps = steps\n        elif isinstance(steps, list):\n            # Sequential mode: List[Step] \n            # \u30b7\u30fc\u30b1\u30f3\u30b7\u30e3\u30eb\u30e2\u30fc\u30c9: List[Step]\n            if not steps:\n                raise ValueError(\"Steps list cannot be empty\")\n            self.steps = {}\n            prev_step_name = None\n\n            for i, step in enumerate(steps):\n                if not hasattr(step, 'name'):\n                    raise ValueError(f\"Step at index {i} must have a 'name' attribute\")\n\n                step_name = step.name\n                self.steps[step_name] = step\n\n                # Set sequential flow: each step goes to next step\n                # \u30b7\u30fc\u30b1\u30f3\u30b7\u30e3\u30eb\u30d5\u30ed\u30fc\u8a2d\u5b9a: \u5404\u30b9\u30c6\u30c3\u30d7\u304c\u6b21\u306e\u30b9\u30c6\u30c3\u30d7\u306b\u9032\u3080\n                if prev_step_name is not None and hasattr(self.steps[prev_step_name], 'next_step'):\n                    if self.steps[prev_step_name].next_step is None:\n                        self.steps[prev_step_name].next_step = step_name\n\n                prev_step_name = step_name\n\n            # Start with first step\n            # \u6700\u521d\u306e\u30b9\u30c6\u30c3\u30d7\u304b\u3089\u958b\u59cb\n            self.start = steps[0].name\n\n        elif steps is not None:\n            # Check if it's a Step instance\n            # Step\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u304b\u3069\u3046\u304b\u3092\u30c1\u30a7\u30c3\u30af\n            if isinstance(steps, Step):\n                # Single step mode: Step\n                # \u5358\u4e00\u30b9\u30c6\u30c3\u30d7\u30e2\u30fc\u30c9: Step\n                if not hasattr(steps, 'name'):\n                    raise ValueError(\"Step must have a 'name' attribute\")\n\n                step_name = steps.name\n                self.start = step_name\n                self.steps = {step_name: steps}\n            else:\n                # Not a valid type\n                # \u6709\u52b9\u306a\u30bf\u30a4\u30d7\u3067\u306f\u306a\u3044\n                raise ValueError(\"steps must be Dict[str, Step], List[Step], or Step\")\n        else:\n            raise ValueError(\"steps parameter cannot be None\")\n\n        self.context = context or Context()\n        self.max_steps = max_steps\n        self.trace_id = trace_id or f\"flow_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n\n        # Initialize context\n        # \u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u3092\u521d\u671f\u5316\n        self.context.trace_id = self.trace_id\n        self.context.next_label = self.start\n\n        # Execution state\n        # \u5b9f\u884c\u72b6\u614b\n        self._running = False\n        self._run_loop_task: Optional[asyncio.Task] = None\n        self._execution_lock = asyncio.Lock()\n\n        # Hooks for observability\n        # \u30aa\u30d6\u30b6\u30fc\u30d0\u30d3\u30ea\u30c6\u30a3\u7528\u30d5\u30c3\u30af\n        self.before_step_hooks: List[Callable[[str, Context], None]] = []\n        self.after_step_hooks: List[Callable[[str, Context, Any], None]] = []\n        self.error_hooks: List[Callable[[str, Context, Exception], None]] = []\n\n    @property\n    def finished(self) -&gt; bool:\n        \"\"\"\n        Check if flow is finished\n        \u30d5\u30ed\u30fc\u304c\u5b8c\u4e86\u3057\u3066\u3044\u308b\u304b\u30c1\u30a7\u30c3\u30af\n\n        Returns:\n            bool: True if finished / \u5b8c\u4e86\u3057\u3066\u3044\u308b\u5834\u5408True\n        \"\"\"\n        return self.context.is_finished()\n\n    @property\n    def current_step_name(self) -&gt; Optional[str]:\n        \"\"\"\n        Get current step name\n        \u73fe\u5728\u306e\u30b9\u30c6\u30c3\u30d7\u540d\u3092\u53d6\u5f97\n\n        Returns:\n            str | None: Current step name / \u73fe\u5728\u306e\u30b9\u30c6\u30c3\u30d7\u540d\n        \"\"\"\n        return self.context.current_step\n\n    @property\n    def next_step_name(self) -&gt; Optional[str]:\n        \"\"\"\n        Get next step name\n        \u6b21\u306e\u30b9\u30c6\u30c3\u30d7\u540d\u3092\u53d6\u5f97\n\n        Returns:\n            str | None: Next step name / \u6b21\u306e\u30b9\u30c6\u30c3\u30d7\u540d\n        \"\"\"\n        return self.context.next_label\n\n    async def run(self, input_data: Optional[str] = None, initial_input: Optional[str] = None) -&gt; Context:\n        \"\"\"\n        Run flow to completion without user input coordination\n        \u30e6\u30fc\u30b6\u30fc\u5165\u529b\u8abf\u6574\u306a\u3057\u3067\u30d5\u30ed\u30fc\u3092\u5b8c\u4e86\u307e\u3067\u5b9f\u884c\n\n        This is for non-interactive workflows that don't require user input.\n        \u3053\u308c\u306f\u30e6\u30fc\u30b6\u30fc\u5165\u529b\u304c\u4e0d\u8981\u306a\u975e\u5bfe\u8a71\u7684\u30ef\u30fc\u30af\u30d5\u30ed\u30fc\u7528\u3067\u3059\u3002\n\n        Args:\n            input_data: Input data to the flow (preferred parameter name) / \u30d5\u30ed\u30fc\u3078\u306e\u5165\u529b\u30c7\u30fc\u30bf\uff08\u63a8\u5968\u30d1\u30e9\u30e1\u30fc\u30bf\u540d\uff09\n            initial_input: Initial input to the flow (deprecated, use input_data) / \u30d5\u30ed\u30fc\u3078\u306e\u521d\u671f\u5165\u529b\uff08\u975e\u63a8\u5968\u3001input_data\u3092\u4f7f\u7528\uff09\n\n        Returns:\n            Context: Final context / \u6700\u7d42\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\n\n        Raises:\n            FlowExecutionError: If execution fails / \u5b9f\u884c\u5931\u6557\u6642\n        \"\"\"\n        async with self._execution_lock:\n            try:\n                self._running = True\n\n                # Reset context for new execution\n                # \u65b0\u3057\u3044\u5b9f\u884c\u7528\u306b\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u3092\u30ea\u30bb\u30c3\u30c8\n                if self.context.step_count &gt; 0:\n                    self.context = Context(trace_id=self.trace_id)\n                    self.context.next_label = self.start\n\n                # Determine input to use (input_data takes precedence)\n                # \u4f7f\u7528\u3059\u308b\u5165\u529b\u3092\u6c7a\u5b9a\uff08input_data\u304c\u512a\u5148\uff09\n                effective_input = input_data or initial_input\n\n                # Add input if provided\n                # \u5165\u529b\u304c\u63d0\u4f9b\u3055\u308c\u3066\u3044\u308b\u5834\u5408\u306f\u8ffd\u52a0\n                if effective_input:\n                    self.context.add_user_message(effective_input)\n\n                current_input = effective_input\n                step_count = 0\n\n                while not self.finished and step_count &lt; self.max_steps:\n                    step_name = self.context.next_label\n                    if not step_name or step_name not in self.steps:\n                        break\n\n                    step = self.steps[step_name]\n\n                    # Execute step\n                    # \u30b9\u30c6\u30c3\u30d7\u3092\u5b9f\u884c\n                    try:\n                        await self._execute_step(step, current_input)\n                        current_input = None  # Only use initial input for first step\n                        step_count += 1\n\n                        # If step is waiting for user input, break\n                        # \u30b9\u30c6\u30c3\u30d7\u304c\u30e6\u30fc\u30b6\u30fc\u5165\u529b\u3092\u5f85\u6a5f\u3057\u3066\u3044\u308b\u5834\u5408\u3001\u4e2d\u65ad\n                        if self.context.awaiting_user_input:\n                            break\n\n                    except Exception as e:\n                        logger.error(f\"Error executing step {step_name}: {e}\")\n                        self._handle_step_error(step_name, e)\n                        break\n\n                # Check for infinite loop\n                # \u7121\u9650\u30eb\u30fc\u30d7\u306e\u30c1\u30a7\u30c3\u30af\n                if step_count &gt;= self.max_steps:\n                    raise FlowExecutionError(f\"Flow exceeded maximum steps ({self.max_steps})\")\n\n                return self.context\n\n            finally:\n                self._running = False\n\n    async def run_loop(self) -&gt; None:\n        \"\"\"\n        Run flow as background task with user input coordination\n        \u30e6\u30fc\u30b6\u30fc\u5165\u529b\u8abf\u6574\u3092\u542b\u3080\u30d0\u30c3\u30af\u30b0\u30e9\u30a6\u30f3\u30c9\u30bf\u30b9\u30af\u3068\u3057\u3066\u30d5\u30ed\u30fc\u3092\u5b9f\u884c\n\n        This method runs the flow continuously, pausing when user input is needed.\n        \u3053\u306e\u30e1\u30bd\u30c3\u30c9\u306f\u30d5\u30ed\u30fc\u3092\u7d99\u7d9a\u7684\u306b\u5b9f\u884c\u3057\u3001\u30e6\u30fc\u30b6\u30fc\u5165\u529b\u304c\u5fc5\u8981\u306a\u6642\u306b\u4e00\u6642\u505c\u6b62\u3057\u307e\u3059\u3002\n        Use feed() to provide user input when the flow is waiting.\n        \u30d5\u30ed\u30fc\u304c\u5f85\u6a5f\u3057\u3066\u3044\u308b\u6642\u306ffeed()\u3092\u4f7f\u7528\u3057\u3066\u30e6\u30fc\u30b6\u30fc\u5165\u529b\u3092\u63d0\u4f9b\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n        \"\"\"\n        async with self._execution_lock:\n            try:\n                self._running = True\n\n                # Reset context for new execution\n                # \u65b0\u3057\u3044\u5b9f\u884c\u7528\u306b\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u3092\u30ea\u30bb\u30c3\u30c8\n                if self.context.step_count &gt; 0:\n                    self.context = Context(trace_id=self.trace_id)\n                    self.context.next_label = self.start\n\n                step_count = 0\n                current_input = None\n\n                while not self.finished and step_count &lt; self.max_steps:\n                    step_name = self.context.next_label\n                    if not step_name or step_name not in self.steps:\n                        break\n\n                    step = self.steps[step_name]\n\n                    # Execute step\n                    # \u30b9\u30c6\u30c3\u30d7\u3092\u5b9f\u884c\n                    try:\n                        await self._execute_step(step, current_input)\n                        current_input = None\n                        step_count += 1\n\n                        # If step is waiting for user input, wait for feed()\n                        # \u30b9\u30c6\u30c3\u30d7\u304c\u30e6\u30fc\u30b6\u30fc\u5165\u529b\u3092\u5f85\u6a5f\u3057\u3066\u3044\u308b\u5834\u5408\u3001feed()\u3092\u5f85\u3064\n                        if self.context.awaiting_user_input:\n                            await self.context.wait_for_user_input()\n                            # After receiving input, continue with the same step\n                            # \u5165\u529b\u53d7\u4fe1\u5f8c\u3001\u540c\u3058\u30b9\u30c6\u30c3\u30d7\u3067\u7d99\u7d9a\n                            current_input = self.context.last_user_input\n                            continue\n\n                    except Exception as e:\n                        logger.error(f\"Error executing step {step_name}: {e}\")\n                        self._handle_step_error(step_name, e)\n                        break\n\n                # Check for infinite loop\n                # \u7121\u9650\u30eb\u30fc\u30d7\u306e\u30c1\u30a7\u30c3\u30af\n                if step_count &gt;= self.max_steps:\n                    raise FlowExecutionError(f\"Flow exceeded maximum steps ({self.max_steps})\")\n\n            finally:\n                self._running = False\n\n    def next_prompt(self) -&gt; Optional[str]:\n        \"\"\"\n        Get next prompt for synchronous CLI usage\n        \u540c\u671fCLI\u4f7f\u7528\u7528\u306e\u6b21\u306e\u30d7\u30ed\u30f3\u30d7\u30c8\u3092\u53d6\u5f97\n\n        Returns:\n            str | None: Prompt if waiting for user input / \u30e6\u30fc\u30b6\u30fc\u5165\u529b\u5f85\u3061\u306e\u5834\u5408\u306e\u30d7\u30ed\u30f3\u30d7\u30c8\n        \"\"\"\n        return self.context.clear_prompt()\n\n    def feed(self, user_input: str) -&gt; None:\n        \"\"\"\n        Provide user input to the flow\n        \u30d5\u30ed\u30fc\u306b\u30e6\u30fc\u30b6\u30fc\u5165\u529b\u3092\u63d0\u4f9b\n\n        Args:\n            user_input: User input text / \u30e6\u30fc\u30b6\u30fc\u5165\u529b\u30c6\u30ad\u30b9\u30c8\n        \"\"\"\n        self.context.provide_user_input(user_input)\n\n    def step(self) -&gt; None:\n        \"\"\"\n        Execute one step synchronously\n        1\u30b9\u30c6\u30c3\u30d7\u3092\u540c\u671f\u7684\u306b\u5b9f\u884c\n\n        This method executes one step and returns immediately.\n        \u3053\u306e\u30e1\u30bd\u30c3\u30c9\u306f1\u30b9\u30c6\u30c3\u30d7\u3092\u5b9f\u884c\u3057\u3066\u3059\u3050\u306b\u8fd4\u308a\u307e\u3059\u3002\n        Use for synchronous CLI applications.\n        \u540c\u671fCLI\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u7528\u306b\u4f7f\u7528\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n        \"\"\"\n        if self.finished:\n            return\n\n        step_name = self.context.next_label\n        if not step_name or step_name not in self.steps:\n            self.context.finish()\n            return\n\n        step = self.steps[step_name]\n\n        # Run step in event loop\n        # \u30a4\u30d9\u30f3\u30c8\u30eb\u30fc\u30d7\u3067\u30b9\u30c6\u30c3\u30d7\u3092\u5b9f\u884c\n        try:\n            loop = asyncio.get_event_loop()\n            if loop.is_running():\n                # If loop is running, create a task\n                # \u30eb\u30fc\u30d7\u304c\u5b9f\u884c\u4e2d\u306e\u5834\u5408\u3001\u30bf\u30b9\u30af\u3092\u4f5c\u6210\n                task = asyncio.create_task(self._execute_step(step, None))\n                # This is a synchronous method, so we can't await\n                # \u3053\u308c\u306f\u540c\u671f\u30e1\u30bd\u30c3\u30c9\u306a\u306e\u3067\u3001await\u3067\u304d\u306a\u3044\n                # The task will run in the background\n                # \u30bf\u30b9\u30af\u306f\u30d0\u30c3\u30af\u30b0\u30e9\u30a6\u30f3\u30c9\u3067\u5b9f\u884c\u3055\u308c\u308b\n            else:\n                # If no loop is running, run until complete\n                # \u30eb\u30fc\u30d7\u304c\u5b9f\u884c\u3055\u308c\u3066\u3044\u306a\u3044\u5834\u5408\u3001\u5b8c\u4e86\u307e\u3067\u5b9f\u884c\n                loop.run_until_complete(self._execute_step(step, None))\n        except Exception as e:\n            logger.error(f\"Error executing step {step_name}: {e}\")\n            self._handle_step_error(step_name, e)\n\n    async def _execute_step(self, step: Step, user_input: Optional[str]) -&gt; None:\n        \"\"\"\n        Execute a single step with hooks and error handling\n        \u30d5\u30c3\u30af\u3068\u30a8\u30e9\u30fc\u30cf\u30f3\u30c9\u30ea\u30f3\u30b0\u3067\u5358\u4e00\u30b9\u30c6\u30c3\u30d7\u3092\u5b9f\u884c\n\n        Args:\n            step: Step to execute / \u5b9f\u884c\u3059\u308b\u30b9\u30c6\u30c3\u30d7\n            user_input: User input if any / \u30e6\u30fc\u30b6\u30fc\u5165\u529b\uff08\u3042\u308c\u3070\uff09\n        \"\"\"\n        step_name = step.name\n\n        # Before step hooks\n        # \u30b9\u30c6\u30c3\u30d7\u524d\u30d5\u30c3\u30af\n        for hook in self.before_step_hooks:\n            try:\n                hook(step_name, self.context)\n            except Exception as e:\n                logger.warning(f\"Before step hook error: {e}\")\n\n        start_time = datetime.now()\n        result = None\n        error = None\n\n        try:\n            # Execute step\n            # \u30b9\u30c6\u30c3\u30d7\u3092\u5b9f\u884c\n            result = await step.run(user_input, self.context)\n            if result != self.context:\n                # Step returned a new context, use it\n                # \u30b9\u30c6\u30c3\u30d7\u304c\u65b0\u3057\u3044\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u3092\u8fd4\u3057\u305f\u5834\u5408\u3001\u305d\u308c\u3092\u4f7f\u7528\n                self.context = result\n\n            logger.debug(f\"Step {step_name} completed in {datetime.now() - start_time}\")\n\n        except Exception as e:\n            error = e\n            logger.error(f\"Step {step_name} failed: {e}\")\n            logger.debug(traceback.format_exc())\n\n            # Add error to context\n            # \u30a8\u30e9\u30fc\u3092\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u306b\u8ffd\u52a0\n            self.context.add_system_message(f\"Step {step_name} failed: {str(e)}\")\n\n            # Call error hooks\n            # \u30a8\u30e9\u30fc\u30d5\u30c3\u30af\u3092\u547c\u3073\u51fa\u3057\n            for hook in self.error_hooks:\n                try:\n                    hook(step_name, self.context, e)\n                except Exception as hook_error:\n                    logger.warning(f\"Error hook failed: {hook_error}\")\n\n            raise e\n\n        finally:\n            # After step hooks\n            # \u30b9\u30c6\u30c3\u30d7\u5f8c\u30d5\u30c3\u30af\n            for hook in self.after_step_hooks:\n                try:\n                    hook(step_name, self.context, result)\n                except Exception as e:\n                    logger.warning(f\"After step hook error: {e}\")\n\n    def _handle_step_error(self, step_name: str, error: Exception) -&gt; None:\n        \"\"\"\n        Handle step execution error\n        \u30b9\u30c6\u30c3\u30d7\u5b9f\u884c\u30a8\u30e9\u30fc\u3092\u51e6\u7406\n\n        Args:\n            step_name: Name of the failed step / \u5931\u6557\u3057\u305f\u30b9\u30c6\u30c3\u30d7\u306e\u540d\u524d\n            error: The error that occurred / \u767a\u751f\u3057\u305f\u30a8\u30e9\u30fc\n        \"\"\"\n        # Mark flow as finished on error\n        # \u30a8\u30e9\u30fc\u6642\u306f\u30d5\u30ed\u30fc\u3092\u5b8c\u4e86\u3068\u3057\u3066\u30de\u30fc\u30af\n        self.context.finish()\n        self.context.set_artifact(\"error\", {\n            \"step\": step_name,\n            \"error\": str(error),\n            \"type\": type(error).__name__\n        })\n\n    def add_hook(\n        self, \n        hook_type: str, \n        callback: Callable\n    ) -&gt; None:\n        \"\"\"\n        Add observability hook\n        \u30aa\u30d6\u30b6\u30fc\u30d0\u30d3\u30ea\u30c6\u30a3\u30d5\u30c3\u30af\u3092\u8ffd\u52a0\n\n        Args:\n            hook_type: Type of hook (\"before_step\", \"after_step\", \"error\") / \u30d5\u30c3\u30af\u30bf\u30a4\u30d7\n            callback: Callback function / \u30b3\u30fc\u30eb\u30d0\u30c3\u30af\u95a2\u6570\n        \"\"\"\n        if hook_type == \"before_step\":\n            self.before_step_hooks.append(callback)\n        elif hook_type == \"after_step\":\n            self.after_step_hooks.append(callback)\n        elif hook_type == \"error\":\n            self.error_hooks.append(callback)\n        else:\n            raise ValueError(f\"Unknown hook type: {hook_type}\")\n\n    def get_step_history(self) -&gt; List[Dict[str, Any]]:\n        \"\"\"\n        Get execution history\n        \u5b9f\u884c\u5c65\u6b74\u3092\u53d6\u5f97\n\n        Returns:\n            List[Dict[str, Any]]: Step execution history / \u30b9\u30c6\u30c3\u30d7\u5b9f\u884c\u5c65\u6b74\n        \"\"\"\n        history = []\n        for msg in self.context.messages:\n            if msg.role == \"system\" and \"Step\" in msg.content:\n                history.append({\n                    \"timestamp\": msg.timestamp,\n                    \"message\": msg.content,\n                    \"metadata\": msg.metadata\n                })\n        return history\n\n    def get_flow_summary(self) -&gt; Dict[str, Any]:\n        \"\"\"\n        Get flow execution summary\n        \u30d5\u30ed\u30fc\u5b9f\u884c\u30b5\u30de\u30ea\u30fc\u3092\u53d6\u5f97\n\n        Returns:\n            Dict[str, Any]: Flow summary / \u30d5\u30ed\u30fc\u30b5\u30de\u30ea\u30fc\n        \"\"\"\n        return {\n            \"trace_id\": self.trace_id,\n            \"start_step\": self.start,\n            \"current_step\": self.current_step_name,\n            \"next_step\": self.next_step_name,\n            \"step_count\": self.context.step_count,\n            \"finished\": self.finished,\n            \"start_time\": self.context.start_time,\n            \"artifacts\": self.context.artifacts,\n            \"message_count\": len(self.context.messages)\n        }\n\n    def reset(self) -&gt; None:\n        \"\"\"\n        Reset flow to initial state\n        \u30d5\u30ed\u30fc\u3092\u521d\u671f\u72b6\u614b\u306b\u30ea\u30bb\u30c3\u30c8\n        \"\"\"\n        self.context = Context(trace_id=self.trace_id)\n        self.context.next_label = self.start\n        self._running = False\n        if self._run_loop_task:\n            self._run_loop_task.cancel()\n            self._run_loop_task = None\n\n    def stop(self) -&gt; None:\n        \"\"\"\n        Stop flow execution\n        \u30d5\u30ed\u30fc\u5b9f\u884c\u3092\u505c\u6b62\n        \"\"\"\n        self._running = False\n        self.context.finish()\n        if self._run_loop_task:\n            self._run_loop_task.cancel()\n            self._run_loop_task = None\n\n    async def start_background_task(self) -&gt; asyncio.Task:\n        \"\"\"\n        Start flow as background task\n        \u30d5\u30ed\u30fc\u3092\u30d0\u30c3\u30af\u30b0\u30e9\u30a6\u30f3\u30c9\u30bf\u30b9\u30af\u3068\u3057\u3066\u958b\u59cb\n\n        Returns:\n            asyncio.Task: Background task / \u30d0\u30c3\u30af\u30b0\u30e9\u30a6\u30f3\u30c9\u30bf\u30b9\u30af\n        \"\"\"\n        if self._run_loop_task and not self._run_loop_task.done():\n            raise RuntimeError(\"Flow is already running as background task\")\n\n        self._run_loop_task = asyncio.create_task(self.run_loop())\n        return self._run_loop_task\n\n    def __str__(self) -&gt; str:\n        \"\"\"String representation of flow\"\"\"\n        return f\"Flow(start={self.start}, steps={len(self.steps)}, finished={self.finished})\"\n\n    def __repr__(self) -&gt; str:\n        return self.__str__()\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.Flow.current_step_name","title":"<code>current_step_name</code>  <code>property</code>","text":"<p>Get current step name \u73fe\u5728\u306e\u30b9\u30c6\u30c3\u30d7\u540d\u3092\u53d6\u5f97</p> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>str | None: Current step name / \u73fe\u5728\u306e\u30b9\u30c6\u30c3\u30d7\u540d</p>"},{"location":"api_reference/#agents_sdk_models.Flow.finished","title":"<code>finished</code>  <code>property</code>","text":"<p>Check if flow is finished \u30d5\u30ed\u30fc\u304c\u5b8c\u4e86\u3057\u3066\u3044\u308b\u304b\u30c1\u30a7\u30c3\u30af</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if finished / \u5b8c\u4e86\u3057\u3066\u3044\u308b\u5834\u5408True</p>"},{"location":"api_reference/#agents_sdk_models.Flow.next_step_name","title":"<code>next_step_name</code>  <code>property</code>","text":"<p>Get next step name \u6b21\u306e\u30b9\u30c6\u30c3\u30d7\u540d\u3092\u53d6\u5f97</p> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>str | None: Next step name / \u6b21\u306e\u30b9\u30c6\u30c3\u30d7\u540d</p>"},{"location":"api_reference/#agents_sdk_models.Flow.__init__","title":"<code>__init__(start=None, steps=None, context=None, max_steps=1000, trace_id=None)</code>","text":"<p>Initialize Flow with flexible step definitions \u67d4\u8edf\u306a\u30b9\u30c6\u30c3\u30d7\u5b9a\u7fa9\u3067Flow\u3092\u521d\u671f\u5316</p> <p>This constructor now supports three ways to define steps: \u3053\u306e\u30b3\u30f3\u30b9\u30c8\u30e9\u30af\u30bf\u306f3\u3064\u306e\u65b9\u6cd5\u3067\u30b9\u30c6\u30c3\u30d7\u3092\u5b9a\u7fa9\u3067\u304d\u307e\u3059\uff1a 1. Traditional: start step name + Dict[str, Step] 2. Sequential: List[Step] (creates sequential workflow) 3. Single: Single Step (creates single-step workflow)</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>Optional[str]</code> <p>Start step label (optional for List/Single mode) / \u958b\u59cb\u30b9\u30c6\u30c3\u30d7\u30e9\u30d9\u30eb\uff08List/Single\u30e2\u30fc\u30c9\u3067\u306f\u7701\u7565\u53ef\uff09</p> <code>None</code> <code>steps</code> <code>Optional[Union[Dict[str, Step], List[Step], Step]]</code> <p>Step definitions - Dict[str, Step], List[Step], or Step / \u30b9\u30c6\u30c3\u30d7\u5b9a\u7fa9 - Dict[str, Step]\u3001List[Step]\u3001\u307e\u305f\u306fStep</p> <code>None</code> <code>context</code> <code>Optional[Context]</code> <p>Initial context (optional) / \u521d\u671f\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\uff08\u30aa\u30d7\u30b7\u30e7\u30f3\uff09</p> <code>None</code> <code>max_steps</code> <code>int</code> <p>Maximum number of steps to prevent infinite loops / \u7121\u9650\u30eb\u30fc\u30d7\u9632\u6b62\u306e\u305f\u3081\u306e\u6700\u5927\u30b9\u30c6\u30c3\u30d7\u6570</p> <code>1000</code> <code>trace_id</code> <code>Optional[str]</code> <p>Trace ID for observability / \u30aa\u30d6\u30b6\u30fc\u30d0\u30d3\u30ea\u30c6\u30a3\u7528\u30c8\u30ec\u30fc\u30b9ID</p> <code>None</code> Source code in <code>src\\agents_sdk_models\\flow.py</code> <pre><code>def __init__(\n    self, \n    start: Optional[str] = None, \n    steps: Optional[Union[Dict[str, Step], List[Step], Step]] = None, \n    context: Optional[Context] = None,\n    max_steps: int = 1000,\n    trace_id: Optional[str] = None\n):\n    \"\"\"\n    Initialize Flow with flexible step definitions\n    \u67d4\u8edf\u306a\u30b9\u30c6\u30c3\u30d7\u5b9a\u7fa9\u3067Flow\u3092\u521d\u671f\u5316\n\n    This constructor now supports three ways to define steps:\n    \u3053\u306e\u30b3\u30f3\u30b9\u30c8\u30e9\u30af\u30bf\u306f3\u3064\u306e\u65b9\u6cd5\u3067\u30b9\u30c6\u30c3\u30d7\u3092\u5b9a\u7fa9\u3067\u304d\u307e\u3059\uff1a\n    1. Traditional: start step name + Dict[str, Step]\n    2. Sequential: List[Step] (creates sequential workflow)\n    3. Single: Single Step (creates single-step workflow)\n\n    Args:\n        start: Start step label (optional for List/Single mode) / \u958b\u59cb\u30b9\u30c6\u30c3\u30d7\u30e9\u30d9\u30eb\uff08List/Single\u30e2\u30fc\u30c9\u3067\u306f\u7701\u7565\u53ef\uff09\n        steps: Step definitions - Dict[str, Step], List[Step], or Step / \u30b9\u30c6\u30c3\u30d7\u5b9a\u7fa9 - Dict[str, Step]\u3001List[Step]\u3001\u307e\u305f\u306fStep\n        context: Initial context (optional) / \u521d\u671f\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\uff08\u30aa\u30d7\u30b7\u30e7\u30f3\uff09\n        max_steps: Maximum number of steps to prevent infinite loops / \u7121\u9650\u30eb\u30fc\u30d7\u9632\u6b62\u306e\u305f\u3081\u306e\u6700\u5927\u30b9\u30c6\u30c3\u30d7\u6570\n        trace_id: Trace ID for observability / \u30aa\u30d6\u30b6\u30fc\u30d0\u30d3\u30ea\u30c6\u30a3\u7528\u30c8\u30ec\u30fc\u30b9ID\n    \"\"\"\n    # Handle flexible step definitions\n    # \u67d4\u8edf\u306a\u30b9\u30c6\u30c3\u30d7\u5b9a\u7fa9\u3092\u51e6\u7406\n    if isinstance(steps, dict):\n        # Traditional mode: Dict[str, Step]\n        # \u5f93\u6765\u30e2\u30fc\u30c9: Dict[str, Step]\n        if start is None:\n            raise ValueError(\"start parameter is required when steps is a dictionary\")\n        self.start = start\n        self.steps = steps\n    elif isinstance(steps, list):\n        # Sequential mode: List[Step] \n        # \u30b7\u30fc\u30b1\u30f3\u30b7\u30e3\u30eb\u30e2\u30fc\u30c9: List[Step]\n        if not steps:\n            raise ValueError(\"Steps list cannot be empty\")\n        self.steps = {}\n        prev_step_name = None\n\n        for i, step in enumerate(steps):\n            if not hasattr(step, 'name'):\n                raise ValueError(f\"Step at index {i} must have a 'name' attribute\")\n\n            step_name = step.name\n            self.steps[step_name] = step\n\n            # Set sequential flow: each step goes to next step\n            # \u30b7\u30fc\u30b1\u30f3\u30b7\u30e3\u30eb\u30d5\u30ed\u30fc\u8a2d\u5b9a: \u5404\u30b9\u30c6\u30c3\u30d7\u304c\u6b21\u306e\u30b9\u30c6\u30c3\u30d7\u306b\u9032\u3080\n            if prev_step_name is not None and hasattr(self.steps[prev_step_name], 'next_step'):\n                if self.steps[prev_step_name].next_step is None:\n                    self.steps[prev_step_name].next_step = step_name\n\n            prev_step_name = step_name\n\n        # Start with first step\n        # \u6700\u521d\u306e\u30b9\u30c6\u30c3\u30d7\u304b\u3089\u958b\u59cb\n        self.start = steps[0].name\n\n    elif steps is not None:\n        # Check if it's a Step instance\n        # Step\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u304b\u3069\u3046\u304b\u3092\u30c1\u30a7\u30c3\u30af\n        if isinstance(steps, Step):\n            # Single step mode: Step\n            # \u5358\u4e00\u30b9\u30c6\u30c3\u30d7\u30e2\u30fc\u30c9: Step\n            if not hasattr(steps, 'name'):\n                raise ValueError(\"Step must have a 'name' attribute\")\n\n            step_name = steps.name\n            self.start = step_name\n            self.steps = {step_name: steps}\n        else:\n            # Not a valid type\n            # \u6709\u52b9\u306a\u30bf\u30a4\u30d7\u3067\u306f\u306a\u3044\n            raise ValueError(\"steps must be Dict[str, Step], List[Step], or Step\")\n    else:\n        raise ValueError(\"steps parameter cannot be None\")\n\n    self.context = context or Context()\n    self.max_steps = max_steps\n    self.trace_id = trace_id or f\"flow_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n\n    # Initialize context\n    # \u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u3092\u521d\u671f\u5316\n    self.context.trace_id = self.trace_id\n    self.context.next_label = self.start\n\n    # Execution state\n    # \u5b9f\u884c\u72b6\u614b\n    self._running = False\n    self._run_loop_task: Optional[asyncio.Task] = None\n    self._execution_lock = asyncio.Lock()\n\n    # Hooks for observability\n    # \u30aa\u30d6\u30b6\u30fc\u30d0\u30d3\u30ea\u30c6\u30a3\u7528\u30d5\u30c3\u30af\n    self.before_step_hooks: List[Callable[[str, Context], None]] = []\n    self.after_step_hooks: List[Callable[[str, Context, Any], None]] = []\n    self.error_hooks: List[Callable[[str, Context, Exception], None]] = []\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.Flow.__str__","title":"<code>__str__()</code>","text":"<p>String representation of flow</p> Source code in <code>src\\agents_sdk_models\\flow.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"String representation of flow\"\"\"\n    return f\"Flow(start={self.start}, steps={len(self.steps)}, finished={self.finished})\"\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.Flow.add_hook","title":"<code>add_hook(hook_type, callback)</code>","text":"<p>Add observability hook \u30aa\u30d6\u30b6\u30fc\u30d0\u30d3\u30ea\u30c6\u30a3\u30d5\u30c3\u30af\u3092\u8ffd\u52a0</p> <p>Parameters:</p> Name Type Description Default <code>hook_type</code> <code>str</code> <p>Type of hook (\"before_step\", \"after_step\", \"error\") / \u30d5\u30c3\u30af\u30bf\u30a4\u30d7</p> required <code>callback</code> <code>Callable</code> <p>Callback function / \u30b3\u30fc\u30eb\u30d0\u30c3\u30af\u95a2\u6570</p> required Source code in <code>src\\agents_sdk_models\\flow.py</code> <pre><code>def add_hook(\n    self, \n    hook_type: str, \n    callback: Callable\n) -&gt; None:\n    \"\"\"\n    Add observability hook\n    \u30aa\u30d6\u30b6\u30fc\u30d0\u30d3\u30ea\u30c6\u30a3\u30d5\u30c3\u30af\u3092\u8ffd\u52a0\n\n    Args:\n        hook_type: Type of hook (\"before_step\", \"after_step\", \"error\") / \u30d5\u30c3\u30af\u30bf\u30a4\u30d7\n        callback: Callback function / \u30b3\u30fc\u30eb\u30d0\u30c3\u30af\u95a2\u6570\n    \"\"\"\n    if hook_type == \"before_step\":\n        self.before_step_hooks.append(callback)\n    elif hook_type == \"after_step\":\n        self.after_step_hooks.append(callback)\n    elif hook_type == \"error\":\n        self.error_hooks.append(callback)\n    else:\n        raise ValueError(f\"Unknown hook type: {hook_type}\")\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.Flow.feed","title":"<code>feed(user_input)</code>","text":"<p>Provide user input to the flow \u30d5\u30ed\u30fc\u306b\u30e6\u30fc\u30b6\u30fc\u5165\u529b\u3092\u63d0\u4f9b</p> <p>Parameters:</p> Name Type Description Default <code>user_input</code> <code>str</code> <p>User input text / \u30e6\u30fc\u30b6\u30fc\u5165\u529b\u30c6\u30ad\u30b9\u30c8</p> required Source code in <code>src\\agents_sdk_models\\flow.py</code> <pre><code>def feed(self, user_input: str) -&gt; None:\n    \"\"\"\n    Provide user input to the flow\n    \u30d5\u30ed\u30fc\u306b\u30e6\u30fc\u30b6\u30fc\u5165\u529b\u3092\u63d0\u4f9b\n\n    Args:\n        user_input: User input text / \u30e6\u30fc\u30b6\u30fc\u5165\u529b\u30c6\u30ad\u30b9\u30c8\n    \"\"\"\n    self.context.provide_user_input(user_input)\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.Flow.get_flow_summary","title":"<code>get_flow_summary()</code>","text":"<p>Get flow execution summary \u30d5\u30ed\u30fc\u5b9f\u884c\u30b5\u30de\u30ea\u30fc\u3092\u53d6\u5f97</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: Flow summary / \u30d5\u30ed\u30fc\u30b5\u30de\u30ea\u30fc</p> Source code in <code>src\\agents_sdk_models\\flow.py</code> <pre><code>def get_flow_summary(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Get flow execution summary\n    \u30d5\u30ed\u30fc\u5b9f\u884c\u30b5\u30de\u30ea\u30fc\u3092\u53d6\u5f97\n\n    Returns:\n        Dict[str, Any]: Flow summary / \u30d5\u30ed\u30fc\u30b5\u30de\u30ea\u30fc\n    \"\"\"\n    return {\n        \"trace_id\": self.trace_id,\n        \"start_step\": self.start,\n        \"current_step\": self.current_step_name,\n        \"next_step\": self.next_step_name,\n        \"step_count\": self.context.step_count,\n        \"finished\": self.finished,\n        \"start_time\": self.context.start_time,\n        \"artifacts\": self.context.artifacts,\n        \"message_count\": len(self.context.messages)\n    }\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.Flow.get_step_history","title":"<code>get_step_history()</code>","text":"<p>Get execution history \u5b9f\u884c\u5c65\u6b74\u3092\u53d6\u5f97</p> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List[Dict[str, Any]]: Step execution history / \u30b9\u30c6\u30c3\u30d7\u5b9f\u884c\u5c65\u6b74</p> Source code in <code>src\\agents_sdk_models\\flow.py</code> <pre><code>def get_step_history(self) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Get execution history\n    \u5b9f\u884c\u5c65\u6b74\u3092\u53d6\u5f97\n\n    Returns:\n        List[Dict[str, Any]]: Step execution history / \u30b9\u30c6\u30c3\u30d7\u5b9f\u884c\u5c65\u6b74\n    \"\"\"\n    history = []\n    for msg in self.context.messages:\n        if msg.role == \"system\" and \"Step\" in msg.content:\n            history.append({\n                \"timestamp\": msg.timestamp,\n                \"message\": msg.content,\n                \"metadata\": msg.metadata\n            })\n    return history\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.Flow.next_prompt","title":"<code>next_prompt()</code>","text":"<p>Get next prompt for synchronous CLI usage \u540c\u671fCLI\u4f7f\u7528\u7528\u306e\u6b21\u306e\u30d7\u30ed\u30f3\u30d7\u30c8\u3092\u53d6\u5f97</p> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>str | None: Prompt if waiting for user input / \u30e6\u30fc\u30b6\u30fc\u5165\u529b\u5f85\u3061\u306e\u5834\u5408\u306e\u30d7\u30ed\u30f3\u30d7\u30c8</p> Source code in <code>src\\agents_sdk_models\\flow.py</code> <pre><code>def next_prompt(self) -&gt; Optional[str]:\n    \"\"\"\n    Get next prompt for synchronous CLI usage\n    \u540c\u671fCLI\u4f7f\u7528\u7528\u306e\u6b21\u306e\u30d7\u30ed\u30f3\u30d7\u30c8\u3092\u53d6\u5f97\n\n    Returns:\n        str | None: Prompt if waiting for user input / \u30e6\u30fc\u30b6\u30fc\u5165\u529b\u5f85\u3061\u306e\u5834\u5408\u306e\u30d7\u30ed\u30f3\u30d7\u30c8\n    \"\"\"\n    return self.context.clear_prompt()\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.Flow.reset","title":"<code>reset()</code>","text":"<p>Reset flow to initial state \u30d5\u30ed\u30fc\u3092\u521d\u671f\u72b6\u614b\u306b\u30ea\u30bb\u30c3\u30c8</p> Source code in <code>src\\agents_sdk_models\\flow.py</code> <pre><code>def reset(self) -&gt; None:\n    \"\"\"\n    Reset flow to initial state\n    \u30d5\u30ed\u30fc\u3092\u521d\u671f\u72b6\u614b\u306b\u30ea\u30bb\u30c3\u30c8\n    \"\"\"\n    self.context = Context(trace_id=self.trace_id)\n    self.context.next_label = self.start\n    self._running = False\n    if self._run_loop_task:\n        self._run_loop_task.cancel()\n        self._run_loop_task = None\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.Flow.run","title":"<code>run(input_data=None, initial_input=None)</code>  <code>async</code>","text":"<p>Run flow to completion without user input coordination \u30e6\u30fc\u30b6\u30fc\u5165\u529b\u8abf\u6574\u306a\u3057\u3067\u30d5\u30ed\u30fc\u3092\u5b8c\u4e86\u307e\u3067\u5b9f\u884c</p> <p>This is for non-interactive workflows that don't require user input. \u3053\u308c\u306f\u30e6\u30fc\u30b6\u30fc\u5165\u529b\u304c\u4e0d\u8981\u306a\u975e\u5bfe\u8a71\u7684\u30ef\u30fc\u30af\u30d5\u30ed\u30fc\u7528\u3067\u3059\u3002</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>Optional[str]</code> <p>Input data to the flow (preferred parameter name) / \u30d5\u30ed\u30fc\u3078\u306e\u5165\u529b\u30c7\u30fc\u30bf\uff08\u63a8\u5968\u30d1\u30e9\u30e1\u30fc\u30bf\u540d\uff09</p> <code>None</code> <code>initial_input</code> <code>Optional[str]</code> <p>Initial input to the flow (deprecated, use input_data) / \u30d5\u30ed\u30fc\u3078\u306e\u521d\u671f\u5165\u529b\uff08\u975e\u63a8\u5968\u3001input_data\u3092\u4f7f\u7528\uff09</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Context</code> <code>Context</code> <p>Final context / \u6700\u7d42\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8</p> <p>Raises:</p> Type Description <code>FlowExecutionError</code> <p>If execution fails / \u5b9f\u884c\u5931\u6557\u6642</p> Source code in <code>src\\agents_sdk_models\\flow.py</code> <pre><code>async def run(self, input_data: Optional[str] = None, initial_input: Optional[str] = None) -&gt; Context:\n    \"\"\"\n    Run flow to completion without user input coordination\n    \u30e6\u30fc\u30b6\u30fc\u5165\u529b\u8abf\u6574\u306a\u3057\u3067\u30d5\u30ed\u30fc\u3092\u5b8c\u4e86\u307e\u3067\u5b9f\u884c\n\n    This is for non-interactive workflows that don't require user input.\n    \u3053\u308c\u306f\u30e6\u30fc\u30b6\u30fc\u5165\u529b\u304c\u4e0d\u8981\u306a\u975e\u5bfe\u8a71\u7684\u30ef\u30fc\u30af\u30d5\u30ed\u30fc\u7528\u3067\u3059\u3002\n\n    Args:\n        input_data: Input data to the flow (preferred parameter name) / \u30d5\u30ed\u30fc\u3078\u306e\u5165\u529b\u30c7\u30fc\u30bf\uff08\u63a8\u5968\u30d1\u30e9\u30e1\u30fc\u30bf\u540d\uff09\n        initial_input: Initial input to the flow (deprecated, use input_data) / \u30d5\u30ed\u30fc\u3078\u306e\u521d\u671f\u5165\u529b\uff08\u975e\u63a8\u5968\u3001input_data\u3092\u4f7f\u7528\uff09\n\n    Returns:\n        Context: Final context / \u6700\u7d42\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\n\n    Raises:\n        FlowExecutionError: If execution fails / \u5b9f\u884c\u5931\u6557\u6642\n    \"\"\"\n    async with self._execution_lock:\n        try:\n            self._running = True\n\n            # Reset context for new execution\n            # \u65b0\u3057\u3044\u5b9f\u884c\u7528\u306b\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u3092\u30ea\u30bb\u30c3\u30c8\n            if self.context.step_count &gt; 0:\n                self.context = Context(trace_id=self.trace_id)\n                self.context.next_label = self.start\n\n            # Determine input to use (input_data takes precedence)\n            # \u4f7f\u7528\u3059\u308b\u5165\u529b\u3092\u6c7a\u5b9a\uff08input_data\u304c\u512a\u5148\uff09\n            effective_input = input_data or initial_input\n\n            # Add input if provided\n            # \u5165\u529b\u304c\u63d0\u4f9b\u3055\u308c\u3066\u3044\u308b\u5834\u5408\u306f\u8ffd\u52a0\n            if effective_input:\n                self.context.add_user_message(effective_input)\n\n            current_input = effective_input\n            step_count = 0\n\n            while not self.finished and step_count &lt; self.max_steps:\n                step_name = self.context.next_label\n                if not step_name or step_name not in self.steps:\n                    break\n\n                step = self.steps[step_name]\n\n                # Execute step\n                # \u30b9\u30c6\u30c3\u30d7\u3092\u5b9f\u884c\n                try:\n                    await self._execute_step(step, current_input)\n                    current_input = None  # Only use initial input for first step\n                    step_count += 1\n\n                    # If step is waiting for user input, break\n                    # \u30b9\u30c6\u30c3\u30d7\u304c\u30e6\u30fc\u30b6\u30fc\u5165\u529b\u3092\u5f85\u6a5f\u3057\u3066\u3044\u308b\u5834\u5408\u3001\u4e2d\u65ad\n                    if self.context.awaiting_user_input:\n                        break\n\n                except Exception as e:\n                    logger.error(f\"Error executing step {step_name}: {e}\")\n                    self._handle_step_error(step_name, e)\n                    break\n\n            # Check for infinite loop\n            # \u7121\u9650\u30eb\u30fc\u30d7\u306e\u30c1\u30a7\u30c3\u30af\n            if step_count &gt;= self.max_steps:\n                raise FlowExecutionError(f\"Flow exceeded maximum steps ({self.max_steps})\")\n\n            return self.context\n\n        finally:\n            self._running = False\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.Flow.run_loop","title":"<code>run_loop()</code>  <code>async</code>","text":"<p>Run flow as background task with user input coordination \u30e6\u30fc\u30b6\u30fc\u5165\u529b\u8abf\u6574\u3092\u542b\u3080\u30d0\u30c3\u30af\u30b0\u30e9\u30a6\u30f3\u30c9\u30bf\u30b9\u30af\u3068\u3057\u3066\u30d5\u30ed\u30fc\u3092\u5b9f\u884c</p> <p>This method runs the flow continuously, pausing when user input is needed. \u3053\u306e\u30e1\u30bd\u30c3\u30c9\u306f\u30d5\u30ed\u30fc\u3092\u7d99\u7d9a\u7684\u306b\u5b9f\u884c\u3057\u3001\u30e6\u30fc\u30b6\u30fc\u5165\u529b\u304c\u5fc5\u8981\u306a\u6642\u306b\u4e00\u6642\u505c\u6b62\u3057\u307e\u3059\u3002 Use feed() to provide user input when the flow is waiting. \u30d5\u30ed\u30fc\u304c\u5f85\u6a5f\u3057\u3066\u3044\u308b\u6642\u306ffeed()\u3092\u4f7f\u7528\u3057\u3066\u30e6\u30fc\u30b6\u30fc\u5165\u529b\u3092\u63d0\u4f9b\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> Source code in <code>src\\agents_sdk_models\\flow.py</code> <pre><code>async def run_loop(self) -&gt; None:\n    \"\"\"\n    Run flow as background task with user input coordination\n    \u30e6\u30fc\u30b6\u30fc\u5165\u529b\u8abf\u6574\u3092\u542b\u3080\u30d0\u30c3\u30af\u30b0\u30e9\u30a6\u30f3\u30c9\u30bf\u30b9\u30af\u3068\u3057\u3066\u30d5\u30ed\u30fc\u3092\u5b9f\u884c\n\n    This method runs the flow continuously, pausing when user input is needed.\n    \u3053\u306e\u30e1\u30bd\u30c3\u30c9\u306f\u30d5\u30ed\u30fc\u3092\u7d99\u7d9a\u7684\u306b\u5b9f\u884c\u3057\u3001\u30e6\u30fc\u30b6\u30fc\u5165\u529b\u304c\u5fc5\u8981\u306a\u6642\u306b\u4e00\u6642\u505c\u6b62\u3057\u307e\u3059\u3002\n    Use feed() to provide user input when the flow is waiting.\n    \u30d5\u30ed\u30fc\u304c\u5f85\u6a5f\u3057\u3066\u3044\u308b\u6642\u306ffeed()\u3092\u4f7f\u7528\u3057\u3066\u30e6\u30fc\u30b6\u30fc\u5165\u529b\u3092\u63d0\u4f9b\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n    \"\"\"\n    async with self._execution_lock:\n        try:\n            self._running = True\n\n            # Reset context for new execution\n            # \u65b0\u3057\u3044\u5b9f\u884c\u7528\u306b\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u3092\u30ea\u30bb\u30c3\u30c8\n            if self.context.step_count &gt; 0:\n                self.context = Context(trace_id=self.trace_id)\n                self.context.next_label = self.start\n\n            step_count = 0\n            current_input = None\n\n            while not self.finished and step_count &lt; self.max_steps:\n                step_name = self.context.next_label\n                if not step_name or step_name not in self.steps:\n                    break\n\n                step = self.steps[step_name]\n\n                # Execute step\n                # \u30b9\u30c6\u30c3\u30d7\u3092\u5b9f\u884c\n                try:\n                    await self._execute_step(step, current_input)\n                    current_input = None\n                    step_count += 1\n\n                    # If step is waiting for user input, wait for feed()\n                    # \u30b9\u30c6\u30c3\u30d7\u304c\u30e6\u30fc\u30b6\u30fc\u5165\u529b\u3092\u5f85\u6a5f\u3057\u3066\u3044\u308b\u5834\u5408\u3001feed()\u3092\u5f85\u3064\n                    if self.context.awaiting_user_input:\n                        await self.context.wait_for_user_input()\n                        # After receiving input, continue with the same step\n                        # \u5165\u529b\u53d7\u4fe1\u5f8c\u3001\u540c\u3058\u30b9\u30c6\u30c3\u30d7\u3067\u7d99\u7d9a\n                        current_input = self.context.last_user_input\n                        continue\n\n                except Exception as e:\n                    logger.error(f\"Error executing step {step_name}: {e}\")\n                    self._handle_step_error(step_name, e)\n                    break\n\n            # Check for infinite loop\n            # \u7121\u9650\u30eb\u30fc\u30d7\u306e\u30c1\u30a7\u30c3\u30af\n            if step_count &gt;= self.max_steps:\n                raise FlowExecutionError(f\"Flow exceeded maximum steps ({self.max_steps})\")\n\n        finally:\n            self._running = False\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.Flow.start_background_task","title":"<code>start_background_task()</code>  <code>async</code>","text":"<p>Start flow as background task \u30d5\u30ed\u30fc\u3092\u30d0\u30c3\u30af\u30b0\u30e9\u30a6\u30f3\u30c9\u30bf\u30b9\u30af\u3068\u3057\u3066\u958b\u59cb</p> <p>Returns:</p> Type Description <code>Task</code> <p>asyncio.Task: Background task / \u30d0\u30c3\u30af\u30b0\u30e9\u30a6\u30f3\u30c9\u30bf\u30b9\u30af</p> Source code in <code>src\\agents_sdk_models\\flow.py</code> <pre><code>async def start_background_task(self) -&gt; asyncio.Task:\n    \"\"\"\n    Start flow as background task\n    \u30d5\u30ed\u30fc\u3092\u30d0\u30c3\u30af\u30b0\u30e9\u30a6\u30f3\u30c9\u30bf\u30b9\u30af\u3068\u3057\u3066\u958b\u59cb\n\n    Returns:\n        asyncio.Task: Background task / \u30d0\u30c3\u30af\u30b0\u30e9\u30a6\u30f3\u30c9\u30bf\u30b9\u30af\n    \"\"\"\n    if self._run_loop_task and not self._run_loop_task.done():\n        raise RuntimeError(\"Flow is already running as background task\")\n\n    self._run_loop_task = asyncio.create_task(self.run_loop())\n    return self._run_loop_task\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.Flow.step","title":"<code>step()</code>","text":"<p>Execute one step synchronously 1\u30b9\u30c6\u30c3\u30d7\u3092\u540c\u671f\u7684\u306b\u5b9f\u884c</p> <p>This method executes one step and returns immediately. \u3053\u306e\u30e1\u30bd\u30c3\u30c9\u306f1\u30b9\u30c6\u30c3\u30d7\u3092\u5b9f\u884c\u3057\u3066\u3059\u3050\u306b\u8fd4\u308a\u307e\u3059\u3002 Use for synchronous CLI applications. \u540c\u671fCLI\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u7528\u306b\u4f7f\u7528\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> Source code in <code>src\\agents_sdk_models\\flow.py</code> <pre><code>def step(self) -&gt; None:\n    \"\"\"\n    Execute one step synchronously\n    1\u30b9\u30c6\u30c3\u30d7\u3092\u540c\u671f\u7684\u306b\u5b9f\u884c\n\n    This method executes one step and returns immediately.\n    \u3053\u306e\u30e1\u30bd\u30c3\u30c9\u306f1\u30b9\u30c6\u30c3\u30d7\u3092\u5b9f\u884c\u3057\u3066\u3059\u3050\u306b\u8fd4\u308a\u307e\u3059\u3002\n    Use for synchronous CLI applications.\n    \u540c\u671fCLI\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u7528\u306b\u4f7f\u7528\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n    \"\"\"\n    if self.finished:\n        return\n\n    step_name = self.context.next_label\n    if not step_name or step_name not in self.steps:\n        self.context.finish()\n        return\n\n    step = self.steps[step_name]\n\n    # Run step in event loop\n    # \u30a4\u30d9\u30f3\u30c8\u30eb\u30fc\u30d7\u3067\u30b9\u30c6\u30c3\u30d7\u3092\u5b9f\u884c\n    try:\n        loop = asyncio.get_event_loop()\n        if loop.is_running():\n            # If loop is running, create a task\n            # \u30eb\u30fc\u30d7\u304c\u5b9f\u884c\u4e2d\u306e\u5834\u5408\u3001\u30bf\u30b9\u30af\u3092\u4f5c\u6210\n            task = asyncio.create_task(self._execute_step(step, None))\n            # This is a synchronous method, so we can't await\n            # \u3053\u308c\u306f\u540c\u671f\u30e1\u30bd\u30c3\u30c9\u306a\u306e\u3067\u3001await\u3067\u304d\u306a\u3044\n            # The task will run in the background\n            # \u30bf\u30b9\u30af\u306f\u30d0\u30c3\u30af\u30b0\u30e9\u30a6\u30f3\u30c9\u3067\u5b9f\u884c\u3055\u308c\u308b\n        else:\n            # If no loop is running, run until complete\n            # \u30eb\u30fc\u30d7\u304c\u5b9f\u884c\u3055\u308c\u3066\u3044\u306a\u3044\u5834\u5408\u3001\u5b8c\u4e86\u307e\u3067\u5b9f\u884c\n            loop.run_until_complete(self._execute_step(step, None))\n    except Exception as e:\n        logger.error(f\"Error executing step {step_name}: {e}\")\n        self._handle_step_error(step_name, e)\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.Flow.stop","title":"<code>stop()</code>","text":"<p>Stop flow execution \u30d5\u30ed\u30fc\u5b9f\u884c\u3092\u505c\u6b62</p> Source code in <code>src\\agents_sdk_models\\flow.py</code> <pre><code>def stop(self) -&gt; None:\n    \"\"\"\n    Stop flow execution\n    \u30d5\u30ed\u30fc\u5b9f\u884c\u3092\u505c\u6b62\n    \"\"\"\n    self._running = False\n    self.context.finish()\n    if self._run_loop_task:\n        self._run_loop_task.cancel()\n        self._run_loop_task = None\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.FlowExecutionError","title":"<code>FlowExecutionError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Exception raised during flow execution \u30d5\u30ed\u30fc\u5b9f\u884c\u4e2d\u306b\u767a\u751f\u3059\u308b\u4f8b\u5916</p> Source code in <code>src\\agents_sdk_models\\flow.py</code> <pre><code>class FlowExecutionError(Exception):\n    \"\"\"\n    Exception raised during flow execution\n    \u30d5\u30ed\u30fc\u5b9f\u884c\u4e2d\u306b\u767a\u751f\u3059\u308b\u4f8b\u5916\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.ForkStep","title":"<code>ForkStep</code>","text":"<p>               Bases: <code>Step</code></p> <p>Step that executes multiple branches in parallel \u8907\u6570\u306e\u30d6\u30e9\u30f3\u30c1\u3092\u4e26\u5217\u5b9f\u884c\u3059\u308b\u30b9\u30c6\u30c3\u30d7</p> <p>This step starts multiple sub-flows concurrently and collects their results. \u3053\u306e\u30b9\u30c6\u30c3\u30d7\u306f\u8907\u6570\u306e\u30b5\u30d6\u30d5\u30ed\u30fc\u3092\u540c\u6642\u306b\u958b\u59cb\u3057\u3001\u7d50\u679c\u3092\u53ce\u96c6\u3057\u307e\u3059\u3002</p> Source code in <code>src\\agents_sdk_models\\step.py</code> <pre><code>class ForkStep(Step):\n    \"\"\"\n    Step that executes multiple branches in parallel\n    \u8907\u6570\u306e\u30d6\u30e9\u30f3\u30c1\u3092\u4e26\u5217\u5b9f\u884c\u3059\u308b\u30b9\u30c6\u30c3\u30d7\n\n    This step starts multiple sub-flows concurrently and collects their results.\n    \u3053\u306e\u30b9\u30c6\u30c3\u30d7\u306f\u8907\u6570\u306e\u30b5\u30d6\u30d5\u30ed\u30fc\u3092\u540c\u6642\u306b\u958b\u59cb\u3057\u3001\u7d50\u679c\u3092\u53ce\u96c6\u3057\u307e\u3059\u3002\n    \"\"\"\n\n    def __init__(self, name: str, branches: List[str], join_step: str):\n        \"\"\"\n        Initialize fork step\n        \u30d5\u30a9\u30fc\u30af\u30b9\u30c6\u30c3\u30d7\u3092\u521d\u671f\u5316\n\n        Args:\n            name: Step name / \u30b9\u30c6\u30c3\u30d7\u540d\n            branches: List of branch step names to execute in parallel / \u4e26\u5217\u5b9f\u884c\u3059\u308b\u30d6\u30e9\u30f3\u30c1\u30b9\u30c6\u30c3\u30d7\u540d\u306e\u30ea\u30b9\u30c8\n            join_step: Step to join results / \u7d50\u679c\u3092\u7d50\u5408\u3059\u308b\u30b9\u30c6\u30c3\u30d7\n        \"\"\"\n        super().__init__(name)\n        self.branches = branches\n        self.join_step = join_step\n\n    async def run(self, user_input: Optional[str], ctx: Context) -&gt; Context:\n        \"\"\"\n        Execute fork step\n        \u30d5\u30a9\u30fc\u30af\u30b9\u30c6\u30c3\u30d7\u3092\u5b9f\u884c\n\n        Args:\n            user_input: User input / \u30e6\u30fc\u30b6\u30fc\u5165\u529b\n            ctx: Current context / \u73fe\u5728\u306e\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\n\n        Returns:\n            Context: Updated context / \u66f4\u65b0\u6e08\u307f\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\n        \"\"\"\n        ctx.update_step_info(self.name)\n\n        # Store branch information for join step\n        # \u30b8\u30e7\u30a4\u30f3\u30b9\u30c6\u30c3\u30d7\u7528\u306b\u30d6\u30e9\u30f3\u30c1\u60c5\u5831\u3092\u4fdd\u5b58\n        ctx.shared_state[f\"{self.name}_branches\"] = self.branches\n        ctx.shared_state[f\"{self.name}_started\"] = True\n\n        # For now, just route to the join step\n        # \u73fe\u5728\u306e\u3068\u3053\u308d\u3001\u30b8\u30e7\u30a4\u30f3\u30b9\u30c6\u30c3\u30d7\u306b\u30eb\u30fc\u30c6\u30a3\u30f3\u30b0\u3059\u308b\u3060\u3051\n        # In a full implementation, this would start parallel execution\n        # \u5b8c\u5168\u306a\u5b9f\u88c5\u3067\u306f\u3001\u3053\u308c\u306f\u4e26\u5217\u5b9f\u884c\u3092\u958b\u59cb\u3059\u308b\n        ctx.goto(self.join_step)\n\n        return ctx\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.ForkStep.__init__","title":"<code>__init__(name, branches, join_step)</code>","text":"<p>Initialize fork step \u30d5\u30a9\u30fc\u30af\u30b9\u30c6\u30c3\u30d7\u3092\u521d\u671f\u5316</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Step name / \u30b9\u30c6\u30c3\u30d7\u540d</p> required <code>branches</code> <code>List[str]</code> <p>List of branch step names to execute in parallel / \u4e26\u5217\u5b9f\u884c\u3059\u308b\u30d6\u30e9\u30f3\u30c1\u30b9\u30c6\u30c3\u30d7\u540d\u306e\u30ea\u30b9\u30c8</p> required <code>join_step</code> <code>str</code> <p>Step to join results / \u7d50\u679c\u3092\u7d50\u5408\u3059\u308b\u30b9\u30c6\u30c3\u30d7</p> required Source code in <code>src\\agents_sdk_models\\step.py</code> <pre><code>def __init__(self, name: str, branches: List[str], join_step: str):\n    \"\"\"\n    Initialize fork step\n    \u30d5\u30a9\u30fc\u30af\u30b9\u30c6\u30c3\u30d7\u3092\u521d\u671f\u5316\n\n    Args:\n        name: Step name / \u30b9\u30c6\u30c3\u30d7\u540d\n        branches: List of branch step names to execute in parallel / \u4e26\u5217\u5b9f\u884c\u3059\u308b\u30d6\u30e9\u30f3\u30c1\u30b9\u30c6\u30c3\u30d7\u540d\u306e\u30ea\u30b9\u30c8\n        join_step: Step to join results / \u7d50\u679c\u3092\u7d50\u5408\u3059\u308b\u30b9\u30c6\u30c3\u30d7\n    \"\"\"\n    super().__init__(name)\n    self.branches = branches\n    self.join_step = join_step\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.ForkStep.run","title":"<code>run(user_input, ctx)</code>  <code>async</code>","text":"<p>Execute fork step \u30d5\u30a9\u30fc\u30af\u30b9\u30c6\u30c3\u30d7\u3092\u5b9f\u884c</p> <p>Parameters:</p> Name Type Description Default <code>user_input</code> <code>Optional[str]</code> <p>User input / \u30e6\u30fc\u30b6\u30fc\u5165\u529b</p> required <code>ctx</code> <code>Context</code> <p>Current context / \u73fe\u5728\u306e\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8</p> required <p>Returns:</p> Name Type Description <code>Context</code> <code>Context</code> <p>Updated context / \u66f4\u65b0\u6e08\u307f\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8</p> Source code in <code>src\\agents_sdk_models\\step.py</code> <pre><code>async def run(self, user_input: Optional[str], ctx: Context) -&gt; Context:\n    \"\"\"\n    Execute fork step\n    \u30d5\u30a9\u30fc\u30af\u30b9\u30c6\u30c3\u30d7\u3092\u5b9f\u884c\n\n    Args:\n        user_input: User input / \u30e6\u30fc\u30b6\u30fc\u5165\u529b\n        ctx: Current context / \u73fe\u5728\u306e\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\n\n    Returns:\n        Context: Updated context / \u66f4\u65b0\u6e08\u307f\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\n    \"\"\"\n    ctx.update_step_info(self.name)\n\n    # Store branch information for join step\n    # \u30b8\u30e7\u30a4\u30f3\u30b9\u30c6\u30c3\u30d7\u7528\u306b\u30d6\u30e9\u30f3\u30c1\u60c5\u5831\u3092\u4fdd\u5b58\n    ctx.shared_state[f\"{self.name}_branches\"] = self.branches\n    ctx.shared_state[f\"{self.name}_started\"] = True\n\n    # For now, just route to the join step\n    # \u73fe\u5728\u306e\u3068\u3053\u308d\u3001\u30b8\u30e7\u30a4\u30f3\u30b9\u30c6\u30c3\u30d7\u306b\u30eb\u30fc\u30c6\u30a3\u30f3\u30b0\u3059\u308b\u3060\u3051\n    # In a full implementation, this would start parallel execution\n    # \u5b8c\u5168\u306a\u5b9f\u88c5\u3067\u306f\u3001\u3053\u308c\u306f\u4e26\u5217\u5b9f\u884c\u3092\u958b\u59cb\u3059\u308b\n    ctx.goto(self.join_step)\n\n    return ctx\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.FunctionStep","title":"<code>FunctionStep</code>","text":"<p>               Bases: <code>Step</code></p> <p>Step that executes a custom function \u30ab\u30b9\u30bf\u30e0\u95a2\u6570\u3092\u5b9f\u884c\u3059\u308b\u30b9\u30c6\u30c3\u30d7</p> <p>This step allows executing arbitrary code within the workflow. \u3053\u306e\u30b9\u30c6\u30c3\u30d7\u306f\u30ef\u30fc\u30af\u30d5\u30ed\u30fc\u5185\u3067\u4efb\u610f\u306e\u30b3\u30fc\u30c9\u3092\u5b9f\u884c\u3067\u304d\u307e\u3059\u3002</p> Source code in <code>src\\agents_sdk_models\\step.py</code> <pre><code>class FunctionStep(Step):\n    \"\"\"\n    Step that executes a custom function\n    \u30ab\u30b9\u30bf\u30e0\u95a2\u6570\u3092\u5b9f\u884c\u3059\u308b\u30b9\u30c6\u30c3\u30d7\n\n    This step allows executing arbitrary code within the workflow.\n    \u3053\u306e\u30b9\u30c6\u30c3\u30d7\u306f\u30ef\u30fc\u30af\u30d5\u30ed\u30fc\u5185\u3067\u4efb\u610f\u306e\u30b3\u30fc\u30c9\u3092\u5b9f\u884c\u3067\u304d\u307e\u3059\u3002\n    \"\"\"\n\n    def __init__(\n        self, \n        name: str, \n        function: Callable[[Optional[str], Context], Union[Context, Awaitable[Context]]], \n        next_step: Optional[str] = None\n    ):\n        \"\"\"\n        Initialize function step\n        \u95a2\u6570\u30b9\u30c6\u30c3\u30d7\u3092\u521d\u671f\u5316\n\n        Args:\n            name: Step name / \u30b9\u30c6\u30c3\u30d7\u540d\n            function: Function to execute / \u5b9f\u884c\u3059\u308b\u95a2\u6570\n            next_step: Next step after execution / \u5b9f\u884c\u5f8c\u306e\u6b21\u30b9\u30c6\u30c3\u30d7\n        \"\"\"\n        super().__init__(name)\n        self.function = function\n        self.next_step = next_step\n\n    async def run(self, user_input: Optional[str], ctx: Context) -&gt; Context:\n        \"\"\"\n        Execute function step\n        \u95a2\u6570\u30b9\u30c6\u30c3\u30d7\u3092\u5b9f\u884c\n\n        Args:\n            user_input: User input / \u30e6\u30fc\u30b6\u30fc\u5165\u529b\n            ctx: Current context / \u73fe\u5728\u306e\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\n\n        Returns:\n            Context: Updated context / \u66f4\u65b0\u6e08\u307f\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\n        \"\"\"\n        ctx.update_step_info(self.name)\n\n        try:\n            # Execute the function (may be async)\n            # \u95a2\u6570\u3092\u5b9f\u884c\uff08\u975e\u540c\u671f\u306e\u53ef\u80fd\u6027\u3042\u308a\uff09\n            result = self.function(user_input, ctx)\n            if asyncio.iscoroutine(result):\n                ctx = await result\n            else:\n                ctx = result\n        except Exception as e:\n            ctx.add_system_message(f\"Function execution error in {self.name}: {e}\")\n\n        # Set next step if specified\n        # \u6307\u5b9a\u3055\u308c\u3066\u3044\u308b\u5834\u5408\u306f\u6b21\u30b9\u30c6\u30c3\u30d7\u3092\u8a2d\u5b9a\n        if self.next_step:\n            ctx.goto(self.next_step)\n\n        return ctx\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.FunctionStep.__init__","title":"<code>__init__(name, function, next_step=None)</code>","text":"<p>Initialize function step \u95a2\u6570\u30b9\u30c6\u30c3\u30d7\u3092\u521d\u671f\u5316</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Step name / \u30b9\u30c6\u30c3\u30d7\u540d</p> required <code>function</code> <code>Callable[[Optional[str], Context], Union[Context, Awaitable[Context]]]</code> <p>Function to execute / \u5b9f\u884c\u3059\u308b\u95a2\u6570</p> required <code>next_step</code> <code>Optional[str]</code> <p>Next step after execution / \u5b9f\u884c\u5f8c\u306e\u6b21\u30b9\u30c6\u30c3\u30d7</p> <code>None</code> Source code in <code>src\\agents_sdk_models\\step.py</code> <pre><code>def __init__(\n    self, \n    name: str, \n    function: Callable[[Optional[str], Context], Union[Context, Awaitable[Context]]], \n    next_step: Optional[str] = None\n):\n    \"\"\"\n    Initialize function step\n    \u95a2\u6570\u30b9\u30c6\u30c3\u30d7\u3092\u521d\u671f\u5316\n\n    Args:\n        name: Step name / \u30b9\u30c6\u30c3\u30d7\u540d\n        function: Function to execute / \u5b9f\u884c\u3059\u308b\u95a2\u6570\n        next_step: Next step after execution / \u5b9f\u884c\u5f8c\u306e\u6b21\u30b9\u30c6\u30c3\u30d7\n    \"\"\"\n    super().__init__(name)\n    self.function = function\n    self.next_step = next_step\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.FunctionStep.run","title":"<code>run(user_input, ctx)</code>  <code>async</code>","text":"<p>Execute function step \u95a2\u6570\u30b9\u30c6\u30c3\u30d7\u3092\u5b9f\u884c</p> <p>Parameters:</p> Name Type Description Default <code>user_input</code> <code>Optional[str]</code> <p>User input / \u30e6\u30fc\u30b6\u30fc\u5165\u529b</p> required <code>ctx</code> <code>Context</code> <p>Current context / \u73fe\u5728\u306e\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8</p> required <p>Returns:</p> Name Type Description <code>Context</code> <code>Context</code> <p>Updated context / \u66f4\u65b0\u6e08\u307f\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8</p> Source code in <code>src\\agents_sdk_models\\step.py</code> <pre><code>async def run(self, user_input: Optional[str], ctx: Context) -&gt; Context:\n    \"\"\"\n    Execute function step\n    \u95a2\u6570\u30b9\u30c6\u30c3\u30d7\u3092\u5b9f\u884c\n\n    Args:\n        user_input: User input / \u30e6\u30fc\u30b6\u30fc\u5165\u529b\n        ctx: Current context / \u73fe\u5728\u306e\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\n\n    Returns:\n        Context: Updated context / \u66f4\u65b0\u6e08\u307f\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\n    \"\"\"\n    ctx.update_step_info(self.name)\n\n    try:\n        # Execute the function (may be async)\n        # \u95a2\u6570\u3092\u5b9f\u884c\uff08\u975e\u540c\u671f\u306e\u53ef\u80fd\u6027\u3042\u308a\uff09\n        result = self.function(user_input, ctx)\n        if asyncio.iscoroutine(result):\n            ctx = await result\n        else:\n            ctx = result\n    except Exception as e:\n        ctx.add_system_message(f\"Function execution error in {self.name}: {e}\")\n\n    # Set next step if specified\n    # \u6307\u5b9a\u3055\u308c\u3066\u3044\u308b\u5834\u5408\u306f\u6b21\u30b9\u30c6\u30c3\u30d7\u3092\u8a2d\u5b9a\n    if self.next_step:\n        ctx.goto(self.next_step)\n\n    return ctx\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.GeminiModel","title":"<code>GeminiModel</code>","text":"<p>               Bases: <code>OpenAIChatCompletionsModel</code></p> <p>Gemini model implementation that extends OpenAI's chat completions model OpenAI\u306e\u30c1\u30e3\u30c3\u30c8\u88dc\u5b8c\u30e2\u30c7\u30eb\u3092\u62e1\u5f35\u3057\u305fGemini\u30e2\u30c7\u30eb\u306e\u5b9f\u88c5</p> Source code in <code>src\\agents_sdk_models\\gemini.py</code> <pre><code>class GeminiModel(OpenAIChatCompletionsModel):\n    \"\"\"\n    Gemini model implementation that extends OpenAI's chat completions model\n    OpenAI\u306e\u30c1\u30e3\u30c3\u30c8\u88dc\u5b8c\u30e2\u30c7\u30eb\u3092\u62e1\u5f35\u3057\u305fGemini\u30e2\u30c7\u30eb\u306e\u5b9f\u88c5\n    \"\"\"\n\n    def __init__(\n        self,\n        model: str = \"gemini-2.0-flash\",\n        temperature: float = 0.3,\n        api_key: str = None,\n        base_url: str = \"https://generativelanguage.googleapis.com/v1beta/openai/\",\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the Gemini model with OpenAI compatible interface\n        OpenAI\u4e92\u63db\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30fc\u30b9\u3067Gemini\u30e2\u30c7\u30eb\u3092\u521d\u671f\u5316\u3059\u308b\n\n        Args:\n            model (str): Name of the Gemini model to use (e.g. \"gemini-2.0-flash\")\n                \u4f7f\u7528\u3059\u308bGemini\u30e2\u30c7\u30eb\u306e\u540d\u524d\uff08\u4f8b\uff1a\"gemini-2.0-flash\"\uff09\n            temperature (float): Sampling temperature between 0 and 1\n                \u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u6e29\u5ea6\uff080\u304b\u30891\u306e\u9593\uff09\n            api_key (str): Gemini API key\n                Gemini API\u30ad\u30fc\n            base_url (str): Base URL for the Gemini API\n                Gemini API\u306e\u30d9\u30fc\u30b9URL\n            **kwargs: Additional arguments to pass to the OpenAI API\n                OpenAI API\u306b\u6e21\u3059\u8ffd\u52a0\u306e\u5f15\u6570\n        \"\"\"\n        if base_url == None:\n            base_url = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n\n        # api_key \u304c None \u306e\u5834\u5408\u306f\u74b0\u5883\u5909\u6570\u304b\u3089\u53d6\u5f97\n        if api_key is None:\n            api_key = os.environ.get(\"GOOGLE_API_KEY\")\n            if api_key is None:\n                raise ValueError(\"Google API key is required. Get one from https://ai.google.dev/\")\n\n        # Create AsyncOpenAI client with Gemini base URL\n        # Gemini\u306e\u30d9\u30fc\u30b9URL\u3067AsyncOpenAI\u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u3092\u4f5c\u6210\n        openai_client = AsyncOpenAI(base_url=base_url, api_key=api_key)\n\n        # Store parameters for later use in API calls\n        # \u5f8c\u3067API\u30b3\u30fc\u30eb\u3067\u4f7f\u7528\u3059\u308b\u305f\u3081\u306b\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u4fdd\u5b58\n        self.temperature = temperature\n        self.kwargs = kwargs\n\n        # Initialize the parent class with our custom client\n        # \u30ab\u30b9\u30bf\u30e0\u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u3067\u89aa\u30af\u30e9\u30b9\u3092\u521d\u671f\u5316\n        super().__init__(\n            model=model,\n            openai_client=openai_client\n        )\n\n    # Override methods that make API calls to include our parameters\n    # API\u30b3\u30fc\u30eb\u3092\u884c\u3046\u30e1\u30bd\u30c3\u30c9\u3092\u30aa\u30fc\u30d0\u30fc\u30e9\u30a4\u30c9\u3057\u3066\u3001\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u542b\u3081\u308b\n    async def _create_chat_completion(self, *args, **kwargs):\n        \"\"\"Override to include temperature and other parameters\"\"\"\n        kwargs[\"temperature\"] = self.temperature\n        kwargs.update(self.kwargs)\n        return await super()._create_chat_completion(*args, **kwargs)\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.GeminiModel.__init__","title":"<code>__init__(model='gemini-2.0-flash', temperature=0.3, api_key=None, base_url='https://generativelanguage.googleapis.com/v1beta/openai/', **kwargs)</code>","text":"<p>Initialize the Gemini model with OpenAI compatible interface OpenAI\u4e92\u63db\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30fc\u30b9\u3067Gemini\u30e2\u30c7\u30eb\u3092\u521d\u671f\u5316\u3059\u308b</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Name of the Gemini model to use (e.g. \"gemini-2.0-flash\") \u4f7f\u7528\u3059\u308bGemini\u30e2\u30c7\u30eb\u306e\u540d\u524d\uff08\u4f8b\uff1a\"gemini-2.0-flash\"\uff09</p> <code>'gemini-2.0-flash'</code> <code>temperature</code> <code>float</code> <p>Sampling temperature between 0 and 1 \u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u6e29\u5ea6\uff080\u304b\u30891\u306e\u9593\uff09</p> <code>0.3</code> <code>api_key</code> <code>str</code> <p>Gemini API key Gemini API\u30ad\u30fc</p> <code>None</code> <code>base_url</code> <code>str</code> <p>Base URL for the Gemini API Gemini API\u306e\u30d9\u30fc\u30b9URL</p> <code>'https://generativelanguage.googleapis.com/v1beta/openai/'</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments to pass to the OpenAI API OpenAI API\u306b\u6e21\u3059\u8ffd\u52a0\u306e\u5f15\u6570</p> <code>{}</code> Source code in <code>src\\agents_sdk_models\\gemini.py</code> <pre><code>def __init__(\n    self,\n    model: str = \"gemini-2.0-flash\",\n    temperature: float = 0.3,\n    api_key: str = None,\n    base_url: str = \"https://generativelanguage.googleapis.com/v1beta/openai/\",\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initialize the Gemini model with OpenAI compatible interface\n    OpenAI\u4e92\u63db\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30fc\u30b9\u3067Gemini\u30e2\u30c7\u30eb\u3092\u521d\u671f\u5316\u3059\u308b\n\n    Args:\n        model (str): Name of the Gemini model to use (e.g. \"gemini-2.0-flash\")\n            \u4f7f\u7528\u3059\u308bGemini\u30e2\u30c7\u30eb\u306e\u540d\u524d\uff08\u4f8b\uff1a\"gemini-2.0-flash\"\uff09\n        temperature (float): Sampling temperature between 0 and 1\n            \u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u6e29\u5ea6\uff080\u304b\u30891\u306e\u9593\uff09\n        api_key (str): Gemini API key\n            Gemini API\u30ad\u30fc\n        base_url (str): Base URL for the Gemini API\n            Gemini API\u306e\u30d9\u30fc\u30b9URL\n        **kwargs: Additional arguments to pass to the OpenAI API\n            OpenAI API\u306b\u6e21\u3059\u8ffd\u52a0\u306e\u5f15\u6570\n    \"\"\"\n    if base_url == None:\n        base_url = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n\n    # api_key \u304c None \u306e\u5834\u5408\u306f\u74b0\u5883\u5909\u6570\u304b\u3089\u53d6\u5f97\n    if api_key is None:\n        api_key = os.environ.get(\"GOOGLE_API_KEY\")\n        if api_key is None:\n            raise ValueError(\"Google API key is required. Get one from https://ai.google.dev/\")\n\n    # Create AsyncOpenAI client with Gemini base URL\n    # Gemini\u306e\u30d9\u30fc\u30b9URL\u3067AsyncOpenAI\u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u3092\u4f5c\u6210\n    openai_client = AsyncOpenAI(base_url=base_url, api_key=api_key)\n\n    # Store parameters for later use in API calls\n    # \u5f8c\u3067API\u30b3\u30fc\u30eb\u3067\u4f7f\u7528\u3059\u308b\u305f\u3081\u306b\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u4fdd\u5b58\n    self.temperature = temperature\n    self.kwargs = kwargs\n\n    # Initialize the parent class with our custom client\n    # \u30ab\u30b9\u30bf\u30e0\u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u3067\u89aa\u30af\u30e9\u30b9\u3092\u521d\u671f\u5316\n    super().__init__(\n        model=model,\n        openai_client=openai_client\n    )\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.GenAgent","title":"<code>GenAgent</code>","text":"<p>               Bases: <code>Step</code></p> <p>Step implementation that wraps AgentPipeline functionality AgentPipeline\u6a5f\u80fd\u3092\u30e9\u30c3\u30d7\u3059\u308bStep\u5b9f\u88c5</p> <p>This class allows using AgentPipeline directly as a Step in Flow workflows, providing generation, evaluation, and retry capabilities within a workflow context. \u3053\u306e\u30af\u30e9\u30b9\u306fAgentPipeline\u3092Flow\u30ef\u30fc\u30af\u30d5\u30ed\u30fc\u5185\u3067\u76f4\u63a5Step\u3068\u3057\u3066\u4f7f\u7528\u3067\u304d\u308b\u3088\u3046\u306b\u3057\u3001 \u30ef\u30fc\u30af\u30d5\u30ed\u30fc\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u5185\u3067\u751f\u6210\u3001\u8a55\u4fa1\u3001\u30ea\u30c8\u30e9\u30a4\u6a5f\u80fd\u3092\u63d0\u4f9b\u3057\u307e\u3059\u3002</p> Source code in <code>src\\agents_sdk_models\\gen_agent.py</code> <pre><code>class GenAgent(Step):\n    \"\"\"\n    Step implementation that wraps AgentPipeline functionality\n    AgentPipeline\u6a5f\u80fd\u3092\u30e9\u30c3\u30d7\u3059\u308bStep\u5b9f\u88c5\n\n    This class allows using AgentPipeline directly as a Step in Flow workflows,\n    providing generation, evaluation, and retry capabilities within a workflow context.\n    \u3053\u306e\u30af\u30e9\u30b9\u306fAgentPipeline\u3092Flow\u30ef\u30fc\u30af\u30d5\u30ed\u30fc\u5185\u3067\u76f4\u63a5Step\u3068\u3057\u3066\u4f7f\u7528\u3067\u304d\u308b\u3088\u3046\u306b\u3057\u3001\n    \u30ef\u30fc\u30af\u30d5\u30ed\u30fc\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u5185\u3067\u751f\u6210\u3001\u8a55\u4fa1\u3001\u30ea\u30c8\u30e9\u30a4\u6a5f\u80fd\u3092\u63d0\u4f9b\u3057\u307e\u3059\u3002\n    \"\"\"\n\n    def __init__(\n        self,\n        name: str,\n        generation_instructions: str,\n        evaluation_instructions: Optional[str] = None,\n        *,\n        input_guardrails: Optional[list] = None,\n        output_guardrails: Optional[list] = None,\n        output_model: Optional[Type[Any]] = None,\n        model: str | None = None,\n        evaluation_model: str | None = None,\n        generation_tools: Optional[list] = None,\n        evaluation_tools: Optional[list] = None,\n        routing_func: Optional[Callable[[Any], Any]] = None,\n        session_history: Optional[list] = None,\n        history_size: int = 10,\n        threshold: int = 85,\n        retries: int = 3,\n        improvement_callback: Optional[Callable[[Any, EvaluationResult], None]] = None,\n        dynamic_prompt: Optional[Callable[[str], str]] = None,\n        retry_comment_importance: Optional[list[str]] = None,\n        locale: str = \"en\",\n        next_step: Optional[str] = None,\n        store_result_key: Optional[str] = None,\n    ) -&gt; None:\n        \"\"\"\n        Initialize GenAgent with AgentPipeline configuration\n        AgentPipeline\u8a2d\u5b9a\u3067GenAgent\u3092\u521d\u671f\u5316\u3059\u308b\n\n        Args:\n            name: Step name / \u30b9\u30c6\u30c3\u30d7\u540d\n            generation_instructions: System prompt for generation / \u751f\u6210\u7528\u30b7\u30b9\u30c6\u30e0\u30d7\u30ed\u30f3\u30d7\u30c8\n            evaluation_instructions: System prompt for evaluation / \u8a55\u4fa1\u7528\u30b7\u30b9\u30c6\u30e0\u30d7\u30ed\u30f3\u30d7\u30c8\n            input_guardrails: Guardrails for generation / \u751f\u6210\u7528\u30ac\u30fc\u30c9\u30ec\u30fc\u30eb\n            output_guardrails: Guardrails for evaluation / \u8a55\u4fa1\u7528\u30ac\u30fc\u30c9\u30ec\u30fc\u30eb\n            output_model: Model for output formatting / \u51fa\u529b\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u7528\u30e2\u30c7\u30eb\n            model: LLM model name / LLM\u30e2\u30c7\u30eb\u540d\n            evaluation_model: Optional LLM model name for evaluation / \u8a55\u4fa1\u7528LLM\u30e2\u30c7\u30eb\u540d\uff08\u4efb\u610f\uff09\n            generation_tools: Tools for generation / \u751f\u6210\u7528\u30c4\u30fc\u30eb\n            evaluation_tools: Tools for evaluation / \u8a55\u4fa1\u7528\u30c4\u30fc\u30eb\n            routing_func: Function for output routing / \u51fa\u529b\u30eb\u30fc\u30c6\u30a3\u30f3\u30b0\u7528\u95a2\u6570\n            session_history: Session history / \u30bb\u30c3\u30b7\u30e7\u30f3\u5c65\u6b74\n            history_size: Size of history to keep / \u4fdd\u6301\u3059\u308b\u5c65\u6b74\u30b5\u30a4\u30ba\n            threshold: Evaluation score threshold / \u8a55\u4fa1\u30b9\u30b3\u30a2\u95be\u5024\n            retries: Number of retry attempts / \u30ea\u30c8\u30e9\u30a4\u8a66\u884c\u56de\u6570\n            improvement_callback: Callback for improvement suggestions / \u6539\u5584\u63d0\u6848\u7528\u30b3\u30fc\u30eb\u30d0\u30c3\u30af\n            dynamic_prompt: Optional function to dynamically build prompt / \u52d5\u7684\u30d7\u30ed\u30f3\u30d7\u30c8\u751f\u6210\u95a2\u6570\uff08\u4efb\u610f\uff09\n            retry_comment_importance: Importance levels of comments to include on retry / \u30ea\u30c8\u30e9\u30a4\u6642\u30b3\u30e1\u30f3\u30c8\u91cd\u8981\u5ea6\u30ec\u30d9\u30eb\n            locale: Language code for localized messages / \u30ed\u30fc\u30ab\u30e9\u30a4\u30ba\u30e1\u30c3\u30bb\u30fc\u30b8\u7528\u8a00\u8a9e\u30b3\u30fc\u30c9\n            next_step: Next step after pipeline execution / \u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u5b9f\u884c\u5f8c\u306e\u6b21\u30b9\u30c6\u30c3\u30d7\n            store_result_key: Key to store result in context shared_state / \u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u5171\u6709\u72b6\u614b\u306b\u7d50\u679c\u3092\u683c\u7d0d\u3059\u308b\u30ad\u30fc\n        \"\"\"\n        # Initialize Step base class\n        # Step\u57fa\u5e95\u30af\u30e9\u30b9\u3092\u521d\u671f\u5316\n        super().__init__(name)\n\n        # Store flow-specific configuration\n        # \u30d5\u30ed\u30fc\u56fa\u6709\u306e\u8a2d\u5b9a\u3092\u4fdd\u5b58\n        self.next_step = next_step\n        self.store_result_key = store_result_key or f\"{name}_result\"\n\n        # Create internal AgentPipeline instance\n        # \u5185\u90e8AgentPipeline\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u3092\u4f5c\u6210\n        self.pipeline = AgentPipeline(\n            name=f\"{name}_pipeline\",\n            generation_instructions=generation_instructions,\n            evaluation_instructions=evaluation_instructions,\n            input_guardrails=input_guardrails,\n            output_guardrails=output_guardrails,\n            output_model=output_model,\n            model=model,\n            evaluation_model=evaluation_model,\n            generation_tools=generation_tools,\n            evaluation_tools=evaluation_tools,\n            routing_func=routing_func,\n            session_history=session_history,\n            history_size=history_size,\n            threshold=threshold,\n            retries=retries,\n            improvement_callback=improvement_callback,\n            dynamic_prompt=dynamic_prompt,\n            retry_comment_importance=retry_comment_importance,\n            locale=locale,\n        )\n\n    async def run(self, user_input: Optional[str], ctx: Context) -&gt; Context:\n        \"\"\"\n        Execute GenAgent step using AgentPipeline\n        AgentPipeline\u3092\u4f7f\u7528\u3057\u3066GenAgent\u30b9\u30c6\u30c3\u30d7\u3092\u5b9f\u884c\u3059\u308b\n\n        Args:\n            user_input: User input for the pipeline / \u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u7528\u30e6\u30fc\u30b6\u30fc\u5165\u529b\n            ctx: Current workflow context / \u73fe\u5728\u306e\u30ef\u30fc\u30af\u30d5\u30ed\u30fc\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\n\n        Returns:\n            Context: Updated context with pipeline results / \u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u7d50\u679c\u4ed8\u304d\u66f4\u65b0\u6e08\u307f\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\n        \"\"\"\n        # English: Update step information in context\n        # \u65e5\u672c\u8a9e: \u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u306e\u30b9\u30c6\u30c3\u30d7\u60c5\u5831\u3092\u66f4\u65b0\n        ctx.update_step_info(self.name)\n\n        try:\n            # English: Determine input text for pipeline\n            # \u65e5\u672c\u8a9e: \u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u7528\u5165\u529b\u30c6\u30ad\u30b9\u30c8\u3092\u6c7a\u5b9a\n            input_text = user_input or ctx.last_user_input or \"\"\n\n            if not input_text:\n                # English: If no input available, add system message and continue\n                # \u65e5\u672c\u8a9e: \u5165\u529b\u304c\u306a\u3044\u5834\u5408\u3001\u30b7\u30b9\u30c6\u30e0\u30e1\u30c3\u30bb\u30fc\u30b8\u3092\u8ffd\u52a0\u3057\u3066\u7d9a\u884c\n                ctx.add_system_message(f\"GenAgent {self.name}: No input available, skipping pipeline execution\")\n                result = None\n            else:\n                # English: Execute pipeline in thread pool to handle sync methods\n                # \u65e5\u672c\u8a9e: \u540c\u671f\u30e1\u30bd\u30c3\u30c9\u3092\u51e6\u7406\u3059\u308b\u305f\u3081\u306b\u30b9\u30ec\u30c3\u30c9\u30d7\u30fc\u30eb\u3067\u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u3092\u5b9f\u884c\n                loop = asyncio.get_event_loop()\n                with ThreadPoolExecutor() as executor:\n                    future = loop.run_in_executor(executor, self.pipeline.run, input_text)\n                    result = await future\n\n            # English: Store result in context\n            # \u65e5\u672c\u8a9e: \u7d50\u679c\u3092\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u306b\u4fdd\u5b58\n            if result is not None:\n                # English: Store in shared state for other steps to access\n                # \u65e5\u672c\u8a9e: \u4ed6\u306e\u30b9\u30c6\u30c3\u30d7\u304c\u30a2\u30af\u30bb\u30b9\u3067\u304d\u308b\u3088\u3046\u5171\u6709\u72b6\u614b\u306b\u4fdd\u5b58\n                ctx.shared_state[self.store_result_key] = result\n                ctx.prev_outputs[self.name] = result\n\n                # English: Add result as assistant message\n                # \u65e5\u672c\u8a9e: \u7d50\u679c\u3092\u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u30e1\u30c3\u30bb\u30fc\u30b8\u3068\u3057\u3066\u8ffd\u52a0\n                ctx.add_assistant_message(str(result))\n\n                # English: Add success system message\n                # \u65e5\u672c\u8a9e: \u6210\u529f\u30b7\u30b9\u30c6\u30e0\u30e1\u30c3\u30bb\u30fc\u30b8\u3092\u8ffd\u52a0\n                ctx.add_system_message(f\"GenAgent {self.name}: Pipeline executed successfully\")\n            else:\n                # English: Handle case where pipeline returned None (evaluation failed)\n                # \u65e5\u672c\u8a9e: \u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u304cNone\u3092\u8fd4\u3057\u305f\u5834\u5408\uff08\u8a55\u4fa1\u5931\u6557\uff09\u3092\u51e6\u7406\n                ctx.shared_state[self.store_result_key] = None\n                ctx.prev_outputs[self.name] = None\n\n                # English: Add failure system message\n                # \u65e5\u672c\u8a9e: \u5931\u6557\u30b7\u30b9\u30c6\u30e0\u30e1\u30c3\u30bb\u30fc\u30b8\u3092\u8ffd\u52a0\n                ctx.add_system_message(f\"GenAgent {self.name}: Pipeline execution failed (evaluation threshold not met)\")\n\n        except Exception as e:\n            # English: Handle execution errors\n            # \u65e5\u672c\u8a9e: \u5b9f\u884c\u30a8\u30e9\u30fc\u3092\u51e6\u7406\n            error_msg = f\"GenAgent {self.name} execution error: {str(e)}\"\n            ctx.add_system_message(error_msg)\n            ctx.shared_state[self.store_result_key] = None\n            ctx.prev_outputs[self.name] = None\n\n            # English: Log error for debugging\n            # \u65e5\u672c\u8a9e: \u30c7\u30d0\u30c3\u30b0\u7528\u30a8\u30e9\u30fc\u30ed\u30b0\n            print(f\"\ud83d\udea8 {error_msg}\")\n\n        # English: Set next step if specified\n        # \u65e5\u672c\u8a9e: \u6307\u5b9a\u3055\u308c\u3066\u3044\u308b\u5834\u5408\u306f\u6b21\u30b9\u30c6\u30c3\u30d7\u3092\u8a2d\u5b9a\n        if self.next_step:\n            ctx.goto(self.next_step)\n\n        return ctx\n\n    def get_pipeline_history(self) -&gt; List[Dict[str, str]]:\n        \"\"\"\n        Get the internal pipeline history\n        \u5185\u90e8\u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u5c65\u6b74\u3092\u53d6\u5f97\u3059\u308b\n\n        Returns:\n            List[Dict[str, str]]: Pipeline history / \u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u5c65\u6b74\n        \"\"\"\n        return self.pipeline._pipeline_history\n\n    def get_session_history(self) -&gt; Optional[List[str]]:\n        \"\"\"\n        Get the session history\n        \u30bb\u30c3\u30b7\u30e7\u30f3\u5c65\u6b74\u3092\u53d6\u5f97\u3059\u308b\n\n        Returns:\n            Optional[List[str]]: Session history / \u30bb\u30c3\u30b7\u30e7\u30f3\u5c65\u6b74\n        \"\"\"\n        return self.pipeline.session_history\n\n    def update_instructions(\n        self, \n        generation_instructions: Optional[str] = None,\n        evaluation_instructions: Optional[str] = None\n    ) -&gt; None:\n        \"\"\"\n        Update pipeline instructions\n        \u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u6307\u793a\u3092\u66f4\u65b0\u3059\u308b\n\n        Args:\n            generation_instructions: New generation instructions / \u65b0\u3057\u3044\u751f\u6210\u6307\u793a\n            evaluation_instructions: New evaluation instructions / \u65b0\u3057\u3044\u8a55\u4fa1\u6307\u793a\n        \"\"\"\n        if generation_instructions is not None:\n            self.pipeline.generation_instructions = generation_instructions.strip()\n            # English: Update the agent instructions\n            # \u65e5\u672c\u8a9e: \u30a8\u30fc\u30b8\u30a7\u30f3\u30c8\u6307\u793a\u3092\u66f4\u65b0\n            self.pipeline.gen_agent.instructions = generation_instructions.strip()\n\n        if evaluation_instructions is not None:\n            self.pipeline.evaluation_instructions = evaluation_instructions.strip() if evaluation_instructions else None\n            # English: Update evaluation agent if it exists\n            # \u65e5\u672c\u8a9e: \u8a55\u4fa1\u30a8\u30fc\u30b8\u30a7\u30f3\u30c8\u304c\u5b58\u5728\u3059\u308b\u5834\u5408\u306f\u66f4\u65b0\n            if self.pipeline.eval_agent and evaluation_instructions:\n                self.pipeline.eval_agent.instructions = evaluation_instructions.strip()\n\n    def clear_history(self) -&gt; None:\n        \"\"\"\n        Clear both pipeline and session history\n        \u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u5c65\u6b74\u3068\u30bb\u30c3\u30b7\u30e7\u30f3\u5c65\u6b74\u306e\u4e21\u65b9\u3092\u30af\u30ea\u30a2\n        \"\"\"\n        self.pipeline._pipeline_history.clear()\n        if self.pipeline.session_history:\n            self.pipeline.session_history.clear()\n\n    def set_threshold(self, threshold: int) -&gt; None:\n        \"\"\"\n        Update evaluation threshold\n        \u8a55\u4fa1\u95be\u5024\u3092\u66f4\u65b0\u3059\u308b\n\n        Args:\n            threshold: New threshold value (0-100) / \u65b0\u3057\u3044\u95be\u5024\uff080-100\uff09\n        \"\"\"\n        if 0 &lt;= threshold &lt;= 100:\n            self.pipeline.threshold = threshold\n        else:\n            raise ValueError(\"Threshold must be between 0 and 100\")\n\n    def __str__(self) -&gt; str:\n        return f\"GenAgent({self.name}, model={self.pipeline.model})\"\n\n    def __repr__(self) -&gt; str:\n        return self.__str__()\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.GenAgent.__init__","title":"<code>__init__(name, generation_instructions, evaluation_instructions=None, *, input_guardrails=None, output_guardrails=None, output_model=None, model=None, evaluation_model=None, generation_tools=None, evaluation_tools=None, routing_func=None, session_history=None, history_size=10, threshold=85, retries=3, improvement_callback=None, dynamic_prompt=None, retry_comment_importance=None, locale='en', next_step=None, store_result_key=None)</code>","text":"<p>Initialize GenAgent with AgentPipeline configuration AgentPipeline\u8a2d\u5b9a\u3067GenAgent\u3092\u521d\u671f\u5316\u3059\u308b</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Step name / \u30b9\u30c6\u30c3\u30d7\u540d</p> required <code>generation_instructions</code> <code>str</code> <p>System prompt for generation / \u751f\u6210\u7528\u30b7\u30b9\u30c6\u30e0\u30d7\u30ed\u30f3\u30d7\u30c8</p> required <code>evaluation_instructions</code> <code>Optional[str]</code> <p>System prompt for evaluation / \u8a55\u4fa1\u7528\u30b7\u30b9\u30c6\u30e0\u30d7\u30ed\u30f3\u30d7\u30c8</p> <code>None</code> <code>input_guardrails</code> <code>Optional[list]</code> <p>Guardrails for generation / \u751f\u6210\u7528\u30ac\u30fc\u30c9\u30ec\u30fc\u30eb</p> <code>None</code> <code>output_guardrails</code> <code>Optional[list]</code> <p>Guardrails for evaluation / \u8a55\u4fa1\u7528\u30ac\u30fc\u30c9\u30ec\u30fc\u30eb</p> <code>None</code> <code>output_model</code> <code>Optional[Type[Any]]</code> <p>Model for output formatting / \u51fa\u529b\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u7528\u30e2\u30c7\u30eb</p> <code>None</code> <code>model</code> <code>str | None</code> <p>LLM model name / LLM\u30e2\u30c7\u30eb\u540d</p> <code>None</code> <code>evaluation_model</code> <code>str | None</code> <p>Optional LLM model name for evaluation / \u8a55\u4fa1\u7528LLM\u30e2\u30c7\u30eb\u540d\uff08\u4efb\u610f\uff09</p> <code>None</code> <code>generation_tools</code> <code>Optional[list]</code> <p>Tools for generation / \u751f\u6210\u7528\u30c4\u30fc\u30eb</p> <code>None</code> <code>evaluation_tools</code> <code>Optional[list]</code> <p>Tools for evaluation / \u8a55\u4fa1\u7528\u30c4\u30fc\u30eb</p> <code>None</code> <code>routing_func</code> <code>Optional[Callable[[Any], Any]]</code> <p>Function for output routing / \u51fa\u529b\u30eb\u30fc\u30c6\u30a3\u30f3\u30b0\u7528\u95a2\u6570</p> <code>None</code> <code>session_history</code> <code>Optional[list]</code> <p>Session history / \u30bb\u30c3\u30b7\u30e7\u30f3\u5c65\u6b74</p> <code>None</code> <code>history_size</code> <code>int</code> <p>Size of history to keep / \u4fdd\u6301\u3059\u308b\u5c65\u6b74\u30b5\u30a4\u30ba</p> <code>10</code> <code>threshold</code> <code>int</code> <p>Evaluation score threshold / \u8a55\u4fa1\u30b9\u30b3\u30a2\u95be\u5024</p> <code>85</code> <code>retries</code> <code>int</code> <p>Number of retry attempts / \u30ea\u30c8\u30e9\u30a4\u8a66\u884c\u56de\u6570</p> <code>3</code> <code>improvement_callback</code> <code>Optional[Callable[[Any, EvaluationResult], None]]</code> <p>Callback for improvement suggestions / \u6539\u5584\u63d0\u6848\u7528\u30b3\u30fc\u30eb\u30d0\u30c3\u30af</p> <code>None</code> <code>dynamic_prompt</code> <code>Optional[Callable[[str], str]]</code> <p>Optional function to dynamically build prompt / \u52d5\u7684\u30d7\u30ed\u30f3\u30d7\u30c8\u751f\u6210\u95a2\u6570\uff08\u4efb\u610f\uff09</p> <code>None</code> <code>retry_comment_importance</code> <code>Optional[list[str]]</code> <p>Importance levels of comments to include on retry / \u30ea\u30c8\u30e9\u30a4\u6642\u30b3\u30e1\u30f3\u30c8\u91cd\u8981\u5ea6\u30ec\u30d9\u30eb</p> <code>None</code> <code>locale</code> <code>str</code> <p>Language code for localized messages / \u30ed\u30fc\u30ab\u30e9\u30a4\u30ba\u30e1\u30c3\u30bb\u30fc\u30b8\u7528\u8a00\u8a9e\u30b3\u30fc\u30c9</p> <code>'en'</code> <code>next_step</code> <code>Optional[str]</code> <p>Next step after pipeline execution / \u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u5b9f\u884c\u5f8c\u306e\u6b21\u30b9\u30c6\u30c3\u30d7</p> <code>None</code> <code>store_result_key</code> <code>Optional[str]</code> <p>Key to store result in context shared_state / \u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u5171\u6709\u72b6\u614b\u306b\u7d50\u679c\u3092\u683c\u7d0d\u3059\u308b\u30ad\u30fc</p> <code>None</code> Source code in <code>src\\agents_sdk_models\\gen_agent.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    generation_instructions: str,\n    evaluation_instructions: Optional[str] = None,\n    *,\n    input_guardrails: Optional[list] = None,\n    output_guardrails: Optional[list] = None,\n    output_model: Optional[Type[Any]] = None,\n    model: str | None = None,\n    evaluation_model: str | None = None,\n    generation_tools: Optional[list] = None,\n    evaluation_tools: Optional[list] = None,\n    routing_func: Optional[Callable[[Any], Any]] = None,\n    session_history: Optional[list] = None,\n    history_size: int = 10,\n    threshold: int = 85,\n    retries: int = 3,\n    improvement_callback: Optional[Callable[[Any, EvaluationResult], None]] = None,\n    dynamic_prompt: Optional[Callable[[str], str]] = None,\n    retry_comment_importance: Optional[list[str]] = None,\n    locale: str = \"en\",\n    next_step: Optional[str] = None,\n    store_result_key: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Initialize GenAgent with AgentPipeline configuration\n    AgentPipeline\u8a2d\u5b9a\u3067GenAgent\u3092\u521d\u671f\u5316\u3059\u308b\n\n    Args:\n        name: Step name / \u30b9\u30c6\u30c3\u30d7\u540d\n        generation_instructions: System prompt for generation / \u751f\u6210\u7528\u30b7\u30b9\u30c6\u30e0\u30d7\u30ed\u30f3\u30d7\u30c8\n        evaluation_instructions: System prompt for evaluation / \u8a55\u4fa1\u7528\u30b7\u30b9\u30c6\u30e0\u30d7\u30ed\u30f3\u30d7\u30c8\n        input_guardrails: Guardrails for generation / \u751f\u6210\u7528\u30ac\u30fc\u30c9\u30ec\u30fc\u30eb\n        output_guardrails: Guardrails for evaluation / \u8a55\u4fa1\u7528\u30ac\u30fc\u30c9\u30ec\u30fc\u30eb\n        output_model: Model for output formatting / \u51fa\u529b\u30d5\u30a9\u30fc\u30de\u30c3\u30c8\u7528\u30e2\u30c7\u30eb\n        model: LLM model name / LLM\u30e2\u30c7\u30eb\u540d\n        evaluation_model: Optional LLM model name for evaluation / \u8a55\u4fa1\u7528LLM\u30e2\u30c7\u30eb\u540d\uff08\u4efb\u610f\uff09\n        generation_tools: Tools for generation / \u751f\u6210\u7528\u30c4\u30fc\u30eb\n        evaluation_tools: Tools for evaluation / \u8a55\u4fa1\u7528\u30c4\u30fc\u30eb\n        routing_func: Function for output routing / \u51fa\u529b\u30eb\u30fc\u30c6\u30a3\u30f3\u30b0\u7528\u95a2\u6570\n        session_history: Session history / \u30bb\u30c3\u30b7\u30e7\u30f3\u5c65\u6b74\n        history_size: Size of history to keep / \u4fdd\u6301\u3059\u308b\u5c65\u6b74\u30b5\u30a4\u30ba\n        threshold: Evaluation score threshold / \u8a55\u4fa1\u30b9\u30b3\u30a2\u95be\u5024\n        retries: Number of retry attempts / \u30ea\u30c8\u30e9\u30a4\u8a66\u884c\u56de\u6570\n        improvement_callback: Callback for improvement suggestions / \u6539\u5584\u63d0\u6848\u7528\u30b3\u30fc\u30eb\u30d0\u30c3\u30af\n        dynamic_prompt: Optional function to dynamically build prompt / \u52d5\u7684\u30d7\u30ed\u30f3\u30d7\u30c8\u751f\u6210\u95a2\u6570\uff08\u4efb\u610f\uff09\n        retry_comment_importance: Importance levels of comments to include on retry / \u30ea\u30c8\u30e9\u30a4\u6642\u30b3\u30e1\u30f3\u30c8\u91cd\u8981\u5ea6\u30ec\u30d9\u30eb\n        locale: Language code for localized messages / \u30ed\u30fc\u30ab\u30e9\u30a4\u30ba\u30e1\u30c3\u30bb\u30fc\u30b8\u7528\u8a00\u8a9e\u30b3\u30fc\u30c9\n        next_step: Next step after pipeline execution / \u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u5b9f\u884c\u5f8c\u306e\u6b21\u30b9\u30c6\u30c3\u30d7\n        store_result_key: Key to store result in context shared_state / \u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u5171\u6709\u72b6\u614b\u306b\u7d50\u679c\u3092\u683c\u7d0d\u3059\u308b\u30ad\u30fc\n    \"\"\"\n    # Initialize Step base class\n    # Step\u57fa\u5e95\u30af\u30e9\u30b9\u3092\u521d\u671f\u5316\n    super().__init__(name)\n\n    # Store flow-specific configuration\n    # \u30d5\u30ed\u30fc\u56fa\u6709\u306e\u8a2d\u5b9a\u3092\u4fdd\u5b58\n    self.next_step = next_step\n    self.store_result_key = store_result_key or f\"{name}_result\"\n\n    # Create internal AgentPipeline instance\n    # \u5185\u90e8AgentPipeline\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u3092\u4f5c\u6210\n    self.pipeline = AgentPipeline(\n        name=f\"{name}_pipeline\",\n        generation_instructions=generation_instructions,\n        evaluation_instructions=evaluation_instructions,\n        input_guardrails=input_guardrails,\n        output_guardrails=output_guardrails,\n        output_model=output_model,\n        model=model,\n        evaluation_model=evaluation_model,\n        generation_tools=generation_tools,\n        evaluation_tools=evaluation_tools,\n        routing_func=routing_func,\n        session_history=session_history,\n        history_size=history_size,\n        threshold=threshold,\n        retries=retries,\n        improvement_callback=improvement_callback,\n        dynamic_prompt=dynamic_prompt,\n        retry_comment_importance=retry_comment_importance,\n        locale=locale,\n    )\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.GenAgent.clear_history","title":"<code>clear_history()</code>","text":"<p>Clear both pipeline and session history \u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u5c65\u6b74\u3068\u30bb\u30c3\u30b7\u30e7\u30f3\u5c65\u6b74\u306e\u4e21\u65b9\u3092\u30af\u30ea\u30a2</p> Source code in <code>src\\agents_sdk_models\\gen_agent.py</code> <pre><code>def clear_history(self) -&gt; None:\n    \"\"\"\n    Clear both pipeline and session history\n    \u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u5c65\u6b74\u3068\u30bb\u30c3\u30b7\u30e7\u30f3\u5c65\u6b74\u306e\u4e21\u65b9\u3092\u30af\u30ea\u30a2\n    \"\"\"\n    self.pipeline._pipeline_history.clear()\n    if self.pipeline.session_history:\n        self.pipeline.session_history.clear()\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.GenAgent.get_pipeline_history","title":"<code>get_pipeline_history()</code>","text":"<p>Get the internal pipeline history \u5185\u90e8\u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u5c65\u6b74\u3092\u53d6\u5f97\u3059\u308b</p> <p>Returns:</p> Type Description <code>List[Dict[str, str]]</code> <p>List[Dict[str, str]]: Pipeline history / \u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u5c65\u6b74</p> Source code in <code>src\\agents_sdk_models\\gen_agent.py</code> <pre><code>def get_pipeline_history(self) -&gt; List[Dict[str, str]]:\n    \"\"\"\n    Get the internal pipeline history\n    \u5185\u90e8\u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u5c65\u6b74\u3092\u53d6\u5f97\u3059\u308b\n\n    Returns:\n        List[Dict[str, str]]: Pipeline history / \u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u5c65\u6b74\n    \"\"\"\n    return self.pipeline._pipeline_history\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.GenAgent.get_session_history","title":"<code>get_session_history()</code>","text":"<p>Get the session history \u30bb\u30c3\u30b7\u30e7\u30f3\u5c65\u6b74\u3092\u53d6\u5f97\u3059\u308b</p> <p>Returns:</p> Type Description <code>Optional[List[str]]</code> <p>Optional[List[str]]: Session history / \u30bb\u30c3\u30b7\u30e7\u30f3\u5c65\u6b74</p> Source code in <code>src\\agents_sdk_models\\gen_agent.py</code> <pre><code>def get_session_history(self) -&gt; Optional[List[str]]:\n    \"\"\"\n    Get the session history\n    \u30bb\u30c3\u30b7\u30e7\u30f3\u5c65\u6b74\u3092\u53d6\u5f97\u3059\u308b\n\n    Returns:\n        Optional[List[str]]: Session history / \u30bb\u30c3\u30b7\u30e7\u30f3\u5c65\u6b74\n    \"\"\"\n    return self.pipeline.session_history\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.GenAgent.run","title":"<code>run(user_input, ctx)</code>  <code>async</code>","text":"<p>Execute GenAgent step using AgentPipeline AgentPipeline\u3092\u4f7f\u7528\u3057\u3066GenAgent\u30b9\u30c6\u30c3\u30d7\u3092\u5b9f\u884c\u3059\u308b</p> <p>Parameters:</p> Name Type Description Default <code>user_input</code> <code>Optional[str]</code> <p>User input for the pipeline / \u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u7528\u30e6\u30fc\u30b6\u30fc\u5165\u529b</p> required <code>ctx</code> <code>Context</code> <p>Current workflow context / \u73fe\u5728\u306e\u30ef\u30fc\u30af\u30d5\u30ed\u30fc\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8</p> required <p>Returns:</p> Name Type Description <code>Context</code> <code>Context</code> <p>Updated context with pipeline results / \u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u7d50\u679c\u4ed8\u304d\u66f4\u65b0\u6e08\u307f\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8</p> Source code in <code>src\\agents_sdk_models\\gen_agent.py</code> <pre><code>async def run(self, user_input: Optional[str], ctx: Context) -&gt; Context:\n    \"\"\"\n    Execute GenAgent step using AgentPipeline\n    AgentPipeline\u3092\u4f7f\u7528\u3057\u3066GenAgent\u30b9\u30c6\u30c3\u30d7\u3092\u5b9f\u884c\u3059\u308b\n\n    Args:\n        user_input: User input for the pipeline / \u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u7528\u30e6\u30fc\u30b6\u30fc\u5165\u529b\n        ctx: Current workflow context / \u73fe\u5728\u306e\u30ef\u30fc\u30af\u30d5\u30ed\u30fc\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\n\n    Returns:\n        Context: Updated context with pipeline results / \u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u7d50\u679c\u4ed8\u304d\u66f4\u65b0\u6e08\u307f\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\n    \"\"\"\n    # English: Update step information in context\n    # \u65e5\u672c\u8a9e: \u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u306e\u30b9\u30c6\u30c3\u30d7\u60c5\u5831\u3092\u66f4\u65b0\n    ctx.update_step_info(self.name)\n\n    try:\n        # English: Determine input text for pipeline\n        # \u65e5\u672c\u8a9e: \u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u7528\u5165\u529b\u30c6\u30ad\u30b9\u30c8\u3092\u6c7a\u5b9a\n        input_text = user_input or ctx.last_user_input or \"\"\n\n        if not input_text:\n            # English: If no input available, add system message and continue\n            # \u65e5\u672c\u8a9e: \u5165\u529b\u304c\u306a\u3044\u5834\u5408\u3001\u30b7\u30b9\u30c6\u30e0\u30e1\u30c3\u30bb\u30fc\u30b8\u3092\u8ffd\u52a0\u3057\u3066\u7d9a\u884c\n            ctx.add_system_message(f\"GenAgent {self.name}: No input available, skipping pipeline execution\")\n            result = None\n        else:\n            # English: Execute pipeline in thread pool to handle sync methods\n            # \u65e5\u672c\u8a9e: \u540c\u671f\u30e1\u30bd\u30c3\u30c9\u3092\u51e6\u7406\u3059\u308b\u305f\u3081\u306b\u30b9\u30ec\u30c3\u30c9\u30d7\u30fc\u30eb\u3067\u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u3092\u5b9f\u884c\n            loop = asyncio.get_event_loop()\n            with ThreadPoolExecutor() as executor:\n                future = loop.run_in_executor(executor, self.pipeline.run, input_text)\n                result = await future\n\n        # English: Store result in context\n        # \u65e5\u672c\u8a9e: \u7d50\u679c\u3092\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u306b\u4fdd\u5b58\n        if result is not None:\n            # English: Store in shared state for other steps to access\n            # \u65e5\u672c\u8a9e: \u4ed6\u306e\u30b9\u30c6\u30c3\u30d7\u304c\u30a2\u30af\u30bb\u30b9\u3067\u304d\u308b\u3088\u3046\u5171\u6709\u72b6\u614b\u306b\u4fdd\u5b58\n            ctx.shared_state[self.store_result_key] = result\n            ctx.prev_outputs[self.name] = result\n\n            # English: Add result as assistant message\n            # \u65e5\u672c\u8a9e: \u7d50\u679c\u3092\u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u30e1\u30c3\u30bb\u30fc\u30b8\u3068\u3057\u3066\u8ffd\u52a0\n            ctx.add_assistant_message(str(result))\n\n            # English: Add success system message\n            # \u65e5\u672c\u8a9e: \u6210\u529f\u30b7\u30b9\u30c6\u30e0\u30e1\u30c3\u30bb\u30fc\u30b8\u3092\u8ffd\u52a0\n            ctx.add_system_message(f\"GenAgent {self.name}: Pipeline executed successfully\")\n        else:\n            # English: Handle case where pipeline returned None (evaluation failed)\n            # \u65e5\u672c\u8a9e: \u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u304cNone\u3092\u8fd4\u3057\u305f\u5834\u5408\uff08\u8a55\u4fa1\u5931\u6557\uff09\u3092\u51e6\u7406\n            ctx.shared_state[self.store_result_key] = None\n            ctx.prev_outputs[self.name] = None\n\n            # English: Add failure system message\n            # \u65e5\u672c\u8a9e: \u5931\u6557\u30b7\u30b9\u30c6\u30e0\u30e1\u30c3\u30bb\u30fc\u30b8\u3092\u8ffd\u52a0\n            ctx.add_system_message(f\"GenAgent {self.name}: Pipeline execution failed (evaluation threshold not met)\")\n\n    except Exception as e:\n        # English: Handle execution errors\n        # \u65e5\u672c\u8a9e: \u5b9f\u884c\u30a8\u30e9\u30fc\u3092\u51e6\u7406\n        error_msg = f\"GenAgent {self.name} execution error: {str(e)}\"\n        ctx.add_system_message(error_msg)\n        ctx.shared_state[self.store_result_key] = None\n        ctx.prev_outputs[self.name] = None\n\n        # English: Log error for debugging\n        # \u65e5\u672c\u8a9e: \u30c7\u30d0\u30c3\u30b0\u7528\u30a8\u30e9\u30fc\u30ed\u30b0\n        print(f\"\ud83d\udea8 {error_msg}\")\n\n    # English: Set next step if specified\n    # \u65e5\u672c\u8a9e: \u6307\u5b9a\u3055\u308c\u3066\u3044\u308b\u5834\u5408\u306f\u6b21\u30b9\u30c6\u30c3\u30d7\u3092\u8a2d\u5b9a\n    if self.next_step:\n        ctx.goto(self.next_step)\n\n    return ctx\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.GenAgent.set_threshold","title":"<code>set_threshold(threshold)</code>","text":"<p>Update evaluation threshold \u8a55\u4fa1\u95be\u5024\u3092\u66f4\u65b0\u3059\u308b</p> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>int</code> <p>New threshold value (0-100) / \u65b0\u3057\u3044\u95be\u5024\uff080-100\uff09</p> required Source code in <code>src\\agents_sdk_models\\gen_agent.py</code> <pre><code>def set_threshold(self, threshold: int) -&gt; None:\n    \"\"\"\n    Update evaluation threshold\n    \u8a55\u4fa1\u95be\u5024\u3092\u66f4\u65b0\u3059\u308b\n\n    Args:\n        threshold: New threshold value (0-100) / \u65b0\u3057\u3044\u95be\u5024\uff080-100\uff09\n    \"\"\"\n    if 0 &lt;= threshold &lt;= 100:\n        self.pipeline.threshold = threshold\n    else:\n        raise ValueError(\"Threshold must be between 0 and 100\")\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.GenAgent.update_instructions","title":"<code>update_instructions(generation_instructions=None, evaluation_instructions=None)</code>","text":"<p>Update pipeline instructions \u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u6307\u793a\u3092\u66f4\u65b0\u3059\u308b</p> <p>Parameters:</p> Name Type Description Default <code>generation_instructions</code> <code>Optional[str]</code> <p>New generation instructions / \u65b0\u3057\u3044\u751f\u6210\u6307\u793a</p> <code>None</code> <code>evaluation_instructions</code> <code>Optional[str]</code> <p>New evaluation instructions / \u65b0\u3057\u3044\u8a55\u4fa1\u6307\u793a</p> <code>None</code> Source code in <code>src\\agents_sdk_models\\gen_agent.py</code> <pre><code>def update_instructions(\n    self, \n    generation_instructions: Optional[str] = None,\n    evaluation_instructions: Optional[str] = None\n) -&gt; None:\n    \"\"\"\n    Update pipeline instructions\n    \u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u6307\u793a\u3092\u66f4\u65b0\u3059\u308b\n\n    Args:\n        generation_instructions: New generation instructions / \u65b0\u3057\u3044\u751f\u6210\u6307\u793a\n        evaluation_instructions: New evaluation instructions / \u65b0\u3057\u3044\u8a55\u4fa1\u6307\u793a\n    \"\"\"\n    if generation_instructions is not None:\n        self.pipeline.generation_instructions = generation_instructions.strip()\n        # English: Update the agent instructions\n        # \u65e5\u672c\u8a9e: \u30a8\u30fc\u30b8\u30a7\u30f3\u30c8\u6307\u793a\u3092\u66f4\u65b0\n        self.pipeline.gen_agent.instructions = generation_instructions.strip()\n\n    if evaluation_instructions is not None:\n        self.pipeline.evaluation_instructions = evaluation_instructions.strip() if evaluation_instructions else None\n        # English: Update evaluation agent if it exists\n        # \u65e5\u672c\u8a9e: \u8a55\u4fa1\u30a8\u30fc\u30b8\u30a7\u30f3\u30c8\u304c\u5b58\u5728\u3059\u308b\u5834\u5408\u306f\u66f4\u65b0\n        if self.pipeline.eval_agent and evaluation_instructions:\n            self.pipeline.eval_agent.instructions = evaluation_instructions.strip()\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.JoinStep","title":"<code>JoinStep</code>","text":"<p>               Bases: <code>Step</code></p> <p>Step that joins results from parallel branches \u4e26\u5217\u30d6\u30e9\u30f3\u30c1\u304b\u3089\u306e\u7d50\u679c\u3092\u7d50\u5408\u3059\u308b\u30b9\u30c6\u30c3\u30d7</p> <p>This step waits for parallel branches to complete and merges their results. \u3053\u306e\u30b9\u30c6\u30c3\u30d7\u306f\u4e26\u5217\u30d6\u30e9\u30f3\u30c1\u306e\u5b8c\u4e86\u3092\u5f85\u6a5f\u3057\u3001\u7d50\u679c\u3092\u30de\u30fc\u30b8\u3057\u307e\u3059\u3002</p> Source code in <code>src\\agents_sdk_models\\step.py</code> <pre><code>class JoinStep(Step):\n    \"\"\"\n    Step that joins results from parallel branches\n    \u4e26\u5217\u30d6\u30e9\u30f3\u30c1\u304b\u3089\u306e\u7d50\u679c\u3092\u7d50\u5408\u3059\u308b\u30b9\u30c6\u30c3\u30d7\n\n    This step waits for parallel branches to complete and merges their results.\n    \u3053\u306e\u30b9\u30c6\u30c3\u30d7\u306f\u4e26\u5217\u30d6\u30e9\u30f3\u30c1\u306e\u5b8c\u4e86\u3092\u5f85\u6a5f\u3057\u3001\u7d50\u679c\u3092\u30de\u30fc\u30b8\u3057\u307e\u3059\u3002\n    \"\"\"\n\n    def __init__(self, name: str, fork_step: str, join_type: str = \"all\", next_step: Optional[str] = None):\n        \"\"\"\n        Initialize join step\n        \u30b8\u30e7\u30a4\u30f3\u30b9\u30c6\u30c3\u30d7\u3092\u521d\u671f\u5316\n\n        Args:\n            name: Step name / \u30b9\u30c6\u30c3\u30d7\u540d\n            fork_step: Associated fork step name / \u95a2\u9023\u3059\u308b\u30d5\u30a9\u30fc\u30af\u30b9\u30c6\u30c3\u30d7\u540d\n            join_type: Join type (\"all\" or \"any\") / \u30b8\u30e7\u30a4\u30f3\u30bf\u30a4\u30d7\uff08\"all\"\u307e\u305f\u306f\"any\"\uff09\n            next_step: Next step after join / \u30b8\u30e7\u30a4\u30f3\u5f8c\u306e\u6b21\u30b9\u30c6\u30c3\u30d7\n        \"\"\"\n        super().__init__(name)\n        self.fork_step = fork_step\n        self.join_type = join_type\n        self.next_step = next_step\n\n    async def run(self, user_input: Optional[str], ctx: Context) -&gt; Context:\n        \"\"\"\n        Execute join step\n        \u30b8\u30e7\u30a4\u30f3\u30b9\u30c6\u30c3\u30d7\u3092\u5b9f\u884c\n\n        Args:\n            user_input: User input / \u30e6\u30fc\u30b6\u30fc\u5165\u529b\n            ctx: Current context / \u73fe\u5728\u306e\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\n\n        Returns:\n            Context: Updated context / \u66f4\u65b0\u6e08\u307f\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\n        \"\"\"\n        ctx.update_step_info(self.name)\n\n        # Get branch information from shared state\n        # \u5171\u6709\u72b6\u614b\u304b\u3089\u30d6\u30e9\u30f3\u30c1\u60c5\u5831\u3092\u53d6\u5f97\n        branches = ctx.shared_state.get(f\"{self.fork_step}_branches\", [])\n\n        # For now, just mark as completed\n        # \u73fe\u5728\u306e\u3068\u3053\u308d\u3001\u5b8c\u4e86\u3068\u3057\u3066\u30de\u30fc\u30af\u3059\u308b\u3060\u3051\n        # In a full implementation, this would wait for and merge branch results\n        # \u5b8c\u5168\u306a\u5b9f\u88c5\u3067\u306f\u3001\u3053\u308c\u306f\u30d6\u30e9\u30f3\u30c1\u7d50\u679c\u3092\u5f85\u6a5f\u3057\u3066\u30de\u30fc\u30b8\u3059\u308b\n        ctx.add_system_message(f\"Joined {len(branches)} branches using {self.join_type} strategy\")\n\n        # Set next step if specified\n        # \u6307\u5b9a\u3055\u308c\u3066\u3044\u308b\u5834\u5408\u306f\u6b21\u30b9\u30c6\u30c3\u30d7\u3092\u8a2d\u5b9a\n        if self.next_step:\n            ctx.goto(self.next_step)\n\n        return ctx\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.JoinStep.__init__","title":"<code>__init__(name, fork_step, join_type='all', next_step=None)</code>","text":"<p>Initialize join step \u30b8\u30e7\u30a4\u30f3\u30b9\u30c6\u30c3\u30d7\u3092\u521d\u671f\u5316</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Step name / \u30b9\u30c6\u30c3\u30d7\u540d</p> required <code>fork_step</code> <code>str</code> <p>Associated fork step name / \u95a2\u9023\u3059\u308b\u30d5\u30a9\u30fc\u30af\u30b9\u30c6\u30c3\u30d7\u540d</p> required <code>join_type</code> <code>str</code> <p>Join type (\"all\" or \"any\") / \u30b8\u30e7\u30a4\u30f3\u30bf\u30a4\u30d7\uff08\"all\"\u307e\u305f\u306f\"any\"\uff09</p> <code>'all'</code> <code>next_step</code> <code>Optional[str]</code> <p>Next step after join / \u30b8\u30e7\u30a4\u30f3\u5f8c\u306e\u6b21\u30b9\u30c6\u30c3\u30d7</p> <code>None</code> Source code in <code>src\\agents_sdk_models\\step.py</code> <pre><code>def __init__(self, name: str, fork_step: str, join_type: str = \"all\", next_step: Optional[str] = None):\n    \"\"\"\n    Initialize join step\n    \u30b8\u30e7\u30a4\u30f3\u30b9\u30c6\u30c3\u30d7\u3092\u521d\u671f\u5316\n\n    Args:\n        name: Step name / \u30b9\u30c6\u30c3\u30d7\u540d\n        fork_step: Associated fork step name / \u95a2\u9023\u3059\u308b\u30d5\u30a9\u30fc\u30af\u30b9\u30c6\u30c3\u30d7\u540d\n        join_type: Join type (\"all\" or \"any\") / \u30b8\u30e7\u30a4\u30f3\u30bf\u30a4\u30d7\uff08\"all\"\u307e\u305f\u306f\"any\"\uff09\n        next_step: Next step after join / \u30b8\u30e7\u30a4\u30f3\u5f8c\u306e\u6b21\u30b9\u30c6\u30c3\u30d7\n    \"\"\"\n    super().__init__(name)\n    self.fork_step = fork_step\n    self.join_type = join_type\n    self.next_step = next_step\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.JoinStep.run","title":"<code>run(user_input, ctx)</code>  <code>async</code>","text":"<p>Execute join step \u30b8\u30e7\u30a4\u30f3\u30b9\u30c6\u30c3\u30d7\u3092\u5b9f\u884c</p> <p>Parameters:</p> Name Type Description Default <code>user_input</code> <code>Optional[str]</code> <p>User input / \u30e6\u30fc\u30b6\u30fc\u5165\u529b</p> required <code>ctx</code> <code>Context</code> <p>Current context / \u73fe\u5728\u306e\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8</p> required <p>Returns:</p> Name Type Description <code>Context</code> <code>Context</code> <p>Updated context / \u66f4\u65b0\u6e08\u307f\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8</p> Source code in <code>src\\agents_sdk_models\\step.py</code> <pre><code>async def run(self, user_input: Optional[str], ctx: Context) -&gt; Context:\n    \"\"\"\n    Execute join step\n    \u30b8\u30e7\u30a4\u30f3\u30b9\u30c6\u30c3\u30d7\u3092\u5b9f\u884c\n\n    Args:\n        user_input: User input / \u30e6\u30fc\u30b6\u30fc\u5165\u529b\n        ctx: Current context / \u73fe\u5728\u306e\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\n\n    Returns:\n        Context: Updated context / \u66f4\u65b0\u6e08\u307f\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\n    \"\"\"\n    ctx.update_step_info(self.name)\n\n    # Get branch information from shared state\n    # \u5171\u6709\u72b6\u614b\u304b\u3089\u30d6\u30e9\u30f3\u30c1\u60c5\u5831\u3092\u53d6\u5f97\n    branches = ctx.shared_state.get(f\"{self.fork_step}_branches\", [])\n\n    # For now, just mark as completed\n    # \u73fe\u5728\u306e\u3068\u3053\u308d\u3001\u5b8c\u4e86\u3068\u3057\u3066\u30de\u30fc\u30af\u3059\u308b\u3060\u3051\n    # In a full implementation, this would wait for and merge branch results\n    # \u5b8c\u5168\u306a\u5b9f\u88c5\u3067\u306f\u3001\u3053\u308c\u306f\u30d6\u30e9\u30f3\u30c1\u7d50\u679c\u3092\u5f85\u6a5f\u3057\u3066\u30de\u30fc\u30b8\u3059\u308b\n    ctx.add_system_message(f\"Joined {len(branches)} branches using {self.join_type} strategy\")\n\n    # Set next step if specified\n    # \u6307\u5b9a\u3055\u308c\u3066\u3044\u308b\u5834\u5408\u306f\u6b21\u30b9\u30c6\u30c3\u30d7\u3092\u8a2d\u5b9a\n    if self.next_step:\n        ctx.goto(self.next_step)\n\n    return ctx\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.Message","title":"<code>Message</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Message class for conversation history \u4f1a\u8a71\u5c65\u6b74\u7528\u30e1\u30c3\u30bb\u30fc\u30b8\u30af\u30e9\u30b9</p> <p>Attributes:</p> Name Type Description <code>role</code> <code>str</code> <p>Message role (user, assistant, system) / \u30e1\u30c3\u30bb\u30fc\u30b8\u306e\u5f79\u5272</p> <code>content</code> <code>str</code> <p>Message content / \u30e1\u30c3\u30bb\u30fc\u30b8\u5185\u5bb9</p> <code>timestamp</code> <code>datetime</code> <p>Message timestamp / \u30e1\u30c3\u30bb\u30fc\u30b8\u306e\u30bf\u30a4\u30e0\u30b9\u30bf\u30f3\u30d7</p> <code>metadata</code> <code>Dict[str, Any]</code> <p>Additional metadata / \u8ffd\u52a0\u30e1\u30bf\u30c7\u30fc\u30bf</p> Source code in <code>src\\agents_sdk_models\\context.py</code> <pre><code>class Message(BaseModel):\n    \"\"\"\n    Message class for conversation history\n    \u4f1a\u8a71\u5c65\u6b74\u7528\u30e1\u30c3\u30bb\u30fc\u30b8\u30af\u30e9\u30b9\n\n    Attributes:\n        role: Message role (user, assistant, system) / \u30e1\u30c3\u30bb\u30fc\u30b8\u306e\u5f79\u5272\n        content: Message content / \u30e1\u30c3\u30bb\u30fc\u30b8\u5185\u5bb9\n        timestamp: Message timestamp / \u30e1\u30c3\u30bb\u30fc\u30b8\u306e\u30bf\u30a4\u30e0\u30b9\u30bf\u30f3\u30d7\n        metadata: Additional metadata / \u8ffd\u52a0\u30e1\u30bf\u30c7\u30fc\u30bf\n    \"\"\"\n    role: str  # Message role (user, assistant, system) / \u30e1\u30c3\u30bb\u30fc\u30b8\u306e\u5f79\u5272\n    content: str  # Message content / \u30e1\u30c3\u30bb\u30fc\u30b8\u5185\u5bb9\n    timestamp: datetime = Field(default_factory=datetime.now)  # Message timestamp / \u30e1\u30c3\u30bb\u30fc\u30b8\u306e\u30bf\u30a4\u30e0\u30b9\u30bf\u30f3\u30d7\n    metadata: Dict[str, Any] = Field(default_factory=dict)  # Additional metadata / \u8ffd\u52a0\u30e1\u30bf\u30c7\u30fc\u30bf\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.OllamaModel","title":"<code>OllamaModel</code>","text":"<p>               Bases: <code>OpenAIChatCompletionsModel</code></p> <p>Ollama model implementation that extends OpenAI's chat completions model OpenAI\u306e\u30c1\u30e3\u30c3\u30c8\u88dc\u5b8c\u30e2\u30c7\u30eb\u3092\u62e1\u5f35\u3057\u305fOllama\u30e2\u30c7\u30eb\u306e\u5b9f\u88c5</p> Source code in <code>src\\agents_sdk_models\\ollama.py</code> <pre><code>class OllamaModel(OpenAIChatCompletionsModel):\n    \"\"\"\n    Ollama model implementation that extends OpenAI's chat completions model\n    OpenAI\u306e\u30c1\u30e3\u30c3\u30c8\u88dc\u5b8c\u30e2\u30c7\u30eb\u3092\u62e1\u5f35\u3057\u305fOllama\u30e2\u30c7\u30eb\u306e\u5b9f\u88c5\n    \"\"\"\n\n    def __init__(\n        self,\n        model: str = \"phi4-mini:latest\",\n        temperature: float = 0.3,\n        base_url: str = None, # \u30c7\u30d5\u30a9\u30eb\u30c8\u306eURL\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the Ollama model with OpenAI compatible interface\n        OpenAI\u4e92\u63db\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30fc\u30b9\u3067Ollama\u30e2\u30c7\u30eb\u3092\u521d\u671f\u5316\u3059\u308b\n\n        Args:\n            model (str): Name of the Ollama model to use (e.g. \"phi4-mini\")\n                \u4f7f\u7528\u3059\u308bOllama\u30e2\u30c7\u30eb\u306e\u540d\u524d\uff08\u4f8b\uff1a\"phi4-mini\"\uff09\n            temperature (float): Sampling temperature between 0 and 1\n                \u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u6e29\u5ea6\uff080\u304b\u30891\u306e\u9593\uff09\n            base_url (str): Base URL for the Ollama API\n                Ollama API\u306e\u30d9\u30fc\u30b9URL\n            **kwargs: Additional arguments to pass to the OpenAI API\n                OpenAI API\u306b\u6e21\u3059\u8ffd\u52a0\u306e\u5f15\u6570\n        \"\"\"\n        # get_llm\u7d4c\u7531\u3067 base_url \u304c None \u306e\u5834\u5408\u306f\u30c7\u30d5\u30a9\u30eb\u30c8\u306e URL \u3092\u8a2d\u5b9a\n        if base_url == None:\n            base_url = os.environ.get(\"OLLAMA_BASE_URL\", \"http://localhost:11434\")\n\n        base_url = base_url.rstrip(\"/\")\n        if not base_url.endswith(\"v1\"):\n            base_url = base_url + \"/v1\"\n\n        # Create AsyncOpenAI client with Ollama base URL\n        # Ollama\u306e\u30d9\u30fc\u30b9URL\u3067AsyncOpenAI\u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u3092\u4f5c\u6210\n        openai_client = AsyncOpenAI(base_url=base_url, api_key=\"ollama\")\n\n        # Store the AsyncOpenAI client on the instance for direct access\n        # \u30c6\u30b9\u30c8\u3067\u53c2\u7167\u3067\u304d\u308b\u3088\u3046 AsyncOpenAI \u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u3092\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u306b\u4fdd\u5b58\u3059\u308b\n        self.openai_client = openai_client\n\n        # Store parameters for later use in API calls\n        # \u5f8c\u3067API\u30b3\u30fc\u30eb\u3067\u4f7f\u7528\u3059\u308b\u305f\u3081\u306b\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u4fdd\u5b58\n        self.temperature = temperature\n        self.kwargs = kwargs\n\n        # Initialize the parent class with our custom client\n        # \u30ab\u30b9\u30bf\u30e0\u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u3067\u89aa\u30af\u30e9\u30b9\u3092\u521d\u671f\u5316\n        super().__init__(\n            model=model,\n            openai_client=openai_client\n        )\n\n    # Override methods that make API calls to include our parameters\n    # API\u30b3\u30fc\u30eb\u3092\u884c\u3046\u30e1\u30bd\u30c3\u30c9\u3092\u30aa\u30fc\u30d0\u30fc\u30e9\u30a4\u30c9\u3057\u3066\u3001\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u542b\u3081\u308b\n    async def _create_chat_completion(self, *args, **kwargs):\n        \"\"\"Override to include temperature and other parameters\"\"\"\n        kwargs[\"temperature\"] = self.temperature\n        kwargs.update(self.kwargs)\n        return await super()._create_chat_completion(*args, **kwargs)\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.OllamaModel.__init__","title":"<code>__init__(model='phi4-mini:latest', temperature=0.3, base_url=None, **kwargs)</code>","text":"<p>Initialize the Ollama model with OpenAI compatible interface OpenAI\u4e92\u63db\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30fc\u30b9\u3067Ollama\u30e2\u30c7\u30eb\u3092\u521d\u671f\u5316\u3059\u308b</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Name of the Ollama model to use (e.g. \"phi4-mini\") \u4f7f\u7528\u3059\u308bOllama\u30e2\u30c7\u30eb\u306e\u540d\u524d\uff08\u4f8b\uff1a\"phi4-mini\"\uff09</p> <code>'phi4-mini:latest'</code> <code>temperature</code> <code>float</code> <p>Sampling temperature between 0 and 1 \u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u6e29\u5ea6\uff080\u304b\u30891\u306e\u9593\uff09</p> <code>0.3</code> <code>base_url</code> <code>str</code> <p>Base URL for the Ollama API Ollama API\u306e\u30d9\u30fc\u30b9URL</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments to pass to the OpenAI API OpenAI API\u306b\u6e21\u3059\u8ffd\u52a0\u306e\u5f15\u6570</p> <code>{}</code> Source code in <code>src\\agents_sdk_models\\ollama.py</code> <pre><code>def __init__(\n    self,\n    model: str = \"phi4-mini:latest\",\n    temperature: float = 0.3,\n    base_url: str = None, # \u30c7\u30d5\u30a9\u30eb\u30c8\u306eURL\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initialize the Ollama model with OpenAI compatible interface\n    OpenAI\u4e92\u63db\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30fc\u30b9\u3067Ollama\u30e2\u30c7\u30eb\u3092\u521d\u671f\u5316\u3059\u308b\n\n    Args:\n        model (str): Name of the Ollama model to use (e.g. \"phi4-mini\")\n            \u4f7f\u7528\u3059\u308bOllama\u30e2\u30c7\u30eb\u306e\u540d\u524d\uff08\u4f8b\uff1a\"phi4-mini\"\uff09\n        temperature (float): Sampling temperature between 0 and 1\n            \u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u6e29\u5ea6\uff080\u304b\u30891\u306e\u9593\uff09\n        base_url (str): Base URL for the Ollama API\n            Ollama API\u306e\u30d9\u30fc\u30b9URL\n        **kwargs: Additional arguments to pass to the OpenAI API\n            OpenAI API\u306b\u6e21\u3059\u8ffd\u52a0\u306e\u5f15\u6570\n    \"\"\"\n    # get_llm\u7d4c\u7531\u3067 base_url \u304c None \u306e\u5834\u5408\u306f\u30c7\u30d5\u30a9\u30eb\u30c8\u306e URL \u3092\u8a2d\u5b9a\n    if base_url == None:\n        base_url = os.environ.get(\"OLLAMA_BASE_URL\", \"http://localhost:11434\")\n\n    base_url = base_url.rstrip(\"/\")\n    if not base_url.endswith(\"v1\"):\n        base_url = base_url + \"/v1\"\n\n    # Create AsyncOpenAI client with Ollama base URL\n    # Ollama\u306e\u30d9\u30fc\u30b9URL\u3067AsyncOpenAI\u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u3092\u4f5c\u6210\n    openai_client = AsyncOpenAI(base_url=base_url, api_key=\"ollama\")\n\n    # Store the AsyncOpenAI client on the instance for direct access\n    # \u30c6\u30b9\u30c8\u3067\u53c2\u7167\u3067\u304d\u308b\u3088\u3046 AsyncOpenAI \u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u3092\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u306b\u4fdd\u5b58\u3059\u308b\n    self.openai_client = openai_client\n\n    # Store parameters for later use in API calls\n    # \u5f8c\u3067API\u30b3\u30fc\u30eb\u3067\u4f7f\u7528\u3059\u308b\u305f\u3081\u306b\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u4fdd\u5b58\n    self.temperature = temperature\n    self.kwargs = kwargs\n\n    # Initialize the parent class with our custom client\n    # \u30ab\u30b9\u30bf\u30e0\u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u3067\u89aa\u30af\u30e9\u30b9\u3092\u521d\u671f\u5316\n    super().__init__(\n        model=model,\n        openai_client=openai_client\n    )\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.Step","title":"<code>Step</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for workflow steps \u30ef\u30fc\u30af\u30d5\u30ed\u30fc\u30b9\u30c6\u30c3\u30d7\u306e\u62bd\u8c61\u57fa\u5e95\u30af\u30e9\u30b9</p> <p>All step implementations must provide: \u5168\u3066\u306e\u30b9\u30c6\u30c3\u30d7\u5b9f\u88c5\u306f\u4ee5\u4e0b\u3092\u63d0\u4f9b\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\uff1a - name: Step identifier for DSL reference / DSL\u53c2\u7167\u7528\u30b9\u30c6\u30c3\u30d7\u8b58\u5225\u5b50 - run: Async execution method / \u975e\u540c\u671f\u5b9f\u884c\u30e1\u30bd\u30c3\u30c9</p> Source code in <code>src\\agents_sdk_models\\step.py</code> <pre><code>class Step(ABC):\n    \"\"\"\n    Abstract base class for workflow steps\n    \u30ef\u30fc\u30af\u30d5\u30ed\u30fc\u30b9\u30c6\u30c3\u30d7\u306e\u62bd\u8c61\u57fa\u5e95\u30af\u30e9\u30b9\n\n    All step implementations must provide:\n    \u5168\u3066\u306e\u30b9\u30c6\u30c3\u30d7\u5b9f\u88c5\u306f\u4ee5\u4e0b\u3092\u63d0\u4f9b\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\uff1a\n    - name: Step identifier for DSL reference / DSL\u53c2\u7167\u7528\u30b9\u30c6\u30c3\u30d7\u8b58\u5225\u5b50\n    - run: Async execution method / \u975e\u540c\u671f\u5b9f\u884c\u30e1\u30bd\u30c3\u30c9\n    \"\"\"\n\n    def __init__(self, name: str):\n        \"\"\"\n        Initialize step with name\n        \u540d\u524d\u3067\u30b9\u30c6\u30c3\u30d7\u3092\u521d\u671f\u5316\n\n        Args:\n            name: Step name / \u30b9\u30c6\u30c3\u30d7\u540d\n        \"\"\"\n        self.name = name\n\n    @abstractmethod\n    async def run(self, user_input: Optional[str], ctx: Context) -&gt; Context:\n        \"\"\"\n        Execute step and return updated context\n        \u30b9\u30c6\u30c3\u30d7\u3092\u5b9f\u884c\u3057\u3001\u66f4\u65b0\u3055\u308c\u305f\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u3092\u8fd4\u3059\n\n        Args:\n            user_input: User input if any / \u30e6\u30fc\u30b6\u30fc\u5165\u529b\uff08\u3042\u308c\u3070\uff09\n            ctx: Current context / \u73fe\u5728\u306e\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\n\n        Returns:\n            Context: Updated context with next_label set / next_label\u304c\u8a2d\u5b9a\u3055\u308c\u305f\u66f4\u65b0\u6e08\u307f\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\n        \"\"\"\n        pass\n\n    def __str__(self) -&gt; str:\n        return f\"{self.__class__.__name__}({self.name})\"\n\n    def __repr__(self) -&gt; str:\n        return self.__str__()\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.Step.__init__","title":"<code>__init__(name)</code>","text":"<p>Initialize step with name \u540d\u524d\u3067\u30b9\u30c6\u30c3\u30d7\u3092\u521d\u671f\u5316</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Step name / \u30b9\u30c6\u30c3\u30d7\u540d</p> required Source code in <code>src\\agents_sdk_models\\step.py</code> <pre><code>def __init__(self, name: str):\n    \"\"\"\n    Initialize step with name\n    \u540d\u524d\u3067\u30b9\u30c6\u30c3\u30d7\u3092\u521d\u671f\u5316\n\n    Args:\n        name: Step name / \u30b9\u30c6\u30c3\u30d7\u540d\n    \"\"\"\n    self.name = name\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.Step.run","title":"<code>run(user_input, ctx)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Execute step and return updated context \u30b9\u30c6\u30c3\u30d7\u3092\u5b9f\u884c\u3057\u3001\u66f4\u65b0\u3055\u308c\u305f\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u3092\u8fd4\u3059</p> <p>Parameters:</p> Name Type Description Default <code>user_input</code> <code>Optional[str]</code> <p>User input if any / \u30e6\u30fc\u30b6\u30fc\u5165\u529b\uff08\u3042\u308c\u3070\uff09</p> required <code>ctx</code> <code>Context</code> <p>Current context / \u73fe\u5728\u306e\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8</p> required <p>Returns:</p> Name Type Description <code>Context</code> <code>Context</code> <p>Updated context with next_label set / next_label\u304c\u8a2d\u5b9a\u3055\u308c\u305f\u66f4\u65b0\u6e08\u307f\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8</p> Source code in <code>src\\agents_sdk_models\\step.py</code> <pre><code>@abstractmethod\nasync def run(self, user_input: Optional[str], ctx: Context) -&gt; Context:\n    \"\"\"\n    Execute step and return updated context\n    \u30b9\u30c6\u30c3\u30d7\u3092\u5b9f\u884c\u3057\u3001\u66f4\u65b0\u3055\u308c\u305f\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u3092\u8fd4\u3059\n\n    Args:\n        user_input: User input if any / \u30e6\u30fc\u30b6\u30fc\u5165\u529b\uff08\u3042\u308c\u3070\uff09\n        ctx: Current context / \u73fe\u5728\u306e\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\n\n    Returns:\n        Context: Updated context with next_label set / next_label\u304c\u8a2d\u5b9a\u3055\u308c\u305f\u66f4\u65b0\u6e08\u307f\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.UserInputStep","title":"<code>UserInputStep</code>","text":"<p>               Bases: <code>Step</code></p> <p>Step that waits for user input \u30e6\u30fc\u30b6\u30fc\u5165\u529b\u3092\u5f85\u6a5f\u3059\u308b\u30b9\u30c6\u30c3\u30d7</p> <p>This step displays a prompt and waits for user response. \u3053\u306e\u30b9\u30c6\u30c3\u30d7\u306f\u30d7\u30ed\u30f3\u30d7\u30c8\u3092\u8868\u793a\u3057\u3001\u30e6\u30fc\u30b6\u30fc\u5fdc\u7b54\u3092\u5f85\u6a5f\u3057\u307e\u3059\u3002 It sets the context to waiting state and returns without advancing. \u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u3092\u5f85\u6a5f\u72b6\u614b\u306b\u8a2d\u5b9a\u3057\u3001\u9032\u884c\u305b\u305a\u306b\u8fd4\u3057\u307e\u3059\u3002</p> Source code in <code>src\\agents_sdk_models\\step.py</code> <pre><code>class UserInputStep(Step):\n    \"\"\"\n    Step that waits for user input\n    \u30e6\u30fc\u30b6\u30fc\u5165\u529b\u3092\u5f85\u6a5f\u3059\u308b\u30b9\u30c6\u30c3\u30d7\n\n    This step displays a prompt and waits for user response.\n    \u3053\u306e\u30b9\u30c6\u30c3\u30d7\u306f\u30d7\u30ed\u30f3\u30d7\u30c8\u3092\u8868\u793a\u3057\u3001\u30e6\u30fc\u30b6\u30fc\u5fdc\u7b54\u3092\u5f85\u6a5f\u3057\u307e\u3059\u3002\n    It sets the context to waiting state and returns without advancing.\n    \u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u3092\u5f85\u6a5f\u72b6\u614b\u306b\u8a2d\u5b9a\u3057\u3001\u9032\u884c\u305b\u305a\u306b\u8fd4\u3057\u307e\u3059\u3002\n    \"\"\"\n\n    def __init__(self, name: str, prompt: str, next_step: Optional[str] = None):\n        \"\"\"\n        Initialize user input step\n        \u30e6\u30fc\u30b6\u30fc\u5165\u529b\u30b9\u30c6\u30c3\u30d7\u3092\u521d\u671f\u5316\n\n        Args:\n            name: Step name / \u30b9\u30c6\u30c3\u30d7\u540d\n            prompt: Prompt to display to user / \u30e6\u30fc\u30b6\u30fc\u306b\u8868\u793a\u3059\u308b\u30d7\u30ed\u30f3\u30d7\u30c8\n            next_step: Next step after input (optional) / \u5165\u529b\u5f8c\u306e\u6b21\u30b9\u30c6\u30c3\u30d7\uff08\u30aa\u30d7\u30b7\u30e7\u30f3\uff09\n        \"\"\"\n        super().__init__(name)\n        self.prompt = prompt\n        self.next_step = next_step\n\n    async def run(self, user_input: Optional[str], ctx: Context) -&gt; Context:\n        \"\"\"\n        Execute user input step\n        \u30e6\u30fc\u30b6\u30fc\u5165\u529b\u30b9\u30c6\u30c3\u30d7\u3092\u5b9f\u884c\n\n        Args:\n            user_input: User input if available / \u5229\u7528\u53ef\u80fd\u306a\u30e6\u30fc\u30b6\u30fc\u5165\u529b\n            ctx: Current context / \u73fe\u5728\u306e\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\n\n        Returns:\n            Context: Updated context / \u66f4\u65b0\u6e08\u307f\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\n        \"\"\"\n        ctx.update_step_info(self.name)\n\n        # If user input is provided, process it\n        # \u30e6\u30fc\u30b6\u30fc\u5165\u529b\u304c\u63d0\u4f9b\u3055\u308c\u3066\u3044\u308b\u5834\u5408\u3001\u51e6\u7406\u3059\u308b\n        if user_input is not None:\n            ctx.provide_user_input(user_input)\n            if self.next_step:\n                ctx.goto(self.next_step)\n            # Note: If next_step is None, flow will end\n            # \u6ce8\uff1anext_step\u304cNone\u306e\u5834\u5408\u3001\u30d5\u30ed\u30fc\u306f\u7d42\u4e86\n        else:\n            # Set waiting state for user input\n            # \u30e6\u30fc\u30b6\u30fc\u5165\u529b\u306e\u5f85\u6a5f\u72b6\u614b\u3092\u8a2d\u5b9a\n            ctx.set_waiting_for_user_input(self.prompt)\n\n        return ctx\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.UserInputStep.__init__","title":"<code>__init__(name, prompt, next_step=None)</code>","text":"<p>Initialize user input step \u30e6\u30fc\u30b6\u30fc\u5165\u529b\u30b9\u30c6\u30c3\u30d7\u3092\u521d\u671f\u5316</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Step name / \u30b9\u30c6\u30c3\u30d7\u540d</p> required <code>prompt</code> <code>str</code> <p>Prompt to display to user / \u30e6\u30fc\u30b6\u30fc\u306b\u8868\u793a\u3059\u308b\u30d7\u30ed\u30f3\u30d7\u30c8</p> required <code>next_step</code> <code>Optional[str]</code> <p>Next step after input (optional) / \u5165\u529b\u5f8c\u306e\u6b21\u30b9\u30c6\u30c3\u30d7\uff08\u30aa\u30d7\u30b7\u30e7\u30f3\uff09</p> <code>None</code> Source code in <code>src\\agents_sdk_models\\step.py</code> <pre><code>def __init__(self, name: str, prompt: str, next_step: Optional[str] = None):\n    \"\"\"\n    Initialize user input step\n    \u30e6\u30fc\u30b6\u30fc\u5165\u529b\u30b9\u30c6\u30c3\u30d7\u3092\u521d\u671f\u5316\n\n    Args:\n        name: Step name / \u30b9\u30c6\u30c3\u30d7\u540d\n        prompt: Prompt to display to user / \u30e6\u30fc\u30b6\u30fc\u306b\u8868\u793a\u3059\u308b\u30d7\u30ed\u30f3\u30d7\u30c8\n        next_step: Next step after input (optional) / \u5165\u529b\u5f8c\u306e\u6b21\u30b9\u30c6\u30c3\u30d7\uff08\u30aa\u30d7\u30b7\u30e7\u30f3\uff09\n    \"\"\"\n    super().__init__(name)\n    self.prompt = prompt\n    self.next_step = next_step\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.UserInputStep.run","title":"<code>run(user_input, ctx)</code>  <code>async</code>","text":"<p>Execute user input step \u30e6\u30fc\u30b6\u30fc\u5165\u529b\u30b9\u30c6\u30c3\u30d7\u3092\u5b9f\u884c</p> <p>Parameters:</p> Name Type Description Default <code>user_input</code> <code>Optional[str]</code> <p>User input if available / \u5229\u7528\u53ef\u80fd\u306a\u30e6\u30fc\u30b6\u30fc\u5165\u529b</p> required <code>ctx</code> <code>Context</code> <p>Current context / \u73fe\u5728\u306e\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8</p> required <p>Returns:</p> Name Type Description <code>Context</code> <code>Context</code> <p>Updated context / \u66f4\u65b0\u6e08\u307f\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8</p> Source code in <code>src\\agents_sdk_models\\step.py</code> <pre><code>async def run(self, user_input: Optional[str], ctx: Context) -&gt; Context:\n    \"\"\"\n    Execute user input step\n    \u30e6\u30fc\u30b6\u30fc\u5165\u529b\u30b9\u30c6\u30c3\u30d7\u3092\u5b9f\u884c\n\n    Args:\n        user_input: User input if available / \u5229\u7528\u53ef\u80fd\u306a\u30e6\u30fc\u30b6\u30fc\u5165\u529b\n        ctx: Current context / \u73fe\u5728\u306e\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\n\n    Returns:\n        Context: Updated context / \u66f4\u65b0\u6e08\u307f\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\n    \"\"\"\n    ctx.update_step_info(self.name)\n\n    # If user input is provided, process it\n    # \u30e6\u30fc\u30b6\u30fc\u5165\u529b\u304c\u63d0\u4f9b\u3055\u308c\u3066\u3044\u308b\u5834\u5408\u3001\u51e6\u7406\u3059\u308b\n    if user_input is not None:\n        ctx.provide_user_input(user_input)\n        if self.next_step:\n            ctx.goto(self.next_step)\n        # Note: If next_step is None, flow will end\n        # \u6ce8\uff1anext_step\u304cNone\u306e\u5834\u5408\u3001\u30d5\u30ed\u30fc\u306f\u7d42\u4e86\n    else:\n        # Set waiting state for user input\n        # \u30e6\u30fc\u30b6\u30fc\u5165\u529b\u306e\u5f85\u6a5f\u72b6\u614b\u3092\u8a2d\u5b9a\n        ctx.set_waiting_for_user_input(self.prompt)\n\n    return ctx\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.create_conditional_flow","title":"<code>create_conditional_flow(initial_step, condition_step, true_branch, false_branch, context=None)</code>","text":"<p>Create a conditional flow with true/false branches true/false\u30d6\u30e9\u30f3\u30c1\u3092\u6301\u3064\u6761\u4ef6\u4ed8\u304d\u30d5\u30ed\u30fc\u3092\u4f5c\u6210</p> <p>Parameters:</p> Name Type Description Default <code>initial_step</code> <code>Step</code> <p>Initial step / \u521d\u671f\u30b9\u30c6\u30c3\u30d7</p> required <code>condition_step</code> <code>Step</code> <p>Condition step / \u6761\u4ef6\u30b9\u30c6\u30c3\u30d7</p> required <code>true_branch</code> <code>List[Step]</code> <p>Steps for true branch / true\u30d6\u30e9\u30f3\u30c1\u306e\u30b9\u30c6\u30c3\u30d7</p> required <code>false_branch</code> <code>List[Step]</code> <p>Steps for false branch / false\u30d6\u30e9\u30f3\u30c1\u306e\u30b9\u30c6\u30c3\u30d7</p> required <code>context</code> <code>Optional[Context]</code> <p>Initial context / \u521d\u671f\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Flow</code> <code>Flow</code> <p>Created flow / \u4f5c\u6210\u3055\u308c\u305f\u30d5\u30ed\u30fc</p> Source code in <code>src\\agents_sdk_models\\flow.py</code> <pre><code>def create_conditional_flow(\n    initial_step: Step,\n    condition_step: Step,\n    true_branch: List[Step],\n    false_branch: List[Step],\n    context: Optional[Context] = None\n) -&gt; Flow:\n    \"\"\"\n    Create a conditional flow with true/false branches\n    true/false\u30d6\u30e9\u30f3\u30c1\u3092\u6301\u3064\u6761\u4ef6\u4ed8\u304d\u30d5\u30ed\u30fc\u3092\u4f5c\u6210\n\n    Args:\n        initial_step: Initial step / \u521d\u671f\u30b9\u30c6\u30c3\u30d7\n        condition_step: Condition step / \u6761\u4ef6\u30b9\u30c6\u30c3\u30d7\n        true_branch: Steps for true branch / true\u30d6\u30e9\u30f3\u30c1\u306e\u30b9\u30c6\u30c3\u30d7\n        false_branch: Steps for false branch / false\u30d6\u30e9\u30f3\u30c1\u306e\u30b9\u30c6\u30c3\u30d7\n        context: Initial context / \u521d\u671f\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\n\n    Returns:\n        Flow: Created flow / \u4f5c\u6210\u3055\u308c\u305f\u30d5\u30ed\u30fc\n    \"\"\"\n    steps = {\n        \"start\": initial_step,\n        \"condition\": condition_step\n    }\n\n    # Add true branch steps\n    # true\u30d6\u30e9\u30f3\u30c1\u30b9\u30c6\u30c3\u30d7\u3092\u8ffd\u52a0\n    for i, step in enumerate(true_branch):\n        step_name = f\"true_{i}\"\n        steps[step_name] = step\n        if i == 0 and hasattr(condition_step, 'if_true'):\n            condition_step.if_true = step_name\n\n    # Add false branch steps\n    # false\u30d6\u30e9\u30f3\u30c1\u30b9\u30c6\u30c3\u30d7\u3092\u8ffd\u52a0\n    for i, step in enumerate(false_branch):\n        step_name = f\"false_{i}\"\n        steps[step_name] = step\n        if i == 0 and hasattr(condition_step, 'if_false'):\n            condition_step.if_false = step_name\n\n    # Connect initial step to condition\n    # \u521d\u671f\u30b9\u30c6\u30c3\u30d7\u3092\u6761\u4ef6\u306b\u63a5\u7d9a\n    if hasattr(initial_step, 'next_step'):\n        initial_step.next_step = \"condition\"\n\n    return Flow(\n        start=\"start\",\n        steps=steps,\n        context=context\n    ) \n</code></pre>"},{"location":"api_reference/#agents_sdk_models.create_evaluated_gen_agent","title":"<code>create_evaluated_gen_agent(name, generation_instructions, evaluation_instructions, model=None, evaluation_model=None, next_step=None, threshold=85, retries=3)</code>","text":"<p>Create a GenAgent with both generation and evaluation \u751f\u6210\u3068\u8a55\u4fa1\u306e\u4e21\u65b9\u3092\u6301\u3064GenAgent\u3092\u4f5c\u6210</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Agent name / \u30a8\u30fc\u30b8\u30a7\u30f3\u30c8\u540d</p> required <code>generation_instructions</code> <code>str</code> <p>Generation instructions / \u751f\u6210\u6307\u793a</p> required <code>evaluation_instructions</code> <code>str</code> <p>Evaluation instructions / \u8a55\u4fa1\u6307\u793a</p> required <code>model</code> <code>Optional[str]</code> <p>LLM model name / LLM\u30e2\u30c7\u30eb\u540d</p> <code>None</code> <code>evaluation_model</code> <code>Optional[str]</code> <p>Evaluation model name / \u8a55\u4fa1\u30e2\u30c7\u30eb\u540d</p> <code>None</code> <code>next_step</code> <code>Optional[str]</code> <p>Next step after execution / \u5b9f\u884c\u5f8c\u306e\u6b21\u30b9\u30c6\u30c3\u30d7</p> <code>None</code> <code>threshold</code> <code>int</code> <p>Evaluation threshold / \u8a55\u4fa1\u95be\u5024</p> <code>85</code> <code>retries</code> <code>int</code> <p>Number of retries / \u30ea\u30c8\u30e9\u30a4\u56de\u6570</p> <code>3</code> <p>Returns:</p> Name Type Description <code>GenAgent</code> <code>GenAgent</code> <p>Configured GenAgent instance / \u8a2d\u5b9a\u6e08\u307fGenAgent\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9</p> Source code in <code>src\\agents_sdk_models\\gen_agent.py</code> <pre><code>def create_evaluated_gen_agent(\n    name: str,\n    generation_instructions: str,\n    evaluation_instructions: str,\n    model: Optional[str] = None,\n    evaluation_model: Optional[str] = None,\n    next_step: Optional[str] = None,\n    threshold: int = 85,\n    retries: int = 3\n) -&gt; GenAgent:\n    \"\"\"\n    Create a GenAgent with both generation and evaluation\n    \u751f\u6210\u3068\u8a55\u4fa1\u306e\u4e21\u65b9\u3092\u6301\u3064GenAgent\u3092\u4f5c\u6210\n\n    Args:\n        name: Agent name / \u30a8\u30fc\u30b8\u30a7\u30f3\u30c8\u540d\n        generation_instructions: Generation instructions / \u751f\u6210\u6307\u793a\n        evaluation_instructions: Evaluation instructions / \u8a55\u4fa1\u6307\u793a\n        model: LLM model name / LLM\u30e2\u30c7\u30eb\u540d\n        evaluation_model: Evaluation model name / \u8a55\u4fa1\u30e2\u30c7\u30eb\u540d\n        next_step: Next step after execution / \u5b9f\u884c\u5f8c\u306e\u6b21\u30b9\u30c6\u30c3\u30d7\n        threshold: Evaluation threshold / \u8a55\u4fa1\u95be\u5024\n        retries: Number of retries / \u30ea\u30c8\u30e9\u30a4\u56de\u6570\n\n    Returns:\n        GenAgent: Configured GenAgent instance / \u8a2d\u5b9a\u6e08\u307fGenAgent\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\n    \"\"\"\n    return GenAgent(\n        name=name,\n        generation_instructions=generation_instructions,\n        evaluation_instructions=evaluation_instructions,\n        model=model,\n        evaluation_model=evaluation_model,\n        next_step=next_step,\n        threshold=threshold,\n        retries=retries\n    ) \n</code></pre>"},{"location":"api_reference/#agents_sdk_models.create_lambda_step","title":"<code>create_lambda_step(name, func, next_step=None)</code>","text":"<p>Create a simple function step from a lambda \u30e9\u30e0\u30c0\u304b\u3089\u7c21\u5358\u306a\u95a2\u6570\u30b9\u30c6\u30c3\u30d7\u3092\u4f5c\u6210</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Step name / \u30b9\u30c6\u30c3\u30d7\u540d</p> required <code>func</code> <code>Callable[[Context], Any]</code> <p>Function to execute / \u5b9f\u884c\u3059\u308b\u95a2\u6570</p> required <code>next_step</code> <code>Optional[str]</code> <p>Next step / \u6b21\u30b9\u30c6\u30c3\u30d7</p> <code>None</code> <p>Returns:</p> Name Type Description <code>FunctionStep</code> <code>FunctionStep</code> <p>Function step / \u95a2\u6570\u30b9\u30c6\u30c3\u30d7</p> Source code in <code>src\\agents_sdk_models\\step.py</code> <pre><code>def create_lambda_step(name: str, func: Callable[[Context], Any], next_step: Optional[str] = None) -&gt; FunctionStep:\n    \"\"\"\n    Create a simple function step from a lambda\n    \u30e9\u30e0\u30c0\u304b\u3089\u7c21\u5358\u306a\u95a2\u6570\u30b9\u30c6\u30c3\u30d7\u3092\u4f5c\u6210\n\n    Args:\n        name: Step name / \u30b9\u30c6\u30c3\u30d7\u540d\n        func: Function to execute / \u5b9f\u884c\u3059\u308b\u95a2\u6570\n        next_step: Next step / \u6b21\u30b9\u30c6\u30c3\u30d7\n\n    Returns:\n        FunctionStep: Function step / \u95a2\u6570\u30b9\u30c6\u30c3\u30d7\n    \"\"\"\n    def wrapper(user_input: Optional[str], ctx: Context) -&gt; Context:\n        func(ctx)\n        return ctx\n\n    return FunctionStep(name, wrapper, next_step) \n</code></pre>"},{"location":"api_reference/#agents_sdk_models.create_simple_condition","title":"<code>create_simple_condition(field_path, expected_value)</code>","text":"<p>Create a simple condition function that checks a field value \u30d5\u30a3\u30fc\u30eb\u30c9\u5024\u3092\u30c1\u30a7\u30c3\u30af\u3059\u308b\u7c21\u5358\u306a\u6761\u4ef6\u95a2\u6570\u3092\u4f5c\u6210</p> <p>Parameters:</p> Name Type Description Default <code>field_path</code> <code>str</code> <p>Dot-separated path to field (e.g., \"shared_state.status\") / \u30d5\u30a3\u30fc\u30eb\u30c9\u3078\u306e\u30c9\u30c3\u30c8\u533a\u5207\u308a\u30d1\u30b9</p> required <code>expected_value</code> <code>Any</code> <p>Expected value / \u671f\u5f85\u5024</p> required <p>Returns:</p> Type Description <code>Callable[[Context], bool]</code> <p>Callable[[Context], bool]: Condition function / \u6761\u4ef6\u95a2\u6570</p> Source code in <code>src\\agents_sdk_models\\step.py</code> <pre><code>def create_simple_condition(field_path: str, expected_value: Any) -&gt; Callable[[Context], bool]:\n    \"\"\"\n    Create a simple condition function that checks a field value\n    \u30d5\u30a3\u30fc\u30eb\u30c9\u5024\u3092\u30c1\u30a7\u30c3\u30af\u3059\u308b\u7c21\u5358\u306a\u6761\u4ef6\u95a2\u6570\u3092\u4f5c\u6210\n\n    Args:\n        field_path: Dot-separated path to field (e.g., \"shared_state.status\") / \u30d5\u30a3\u30fc\u30eb\u30c9\u3078\u306e\u30c9\u30c3\u30c8\u533a\u5207\u308a\u30d1\u30b9\n        expected_value: Expected value / \u671f\u5f85\u5024\n\n    Returns:\n        Callable[[Context], bool]: Condition function / \u6761\u4ef6\u95a2\u6570\n    \"\"\"\n    def condition(ctx: Context) -&gt; bool:\n        try:\n            # Navigate to the field using dot notation\n            # \u30c9\u30c3\u30c8\u8a18\u6cd5\u3092\u4f7f\u7528\u3057\u3066\u30d5\u30a3\u30fc\u30eb\u30c9\u306b\u79fb\u52d5\n            obj = ctx\n            for part in field_path.split('.'):\n                if hasattr(obj, part):\n                    obj = getattr(obj, part)\n                elif isinstance(obj, dict) and part in obj:\n                    obj = obj[part]\n                else:\n                    return False\n            return obj == expected_value\n        except Exception:\n            return False\n\n    return condition\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.create_simple_flow","title":"<code>create_simple_flow(steps, context=None)</code>","text":"<p>Create a simple linear flow from a list of steps \u30b9\u30c6\u30c3\u30d7\u306e\u30ea\u30b9\u30c8\u304b\u3089\u7c21\u5358\u306a\u7dda\u5f62\u30d5\u30ed\u30fc\u3092\u4f5c\u6210</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>List[tuple[str, Step]]</code> <p>List of (name, step) tuples / (\u540d\u524d, \u30b9\u30c6\u30c3\u30d7)\u30bf\u30d7\u30eb\u306e\u30ea\u30b9\u30c8</p> required <code>context</code> <code>Optional[Context]</code> <p>Initial context / \u521d\u671f\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Flow</code> <code>Flow</code> <p>Created flow / \u4f5c\u6210\u3055\u308c\u305f\u30d5\u30ed\u30fc</p> Source code in <code>src\\agents_sdk_models\\flow.py</code> <pre><code>def create_simple_flow(\n    steps: List[tuple[str, Step]], \n    context: Optional[Context] = None\n) -&gt; Flow:\n    \"\"\"\n    Create a simple linear flow from a list of steps\n    \u30b9\u30c6\u30c3\u30d7\u306e\u30ea\u30b9\u30c8\u304b\u3089\u7c21\u5358\u306a\u7dda\u5f62\u30d5\u30ed\u30fc\u3092\u4f5c\u6210\n\n    Args:\n        steps: List of (name, step) tuples / (\u540d\u524d, \u30b9\u30c6\u30c3\u30d7)\u30bf\u30d7\u30eb\u306e\u30ea\u30b9\u30c8\n        context: Initial context / \u521d\u671f\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\n\n    Returns:\n        Flow: Created flow / \u4f5c\u6210\u3055\u308c\u305f\u30d5\u30ed\u30fc\n    \"\"\"\n    if not steps:\n        raise ValueError(\"At least one step is required\")\n\n    step_dict = {}\n    for i, (name, step) in enumerate(steps):\n        # Set next step for each step\n        # \u5404\u30b9\u30c6\u30c3\u30d7\u306e\u6b21\u30b9\u30c6\u30c3\u30d7\u3092\u8a2d\u5b9a\n        if hasattr(step, 'next_step') and step.next_step is None:\n            if i &lt; len(steps) - 1:\n                step.next_step = steps[i + 1][0]\n        step_dict[name] = step\n\n    return Flow(\n        start=steps[0][0],\n        steps=step_dict,\n        context=context\n    )\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.create_simple_gen_agent","title":"<code>create_simple_gen_agent(name, instructions, model=None, next_step=None, threshold=85, retries=3)</code>","text":"<p>Create a simple GenAgent with basic configuration \u57fa\u672c\u8a2d\u5b9a\u3067\u30b7\u30f3\u30d7\u30eb\u306aGenAgent\u3092\u4f5c\u6210</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Agent name / \u30a8\u30fc\u30b8\u30a7\u30f3\u30c8\u540d</p> required <code>instructions</code> <code>str</code> <p>Generation instructions / \u751f\u6210\u6307\u793a</p> required <code>model</code> <code>Optional[str]</code> <p>LLM model name / LLM\u30e2\u30c7\u30eb\u540d</p> <code>None</code> <code>next_step</code> <code>Optional[str]</code> <p>Next step after execution / \u5b9f\u884c\u5f8c\u306e\u6b21\u30b9\u30c6\u30c3\u30d7</p> <code>None</code> <code>threshold</code> <code>int</code> <p>Evaluation threshold / \u8a55\u4fa1\u95be\u5024</p> <code>85</code> <code>retries</code> <code>int</code> <p>Number of retries / \u30ea\u30c8\u30e9\u30a4\u56de\u6570</p> <code>3</code> <p>Returns:</p> Name Type Description <code>GenAgent</code> <code>GenAgent</code> <p>Configured GenAgent instance / \u8a2d\u5b9a\u6e08\u307fGenAgent\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9</p> Source code in <code>src\\agents_sdk_models\\gen_agent.py</code> <pre><code>def create_simple_gen_agent(\n    name: str,\n    instructions: str,\n    model: Optional[str] = None,\n    next_step: Optional[str] = None,\n    threshold: int = 85,\n    retries: int = 3\n) -&gt; GenAgent:\n    \"\"\"\n    Create a simple GenAgent with basic configuration\n    \u57fa\u672c\u8a2d\u5b9a\u3067\u30b7\u30f3\u30d7\u30eb\u306aGenAgent\u3092\u4f5c\u6210\n\n    Args:\n        name: Agent name / \u30a8\u30fc\u30b8\u30a7\u30f3\u30c8\u540d\n        instructions: Generation instructions / \u751f\u6210\u6307\u793a\n        model: LLM model name / LLM\u30e2\u30c7\u30eb\u540d\n        next_step: Next step after execution / \u5b9f\u884c\u5f8c\u306e\u6b21\u30b9\u30c6\u30c3\u30d7\n        threshold: Evaluation threshold / \u8a55\u4fa1\u95be\u5024\n        retries: Number of retries / \u30ea\u30c8\u30e9\u30a4\u56de\u6570\n\n    Returns:\n        GenAgent: Configured GenAgent instance / \u8a2d\u5b9a\u6e08\u307fGenAgent\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\n    \"\"\"\n    return GenAgent(\n        name=name,\n        generation_instructions=instructions,\n        evaluation_instructions=None,  # No evaluation for simple agent\n        model=model,\n        next_step=next_step,\n        threshold=threshold,\n        retries=retries\n    )\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.disable_tracing","title":"<code>disable_tracing()</code>","text":"<p>English: Disable all tracing. \u65e5\u672c\u8a9e: \u30c8\u30ec\u30fc\u30b7\u30f3\u30b0\u6a5f\u80fd\u3092\u3059\u3079\u3066\u7121\u52b9\u5316\u3057\u307e\u3059\u3002</p> Source code in <code>src\\agents_sdk_models\\tracing.py</code> <pre><code>def disable_tracing():\n    \"\"\"\n    English: Disable all tracing.\n    \u65e5\u672c\u8a9e: \u30c8\u30ec\u30fc\u30b7\u30f3\u30b0\u6a5f\u80fd\u3092\u3059\u3079\u3066\u7121\u52b9\u5316\u3057\u307e\u3059\u3002\n    \"\"\"\n    set_tracing_disabled(True)\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.enable_console_tracing","title":"<code>enable_console_tracing()</code>","text":"<p>English: Enable console tracing by registering ConsoleTracingProcessor and enabling tracing. \u65e5\u672c\u8a9e: ConsoleTracingProcessor\u3092\u767b\u9332\u3057\u3066\u30c8\u30ec\u30fc\u30b7\u30f3\u30b0\u3092\u6709\u52b9\u5316\u3057\u307e\u3059\u3002</p> Source code in <code>src\\agents_sdk_models\\tracing.py</code> <pre><code>def enable_console_tracing():\n    \"\"\"\n    English: Enable console tracing by registering ConsoleTracingProcessor and enabling tracing.\n    \u65e5\u672c\u8a9e: ConsoleTracingProcessor\u3092\u767b\u9332\u3057\u3066\u30c8\u30ec\u30fc\u30b7\u30f3\u30b0\u3092\u6709\u52b9\u5316\u3057\u307e\u3059\u3002\n    \"\"\"\n    # Enable tracing in Agents SDK\n    set_tracing_disabled(False)\n    # Register console tracing processor\n    set_trace_processors([ConsoleTracingProcessor()])\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.get_available_models","title":"<code>get_available_models(providers, ollama_base_url=None)</code>","text":"<p>Get available model names for specified providers (synchronous version).</p> <p>English: Get available model names for specified providers (synchronous version).</p> <p>\u65e5\u672c\u8a9e: \u6307\u5b9a\u3055\u308c\u305f\u30d7\u30ed\u30d0\u30a4\u30c0\u30fc\u306e\u5229\u7528\u53ef\u80fd\u306a\u30e2\u30c7\u30eb\u540d\u3092\u53d6\u5f97\u3057\u307e\u3059\uff08\u540c\u671f\u7248\uff09\u3002</p> <p>Parameters:</p> Name Type Description Default <code>providers</code> <code>List[ProviderType]</code> <p>List of providers to get models for. \u30e2\u30c7\u30eb\u3092\u53d6\u5f97\u3059\u308b\u30d7\u30ed\u30d0\u30a4\u30c0\u30fc\u306e\u30ea\u30b9\u30c8\u3002</p> required <code>ollama_base_url</code> <code>Optional[str]</code> <p>Base URL for Ollama API. If None, uses environment variable or default. Ollama API \u306e\u30d9\u30fc\u30b9 URL\u3002None \u306e\u5834\u5408\u3001\u74b0\u5883\u5909\u6570\u307e\u305f\u306f\u30c7\u30d5\u30a9\u30eb\u30c8\u3092\u4f7f\u7528\u3002</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, List[str]]</code> <p>dict[str, List[str]]: Dictionary mapping provider names to lists of available models.                  \u30d7\u30ed\u30d0\u30a4\u30c0\u30fc\u540d\u3068\u5229\u7528\u53ef\u80fd\u306a\u30e2\u30c7\u30eb\u306e\u30ea\u30b9\u30c8\u306e\u30de\u30c3\u30d4\u30f3\u30b0\u8f9e\u66f8\u3002</p> Source code in <code>src\\agents_sdk_models\\llm.py</code> <pre><code>def get_available_models(\n    providers: List[ProviderType],\n    ollama_base_url: Optional[str] = None\n) -&gt; dict[str, List[str]]:\n    \"\"\"\n    Get available model names for specified providers (synchronous version).\n\n    English:\n    Get available model names for specified providers (synchronous version).\n\n    \u65e5\u672c\u8a9e:\n    \u6307\u5b9a\u3055\u308c\u305f\u30d7\u30ed\u30d0\u30a4\u30c0\u30fc\u306e\u5229\u7528\u53ef\u80fd\u306a\u30e2\u30c7\u30eb\u540d\u3092\u53d6\u5f97\u3057\u307e\u3059\uff08\u540c\u671f\u7248\uff09\u3002\n\n    Args:\n        providers (List[ProviderType]): List of providers to get models for.\n            \u30e2\u30c7\u30eb\u3092\u53d6\u5f97\u3059\u308b\u30d7\u30ed\u30d0\u30a4\u30c0\u30fc\u306e\u30ea\u30b9\u30c8\u3002\n        ollama_base_url (Optional[str]): Base URL for Ollama API. If None, uses environment variable or default.\n            Ollama API \u306e\u30d9\u30fc\u30b9 URL\u3002None \u306e\u5834\u5408\u3001\u74b0\u5883\u5909\u6570\u307e\u305f\u306f\u30c7\u30d5\u30a9\u30eb\u30c8\u3092\u4f7f\u7528\u3002\n\n    Returns:\n        dict[str, List[str]]: Dictionary mapping provider names to lists of available models.\n                             \u30d7\u30ed\u30d0\u30a4\u30c0\u30fc\u540d\u3068\u5229\u7528\u53ef\u80fd\u306a\u30e2\u30c7\u30eb\u306e\u30ea\u30b9\u30c8\u306e\u30de\u30c3\u30d4\u30f3\u30b0\u8f9e\u66f8\u3002\n    \"\"\"\n    try:\n        # English: Try to get the current event loop\n        # \u65e5\u672c\u8a9e: \u73fe\u5728\u306e\u30a4\u30d9\u30f3\u30c8\u30eb\u30fc\u30d7\u3092\u53d6\u5f97\u3057\u3088\u3046\u3068\u3059\u308b\n        loop = asyncio.get_running_loop()\n        # English: If we're in a running loop, we need to handle this differently\n        # \u65e5\u672c\u8a9e: \u5b9f\u884c\u4e2d\u306e\u30eb\u30fc\u30d7\u5185\u306b\u3044\u308b\u5834\u5408\u3001\u7570\u306a\u308b\u65b9\u6cd5\u3067\u51e6\u7406\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\n        import concurrent.futures\n        with concurrent.futures.ThreadPoolExecutor() as executor:\n            future = executor.submit(asyncio.run, get_available_models_async(providers, ollama_base_url))\n            return future.result()\n    except RuntimeError:\n        # English: No running event loop, safe to use asyncio.run()\n        # \u65e5\u672c\u8a9e: \u5b9f\u884c\u4e2d\u306e\u30a4\u30d9\u30f3\u30c8\u30eb\u30fc\u30d7\u304c\u306a\u3044\u5834\u5408\u3001asyncio.run() \u3092\u5b89\u5168\u306b\u4f7f\u7528\n        return asyncio.run(get_available_models_async(providers, ollama_base_url)) \n</code></pre>"},{"location":"api_reference/#agents_sdk_models.get_available_models_async","title":"<code>get_available_models_async(providers, ollama_base_url=None)</code>  <code>async</code>","text":"<p>Get available model names for specified providers.</p> <p>English: Get available model names for specified providers.</p> <p>\u65e5\u672c\u8a9e: \u6307\u5b9a\u3055\u308c\u305f\u30d7\u30ed\u30d0\u30a4\u30c0\u30fc\u306e\u5229\u7528\u53ef\u80fd\u306a\u30e2\u30c7\u30eb\u540d\u3092\u53d6\u5f97\u3057\u307e\u3059\u3002</p> <p>Parameters:</p> Name Type Description Default <code>providers</code> <code>List[ProviderType]</code> <p>List of providers to get models for. \u30e2\u30c7\u30eb\u3092\u53d6\u5f97\u3059\u308b\u30d7\u30ed\u30d0\u30a4\u30c0\u30fc\u306e\u30ea\u30b9\u30c8\u3002</p> required <code>ollama_base_url</code> <code>Optional[str]</code> <p>Base URL for Ollama API. If None, uses environment variable or default. Ollama API \u306e\u30d9\u30fc\u30b9 URL\u3002None \u306e\u5834\u5408\u3001\u74b0\u5883\u5909\u6570\u307e\u305f\u306f\u30c7\u30d5\u30a9\u30eb\u30c8\u3092\u4f7f\u7528\u3002</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, List[str]]</code> <p>dict[str, List[str]]: Dictionary mapping provider names to lists of available models.                  \u30d7\u30ed\u30d0\u30a4\u30c0\u30fc\u540d\u3068\u5229\u7528\u53ef\u80fd\u306a\u30e2\u30c7\u30eb\u306e\u30ea\u30b9\u30c8\u306e\u30de\u30c3\u30d4\u30f3\u30b0\u8f9e\u66f8\u3002</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an unsupported provider is specified.         \u30b5\u30dd\u30fc\u30c8\u3055\u308c\u3066\u3044\u306a\u3044\u30d7\u30ed\u30d0\u30a4\u30c0\u30fc\u304c\u6307\u5b9a\u3055\u308c\u305f\u5834\u5408\u3002</p> <code>RequestError</code> <p>If there's an error connecting to the Ollama API.                Ollama API \u3078\u306e\u63a5\u7d9a\u30a8\u30e9\u30fc\u304c\u767a\u751f\u3057\u305f\u5834\u5408\u3002</p> Source code in <code>src\\agents_sdk_models\\llm.py</code> <pre><code>async def get_available_models_async(\n    providers: List[ProviderType],\n    ollama_base_url: Optional[str] = None\n) -&gt; dict[str, List[str]]:\n    \"\"\"\n    Get available model names for specified providers.\n\n    English:\n    Get available model names for specified providers.\n\n    \u65e5\u672c\u8a9e:\n    \u6307\u5b9a\u3055\u308c\u305f\u30d7\u30ed\u30d0\u30a4\u30c0\u30fc\u306e\u5229\u7528\u53ef\u80fd\u306a\u30e2\u30c7\u30eb\u540d\u3092\u53d6\u5f97\u3057\u307e\u3059\u3002\n\n    Args:\n        providers (List[ProviderType]): List of providers to get models for.\n            \u30e2\u30c7\u30eb\u3092\u53d6\u5f97\u3059\u308b\u30d7\u30ed\u30d0\u30a4\u30c0\u30fc\u306e\u30ea\u30b9\u30c8\u3002\n        ollama_base_url (Optional[str]): Base URL for Ollama API. If None, uses environment variable or default.\n            Ollama API \u306e\u30d9\u30fc\u30b9 URL\u3002None \u306e\u5834\u5408\u3001\u74b0\u5883\u5909\u6570\u307e\u305f\u306f\u30c7\u30d5\u30a9\u30eb\u30c8\u3092\u4f7f\u7528\u3002\n\n    Returns:\n        dict[str, List[str]]: Dictionary mapping provider names to lists of available models.\n                             \u30d7\u30ed\u30d0\u30a4\u30c0\u30fc\u540d\u3068\u5229\u7528\u53ef\u80fd\u306a\u30e2\u30c7\u30eb\u306e\u30ea\u30b9\u30c8\u306e\u30de\u30c3\u30d4\u30f3\u30b0\u8f9e\u66f8\u3002\n\n    Raises:\n        ValueError: If an unsupported provider is specified.\n                    \u30b5\u30dd\u30fc\u30c8\u3055\u308c\u3066\u3044\u306a\u3044\u30d7\u30ed\u30d0\u30a4\u30c0\u30fc\u304c\u6307\u5b9a\u3055\u308c\u305f\u5834\u5408\u3002\n        httpx.RequestError: If there's an error connecting to the Ollama API.\n                           Ollama API \u3078\u306e\u63a5\u7d9a\u30a8\u30e9\u30fc\u304c\u767a\u751f\u3057\u305f\u5834\u5408\u3002\n    \"\"\"\n    result = {}\n\n    for provider in providers:\n        if provider == \"openai\":\n            # English: OpenAI models - latest available models\n            # \u65e5\u672c\u8a9e: OpenAI \u30e2\u30c7\u30eb - \u6700\u65b0\u306e\u5229\u7528\u53ef\u80fd\u306a\u30e2\u30c7\u30eb\n            result[\"openai\"] = [\n                \"gpt-4o\",\n                \"gpt-4o-mini\",\n                \"gpt-4.1\",\n                \"o3\",\n                \"o4-mini\"\n            ]\n        elif provider == \"google\":\n            # English: Google Gemini models - latest 2.5 series models\n            # \u65e5\u672c\u8a9e: Google Gemini \u30e2\u30c7\u30eb - \u6700\u65b0\u306e 2.5 \u30b7\u30ea\u30fc\u30ba\u30e2\u30c7\u30eb\n            result[\"google\"] = [\n                \"gemini-2.5-pro\",\n                \"gemini-2.5-flash\"\n            ]\n        elif provider == \"anthropic\":\n            # English: Anthropic Claude models - latest Claude-4 series models\n            # \u65e5\u672c\u8a9e: Anthropic Claude \u30e2\u30c7\u30eb - \u6700\u65b0\u306e Claude-4 \u30b7\u30ea\u30fc\u30ba\u30e2\u30c7\u30eb\n            result[\"anthropic\"] = [\n                \"claude-opus-4\",\n                \"claude-sonnet-4\"\n            ]\n        elif provider == \"ollama\":\n            # English: Get Ollama base URL from parameter, environment variable, or default\n            # \u65e5\u672c\u8a9e: \u30d1\u30e9\u30e1\u30fc\u30bf\u3001\u74b0\u5883\u5909\u6570\u3001\u307e\u305f\u306f\u30c7\u30d5\u30a9\u30eb\u30c8\u304b\u3089 Ollama \u30d9\u30fc\u30b9 URL \u3092\u53d6\u5f97\n            if ollama_base_url is None:\n                ollama_base_url = os.environ.get(\"OLLAMA_BASE_URL\", \"http://localhost:11434\")\n\n            try:\n                # English: Fetch available models from Ollama API\n                # \u65e5\u672c\u8a9e: Ollama API \u304b\u3089\u5229\u7528\u53ef\u80fd\u306a\u30e2\u30c7\u30eb\u3092\u53d6\u5f97\n                async with httpx.AsyncClient() as client:\n                    response = await client.get(f\"{ollama_base_url}/api/tags\")\n                    response.raise_for_status()\n\n                    # English: Parse the response to extract model names\n                    # \u65e5\u672c\u8a9e: \u30ec\u30b9\u30dd\u30f3\u30b9\u3092\u89e3\u6790\u3057\u3066\u30e2\u30c7\u30eb\u540d\u3092\u62bd\u51fa\n                    data = response.json()\n                    models = []\n                    if \"models\" in data:\n                        for model_info in data[\"models\"]:\n                            if \"name\" in model_info:\n                                models.append(model_info[\"name\"])\n\n                    result[\"ollama\"] = models\n\n            except httpx.RequestError as e:\n                # English: If connection fails, return empty list with error info\n                # \u65e5\u672c\u8a9e: \u63a5\u7d9a\u306b\u5931\u6557\u3057\u305f\u5834\u5408\u3001\u30a8\u30e9\u30fc\u60c5\u5831\u3068\u5171\u306b\u7a7a\u306e\u30ea\u30b9\u30c8\u3092\u8fd4\u3059\n                result[\"ollama\"] = []\n                print(f\"Warning: Could not connect to Ollama at {ollama_base_url}: {e}\")\n            except Exception as e:\n                # English: Handle other errors\n                # \u65e5\u672c\u8a9e: \u305d\u306e\u4ed6\u306e\u30a8\u30e9\u30fc\u3092\u51e6\u7406\n                result[\"ollama\"] = []\n                print(f\"Warning: Error fetching Ollama models: {e}\")\n        else:\n            raise ValueError(f\"Unsupported provider: {provider}. Must be one of {ProviderType.__args__}\")\n\n    return result\n</code></pre>"},{"location":"api_reference/#agents_sdk_models.get_llm","title":"<code>get_llm(model=None, provider=None, temperature=0.3, api_key=None, base_url=None, thinking=False, **kwargs)</code>","text":"<p>Factory function to get an instance of a language model based on the provider.</p> <p>English: Factory function to get an instance of a language model based on the provider.</p> <p>\u65e5\u672c\u8a9e: \u30d7\u30ed\u30d0\u30a4\u30c0\u30fc\u306b\u57fa\u3065\u3044\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u306e\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u3092\u53d6\u5f97\u3059\u308b\u30d5\u30a1\u30af\u30c8\u30ea\u95a2\u6570\u3002</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>ProviderType</code> <p>The LLM provider (\"openai\", \"google\", \"anthropic\", \"ollama\"). Defaults to \"openai\". LLM \u30d7\u30ed\u30d0\u30a4\u30c0\u30fc (\"openai\", \"google\", \"anthropic\", \"ollama\")\u3002\u30c7\u30d5\u30a9\u30eb\u30c8\u306f \"openai\"\u3002</p> <code>None</code> <code>model</code> <code>Optional[str]</code> <p>The specific model name for the provider. If None, uses the default for the provider. \u30d7\u30ed\u30d0\u30a4\u30c0\u30fc\u56fa\u6709\u306e\u30e2\u30c7\u30eb\u540d\u3002None \u306e\u5834\u5408\u3001\u30d7\u30ed\u30d0\u30a4\u30c0\u30fc\u306e\u30c7\u30d5\u30a9\u30eb\u30c8\u3092\u4f7f\u7528\u3057\u307e\u3059\u3002</p> <code>None</code> <code>temperature</code> <code>float</code> <p>Sampling temperature. Defaults to 0.3. \u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u6e29\u5ea6\u3002\u30c7\u30d5\u30a9\u30eb\u30c8\u306f 0.3\u3002</p> <code>0.3</code> <code>api_key</code> <code>Optional[str]</code> <p>API key for the provider, if required. \u30d7\u30ed\u30d0\u30a4\u30c0\u30fc\u306e API \u30ad\u30fc (\u5fc5\u8981\u306a\u5834\u5408)\u3002</p> <code>None</code> <code>base_url</code> <code>Optional[str]</code> <p>Base URL for the provider's API, if needed (e.g., for self-hosted Ollama or OpenAI-compatible APIs). \u30d7\u30ed\u30d0\u30a4\u30c0\u30fc API \u306e\u30d9\u30fc\u30b9 URL (\u5fc5\u8981\u306a\u5834\u5408\u3001\u4f8b: \u30bb\u30eb\u30d5\u30db\u30b9\u30c8\u306e Ollama \u3084 OpenAI \u4e92\u63db API)\u3002</p> <code>None</code> <code>thinking</code> <code>bool</code> <p>Enable thinking mode for Claude models. Defaults to False. Claude \u30e2\u30c7\u30eb\u306e\u601d\u8003\u30e2\u30fc\u30c9\u3092\u6709\u52b9\u306b\u3059\u308b\u304b\u3002\u30c7\u30d5\u30a9\u30eb\u30c8\u306f False\u3002</p> <code>False</code> <code>tracing</code> <code>bool</code> <p>Whether to enable tracing for the Agents SDK. Defaults to False. Agents SDK \u306e\u30c8\u30ec\u30fc\u30b7\u30f3\u30b0\u3092\u6709\u52b9\u5316\u3059\u308b\u304b\u3002\u30c7\u30d5\u30a9\u30eb\u30c8\u306f False\u3002</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the model constructor. \u30e2\u30c7\u30eb\u306e\u30b3\u30f3\u30b9\u30c8\u30e9\u30af\u30bf\u306b\u6e21\u3059\u8ffd\u52a0\u306e\u30ad\u30fc\u30ef\u30fc\u30c9\u5f15\u6570\u3002</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Model</code> <code>Model</code> <p>An instance of the appropriate language model class.    \u9069\u5207\u306a\u8a00\u8a9e\u30e2\u30c7\u30eb\u30af\u30e9\u30b9\u306e\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u3002</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an unsupported provider is specified.         \u30b5\u30dd\u30fc\u30c8\u3055\u308c\u3066\u3044\u306a\u3044\u30d7\u30ed\u30d0\u30a4\u30c0\u30fc\u304c\u6307\u5b9a\u3055\u308c\u305f\u5834\u5408\u3002</p> Source code in <code>src\\agents_sdk_models\\llm.py</code> <pre><code>def get_llm(\n    model: Optional[str] = None,\n    provider: Optional[ProviderType] = None,\n    temperature: float = 0.3,\n    api_key: Optional[str] = None,\n    base_url: Optional[str] = None,\n    thinking: bool = False,\n    **kwargs: Any,\n) -&gt; Model:\n    \"\"\"\n    Factory function to get an instance of a language model based on the provider.\n\n    English:\n    Factory function to get an instance of a language model based on the provider.\n\n    \u65e5\u672c\u8a9e:\n    \u30d7\u30ed\u30d0\u30a4\u30c0\u30fc\u306b\u57fa\u3065\u3044\u3066\u8a00\u8a9e\u30e2\u30c7\u30eb\u306e\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u3092\u53d6\u5f97\u3059\u308b\u30d5\u30a1\u30af\u30c8\u30ea\u95a2\u6570\u3002\n\n    Args:\n        provider (ProviderType): The LLM provider (\"openai\", \"google\", \"anthropic\", \"ollama\"). Defaults to \"openai\".\n            LLM \u30d7\u30ed\u30d0\u30a4\u30c0\u30fc (\"openai\", \"google\", \"anthropic\", \"ollama\")\u3002\u30c7\u30d5\u30a9\u30eb\u30c8\u306f \"openai\"\u3002\n        model (Optional[str]): The specific model name for the provider. If None, uses the default for the provider.\n            \u30d7\u30ed\u30d0\u30a4\u30c0\u30fc\u56fa\u6709\u306e\u30e2\u30c7\u30eb\u540d\u3002None \u306e\u5834\u5408\u3001\u30d7\u30ed\u30d0\u30a4\u30c0\u30fc\u306e\u30c7\u30d5\u30a9\u30eb\u30c8\u3092\u4f7f\u7528\u3057\u307e\u3059\u3002\n        temperature (float): Sampling temperature. Defaults to 0.3.\n            \u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u6e29\u5ea6\u3002\u30c7\u30d5\u30a9\u30eb\u30c8\u306f 0.3\u3002\n        api_key (Optional[str]): API key for the provider, if required.\n            \u30d7\u30ed\u30d0\u30a4\u30c0\u30fc\u306e API \u30ad\u30fc (\u5fc5\u8981\u306a\u5834\u5408)\u3002\n        base_url (Optional[str]): Base URL for the provider's API, if needed (e.g., for self-hosted Ollama or OpenAI-compatible APIs).\n            \u30d7\u30ed\u30d0\u30a4\u30c0\u30fc API \u306e\u30d9\u30fc\u30b9 URL (\u5fc5\u8981\u306a\u5834\u5408\u3001\u4f8b: \u30bb\u30eb\u30d5\u30db\u30b9\u30c8\u306e Ollama \u3084 OpenAI \u4e92\u63db API)\u3002\n        thinking (bool): Enable thinking mode for Claude models. Defaults to False.\n            Claude \u30e2\u30c7\u30eb\u306e\u601d\u8003\u30e2\u30fc\u30c9\u3092\u6709\u52b9\u306b\u3059\u308b\u304b\u3002\u30c7\u30d5\u30a9\u30eb\u30c8\u306f False\u3002\n        tracing (bool): Whether to enable tracing for the Agents SDK. Defaults to False.\n            Agents SDK \u306e\u30c8\u30ec\u30fc\u30b7\u30f3\u30b0\u3092\u6709\u52b9\u5316\u3059\u308b\u304b\u3002\u30c7\u30d5\u30a9\u30eb\u30c8\u306f False\u3002\n        **kwargs (Any): Additional keyword arguments to pass to the model constructor.\n            \u30e2\u30c7\u30eb\u306e\u30b3\u30f3\u30b9\u30c8\u30e9\u30af\u30bf\u306b\u6e21\u3059\u8ffd\u52a0\u306e\u30ad\u30fc\u30ef\u30fc\u30c9\u5f15\u6570\u3002\n\n    Returns:\n        Model: An instance of the appropriate language model class.\n               \u9069\u5207\u306a\u8a00\u8a9e\u30e2\u30c7\u30eb\u30af\u30e9\u30b9\u306e\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u3002\n\n    Raises:\n        ValueError: If an unsupported provider is specified.\n                    \u30b5\u30dd\u30fc\u30c8\u3055\u308c\u3066\u3044\u306a\u3044\u30d7\u30ed\u30d0\u30a4\u30c0\u30fc\u304c\u6307\u5b9a\u3055\u308c\u305f\u5834\u5408\u3002\n    \"\"\"\n    # English: Configure OpenAI Agents SDK tracing\n    # \u65e5\u672c\u8a9e: OpenAI Agents SDK \u306e\u30c8\u30ec\u30fc\u30b7\u30f3\u30b0\u3092\u8a2d\u5b9a\u3059\u308b\n    # set_tracing_disabled(not tracing)\n\n\n    if model is None:\n        model = os.environ.get(\"LLM_MODEL\", \"gpt-4o-mini\")\n\n    def get_provider_canditate(model: str) -&gt; ProviderType:\n        if \"gpt\" in model:\n            return \"openai\"\n        if \"o3\" in model or \"o4\" in model:\n            return \"openai\"\n        elif \"gemini\" in model:\n            return \"google\"\n        elif \"claude\" in model:\n            return \"anthropic\"\n        else:\n            return \"ollama\"\n\n    if provider is None:\n        provider = get_provider_canditate(model)\n\n    if provider == \"openai\":\n        # Use the standard OpenAI model from the agents library\n        # agents\u30e9\u30a4\u30d6\u30e9\u30ea\u306e\u6a19\u6e96 OpenAI \u30e2\u30c7\u30eb\u3092\u4f7f\u7528\n        openai_kwargs = kwargs.copy()\n\n        # English: Prepare arguments for OpenAI client and model\n        # \u65e5\u672c\u8a9e: OpenAI \u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u3068\u30e2\u30c7\u30eb\u306e\u5f15\u6570\u3092\u6e96\u5099\n        client_args = {}\n        model_args = {}\n\n        # English: Set API key for client\n        # \u65e5\u672c\u8a9e: \u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u306b API \u30ad\u30fc\u3092\u8a2d\u5b9a\n        if api_key:\n            client_args['api_key'] = api_key\n        # English: Set base URL for client\n        # \u65e5\u672c\u8a9e: \u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u306b\u30d9\u30fc\u30b9 URL \u3092\u8a2d\u5b9a\n        if base_url:\n            client_args['base_url'] = base_url\n\n        # English: Set model name for model constructor\n        # \u65e5\u672c\u8a9e: \u30e2\u30c7\u30eb\u30b3\u30f3\u30b9\u30c8\u30e9\u30af\u30bf\u306b\u30e2\u30c7\u30eb\u540d\u3092\u8a2d\u5b9a\n        model_args['model'] = model if model else \"gpt-4o-mini\" # Default to gpt-4o-mini\n\n        # English: Temperature is likely handled by the runner or set post-init,\n        # English: so remove it from constructor args.\n        # \u65e5\u672c\u8a9e: temperature \u306f\u30e9\u30f3\u30ca\u30fc\u306b\u3088\u3063\u3066\u51e6\u7406\u3055\u308c\u308b\u304b\u3001\u521d\u671f\u5316\u5f8c\u306b\u8a2d\u5b9a\u3055\u308c\u308b\u53ef\u80fd\u6027\u304c\u9ad8\u3044\u305f\u3081\u3001\n        # \u65e5\u672c\u8a9e: \u30b3\u30f3\u30b9\u30c8\u30e9\u30af\u30bf\u5f15\u6570\u304b\u3089\u524a\u9664\u3057\u307e\u3059\u3002\n        # model_args['temperature'] = temperature # Removed based on TypeError\n\n        # English: Add any other relevant kwargs passed in, EXCLUDING temperature\n        # \u65e5\u672c\u8a9e: \u6e21\u3055\u308c\u305f\u4ed6\u306e\u95a2\u9023\u3059\u308b kwargs \u3092\u8ffd\u52a0 (temperature \u3092\u9664\u304f)\n        # Example: max_tokens, etc. Filter out args meant for the client.\n        # \u4f8b: max_tokens \u306a\u3069\u3002\u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u5411\u3051\u306e\u5f15\u6570\u3092\u9664\u5916\u3057\u307e\u3059\u3002\n        for key, value in kwargs.items():\n            # English: Exclude client args, thinking, temperature, and tracing\n            # \u65e5\u672c\u8a9e: \u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u5f15\u6570\u3001thinking\u3001temperature\u3001tracing \u3092\u9664\u5916\n            if key not in ['api_key', 'base_url', 'thinking', 'temperature', 'tracing']:\n                model_args[key] = value\n\n        # English: Remove 'thinking' as it's not used by OpenAI model\n        # \u65e5\u672c\u8a9e: OpenAI \u30e2\u30c7\u30eb\u3067\u306f\u4f7f\u7528\u3055\u308c\u306a\u3044\u305f\u3081 'thinking' \u3092\u524a\u9664\n        model_args.pop('thinking', None)\n\n        # English: Instantiate the OpenAI client\n        # \u65e5\u672c\u8a9e: OpenAI \u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u3092\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u5316\n        openai_client = AsyncOpenAI(**client_args)\n\n        # English: Instantiate and return the model, passing the client and model args\n        # \u65e5\u672c\u8a9e: \u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u3068\u30e2\u30c7\u30eb\u5f15\u6570\u3092\u6e21\u3057\u3066\u30e2\u30c7\u30eb\u3092\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u5316\u3057\u3066\u8fd4\u3059\n        return OpenAIResponsesModel(\n            openai_client=openai_client,\n            **model_args\n        )\n    elif provider == \"google\":\n        gemini_kwargs = kwargs.copy()\n        if model:\n            gemini_kwargs['model'] = model\n        # thinking is not used by GeminiModel\n        gemini_kwargs.pop('thinking', None)\n        return GeminiModel(\n            temperature=temperature,\n            api_key=api_key,\n            base_url=base_url, # Although Gemini doesn't typically use base_url, pass it if provided\n            **gemini_kwargs\n        )\n    elif provider == \"anthropic\":\n        claude_kwargs = kwargs.copy()\n        if model:\n            claude_kwargs['model'] = model\n        return ClaudeModel(\n            temperature=temperature,\n            api_key=api_key,\n            base_url=base_url, # Although Claude doesn't typically use base_url, pass it if provided\n            thinking=thinking,\n            **claude_kwargs\n        )\n    elif provider == \"ollama\":\n        ollama_kwargs = kwargs.copy()\n        if model:\n            ollama_kwargs['model'] = model\n        # thinking is not used by OllamaModel\n        ollama_kwargs.pop('thinking', None)\n        return OllamaModel(\n            temperature=temperature,\n            base_url=base_url,\n            api_key=api_key, # Although Ollama doesn't typically use api_key, pass it if provided\n            **ollama_kwargs\n        )\n    else:\n        raise ValueError(f\"Unsupported provider: {provider}. Must be one of {ProviderType.__args__}\") \n</code></pre>"},{"location":"api_reference/#_1","title":"\u30af\u30e9\u30b9\u30fb\u95a2\u6570\u4e00\u89a7","text":"\u540d\u524d \u7a2e\u5225 \u6982\u8981 get_llm \u95a2\u6570 \u30e2\u30c7\u30eb\u540d\u30fb\u30d7\u30ed\u30d0\u30a4\u30c0\u30fc\u540d\u304b\u3089LLM\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u3092\u53d6\u5f97 AgentPipeline \u30af\u30e9\u30b9 \u751f\u6210\u30fb\u8a55\u4fa1\u30fb\u30c4\u30fc\u30eb\u30fb\u30ac\u30fc\u30c9\u30ec\u30fc\u30eb\u3092\u7d71\u5408\u3057\u305f\u30d1\u30a4\u30d7\u30e9\u30a4\u30f3 ConsoleTracingProcessor \u30af\u30e9\u30b9 \u30b3\u30f3\u30bd\u30fc\u30eb\u8272\u5206\u3051\u30c8\u30ec\u30fc\u30b9\u51fa\u529b\u7528\u30d7\u30ed\u30bb\u30c3\u30b5 enable_console_tracing \u95a2\u6570 \u30b3\u30f3\u30bd\u30fc\u30eb\u30c8\u30ec\u30fc\u30b7\u30f3\u30b0\u3092\u6709\u52b9\u5316 disable_tracing \u95a2\u6570 \u30c8\u30ec\u30fc\u30b7\u30f3\u30b0\u6a5f\u80fd\u3092\u3059\u3079\u3066\u7121\u52b9\u5316 OpenAIResponsesModel \u30af\u30e9\u30b9 OpenAI\u7528\u30e2\u30c7\u30eb\u30e9\u30c3\u30d1\u30fc GeminiModel \u30af\u30e9\u30b9 Google Gemini\u7528\u30e2\u30c7\u30eb\u30e9\u30c3\u30d1\u30fc ClaudeModel \u30af\u30e9\u30b9 Anthropic Claude\u7528\u30e2\u30c7\u30eb\u30e9\u30c3\u30d1\u30fc OllamaModel \u30af\u30e9\u30b9 Ollama\u7528\u30e2\u30c7\u30eb\u30e9\u30c3\u30d1\u30fc"},{"location":"api_reference/#get_llm","title":"get_llm","text":"<ul> <li> <p>\u30e2\u30c7\u30eb\u540d\u30fb\u30d7\u30ed\u30d0\u30a4\u30c0\u30fc\u540d\u304b\u3089LLM\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u3092\u8fd4\u3059\u30d5\u30a1\u30af\u30c8\u30ea\u95a2\u6570</p> </li> <li> <p>\u5f15\u6570:</p> <ul> <li>model (str): \u30e2\u30c7\u30eb\u540d</li> <li>provider (str, optional): \u30d7\u30ed\u30d0\u30a4\u30c0\u30fc\u540d\uff08\u7701\u7565\u6642\u306f\u81ea\u52d5\u63a8\u8ad6\uff09</li> <li>temperature (float, optional): \u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u6e29\u5ea6</li> <li>api_key (str, optional): \u30d7\u30ed\u30d0\u30a4\u30c0\u30fcAPI\u30ad\u30fc</li> <li>base_url (str, optional): \u30d7\u30ed\u30d0\u30a4\u30c0\u30fcAPI\u30d9\u30fc\u30b9URL</li> <li>thinking (bool, optional): Claude\u30e2\u30c7\u30eb\u601d\u8003\u30e2\u30fc\u30c9</li> <li>tracing (bool, optional): Agents SDK \u30c8\u30ec\u30fc\u30b7\u30f3\u30b0\u6709\u52b9\u5316</li> </ul> </li> <li>\u623b\u308a\u5024: LLM\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9</li> </ul>"},{"location":"api_reference/#_2","title":"\u5f15\u6570","text":"\u540d\u524d \u578b \u5fc5\u9808/\u30aa\u30d7\u30b7\u30e7\u30f3 \u30c7\u30d5\u30a9\u30eb\u30c8 \u8aac\u660e model str \u5fc5\u9808 - \u4f7f\u7528\u3059\u308bLLM\u30e2\u30c7\u30eb\u540d provider str (optional) \u30aa\u30d7\u30b7\u30e7\u30f3 None \u30e2\u30c7\u30eb\u306e\u30d7\u30ed\u30d0\u30a4\u30c0\u30fc\u540d\uff08\u81ea\u52d5\u63a8\u8ad6\u53ef\uff09 temperature float (optional) \u30aa\u30d7\u30b7\u30e7\u30f3 0.3 \u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u6e29\u5ea6 api_key str (optional) \u30aa\u30d7\u30b7\u30e7\u30f3 None \u30d7\u30ed\u30d0\u30a4\u30c0\u30fcAPI\u30ad\u30fc base_url str (optional) \u30aa\u30d7\u30b7\u30e7\u30f3 None \u30d7\u30ed\u30d0\u30a4\u30c0\u30fcAPI\u30d9\u30fc\u30b9URL thinking bool (optional) \u30aa\u30d7\u30b7\u30e7\u30f3 False Claude\u30e2\u30c7\u30eb\u306e\u601d\u8003\u30e2\u30fc\u30c9 tracing bool (optional) \u30aa\u30d7\u30b7\u30e7\u30f3 False Agents SDK\u306e\u30c8\u30ec\u30fc\u30b7\u30f3\u30b0\u3092\u6709\u52b9\u5316\u3059\u308b\u304b"},{"location":"api_reference/#_3","title":"\u623b\u308a\u5024","text":"<p><code>LLM\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9</code></p>"},{"location":"api_reference/#enable_console_tracing","title":"enable_console_tracing","text":"<ul> <li>\u30b3\u30f3\u30bd\u30fc\u30eb\u30c8\u30ec\u30fc\u30b7\u30f3\u30b0\uff08<code>ConsoleTracingProcessor</code>\uff09\u3092\u6709\u52b9\u5316\u3057\u307e\u3059\u3002</li> <li>\u5f15\u6570: \u306a\u3057</li> <li>\u623b\u308a\u5024: \u306a\u3057</li> </ul>"},{"location":"api_reference/#disable_tracing","title":"disable_tracing","text":"<ul> <li>\u5168\u3066\u306e\u30c8\u30ec\u30fc\u30b7\u30f3\u30b0\u6a5f\u80fd\uff08SDK\u304a\u3088\u3073\u30b3\u30f3\u30bd\u30fc\u30eb\uff09\u3092\u7121\u52b9\u5316\u3057\u307e\u3059\u3002</li> <li>\u5f15\u6570: \u306a\u3057</li> <li>\u623b\u308a\u5024: \u306a\u3057</li> </ul>"},{"location":"api_reference/#agentpipeline","title":"AgentPipeline","text":"<ul> <li>\u751f\u6210\u30fb\u8a55\u4fa1\u30fb\u30c4\u30fc\u30eb\u30fb\u30ac\u30fc\u30c9\u30ec\u30fc\u30eb\u3092\u7d71\u5408\u3057\u305f\u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u7ba1\u7406\u30af\u30e9\u30b9</li> <li>\u4e3b\u306a\u5f15\u6570:<ul> <li>name (str): \u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u540d</li> <li>generation_instructions (str): \u751f\u6210\u7528\u30d7\u30ed\u30f3\u30d7\u30c8</li> <li>evaluation_instructions (str, optional): \u8a55\u4fa1\u7528\u30d7\u30ed\u30f3\u30d7\u30c8</li> <li>model (str or LLM): \u4f7f\u7528\u3059\u308b\u30e2\u30c7\u30eb</li> <li>evaluation_model (str or LLM, optional): \u8a55\u4fa1\u306b\u4f7f\u7528\u3059\u308b\u30e2\u30c7\u30eb\u540d\u307e\u305f\u306fLLM\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\uff08\u7701\u7565\u6642\u306f<code>model</code>\u3092\u4f7f\u7528\uff09</li> <li>generation_tools (list, optional): \u751f\u6210\u6642\u30c4\u30fc\u30eb</li> <li>input_guardrails/output_guardrails (list, optional): \u5165\u51fa\u529b\u30ac\u30fc\u30c9\u30ec\u30fc\u30eb</li> <li>threshold (int): \u8a55\u4fa1\u95be\u5024</li> <li>retries (int): \u30ea\u30c8\u30e9\u30a4\u56de\u6570</li> <li>retry_comment_importance (list[str], optional): \u91cd\u8981\u5ea6\u6307\u5b9a</li> </ul> </li> <li>\u4e3b\u306a\u30e1\u30bd\u30c3\u30c9:<ul> <li>run(input): \u5165\u529b\u306b\u5bfe\u3057\u3066\u751f\u6210\u30fb\u8a55\u4fa1\u30fb\u81ea\u5df1\u6539\u5584\u3092\u5b9f\u884c</li> </ul> </li> <li>\u623b\u308a\u5024: \u751f\u6210\u30fb\u8a55\u4fa1\u7d50\u679c</li> </ul>"},{"location":"api_reference/#_4","title":"\u5f15\u6570","text":"\u540d\u524d \u578b \u5fc5\u9808/\u30aa\u30d7\u30b7\u30e7\u30f3 \u30c7\u30d5\u30a9\u30eb\u30c8 \u8aac\u660e name str \u5fc5\u9808 - \u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u540d generation_instructions str \u5fc5\u9808 - \u751f\u6210\u7528\u30b7\u30b9\u30c6\u30e0\u30d7\u30ed\u30f3\u30d7\u30c8 evaluation_instructions str (optional) \u30aa\u30d7\u30b7\u30e7\u30f3 None \u8a55\u4fa1\u7528\u30b7\u30b9\u30c6\u30e0\u30d7\u30ed\u30f3\u30d7\u30c8 model str or LLM \u30aa\u30d7\u30b7\u30e7\u30f3 None \u4f7f\u7528\u3059\u308bLLM\u30e2\u30c7\u30eb\u540d\u307e\u305f\u306fLLM\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9 evaluation_model str or LLM \u30aa\u30d7\u30b7\u30e7\u30f3 None \u8a55\u4fa1\u306b\u4f7f\u7528\u3059\u308b\u30e2\u30c7\u30eb\u540d\u307e\u305f\u306fLLM\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\uff08\u7701\u7565\u6642\u306fmodel\u3092\u4f7f\u7528\uff09 generation_tools list (optional) \u30aa\u30d7\u30b7\u30e7\u30f3 [] \u751f\u6210\u6642\u306b\u4f7f\u7528\u3059\u308b\u30c4\u30fc\u30eb\u306e\u30ea\u30b9\u30c8 evaluation_tools list (optional) \u30aa\u30d7\u30b7\u30e7\u30f3 [] \u8a55\u4fa1\u6642\u306b\u4f7f\u7528\u3059\u308b\u30c4\u30fc\u30eb\u306e\u30ea\u30b9\u30c8 input_guardrails list (optional) \u30aa\u30d7\u30b7\u30e7\u30f3 [] \u751f\u6210\u6642\u306e\u5165\u529b\u30ac\u30fc\u30c9\u30ec\u30fc\u30eb\u30ea\u30b9\u30c8 output_guardrails list (optional) \u30aa\u30d7\u30b7\u30e7\u30f3 [] \u8a55\u4fa1\u6642\u306e\u51fa\u529b\u30ac\u30fc\u30c9\u30ec\u30fc\u30eb\u30ea\u30b9\u30c8 routing_func Callable (optional) \u30aa\u30d7\u30b7\u30e7\u30f3 None \u51fa\u529b\u30eb\u30fc\u30c6\u30a3\u30f3\u30b0\u7528\u95a2\u6570 session_history list (optional) \u30aa\u30d7\u30b7\u30e7\u30f3 [] \u30bb\u30c3\u30b7\u30e7\u30f3\u5c65\u6b74 history_size int \u30aa\u30d7\u30b7\u30e7\u30f3 10 \u5c65\u6b74\u4fdd\u6301\u6570 threshold int \u30aa\u30d7\u30b7\u30e7\u30f3 85 \u8a55\u4fa1\u30b9\u30b3\u30a2\u306e\u95be\u5024 retries int \u30aa\u30d7\u30b7\u30e7\u30f3 3 \u30ea\u30c8\u30e9\u30a4\u8a66\u884c\u56de\u6570 improvement_callback Callable[[Any, EvaluationResult], None] (optional) \u30aa\u30d7\u30b7\u30e7\u30f3 None \u6539\u5584\u63d0\u6848\u7528\u30b3\u30fc\u30eb\u30d0\u30c3\u30af dynamic_prompt Callable[[str], str] (optional) \u30aa\u30d7\u30b7\u30e7\u30f3 None \u52d5\u7684\u30d7\u30ed\u30f3\u30d7\u30c8\u751f\u6210\u95a2\u6570 retry_comment_importance list[str] (optional) \u30aa\u30d7\u30b7\u30e7\u30f3 [] \u30ea\u30c8\u30e9\u30a4\u6642\u306b\u30d7\u30ed\u30f3\u30d7\u30c8\u306b\u542b\u3081\u308b\u30b3\u30e1\u30f3\u30c8\u91cd\u5927\u5ea6"},{"location":"api_reference/#_5","title":"\u623b\u308a\u5024","text":"<p><code>\u751f\u6210\u30fb\u8a55\u4fa1\u7d50\u679c\uff08</code>EvaluationResult<code>\u3092\u542b\u3080\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\uff09</code></p>"},{"location":"api_reference/#_6","title":"\u30e2\u30c7\u30eb\u30e9\u30c3\u30d1\u30fc\u30af\u30e9\u30b9","text":"\u30af\u30e9\u30b9\u540d \u6982\u8981 OpenAIResponsesModel OpenAI API\u7528 GeminiModel Google Gemini API\u7528 ClaudeModel Anthropic Claude API\u7528 OllamaModel Ollama API\u7528"},{"location":"api_reference/#mermaid","title":"\u30af\u30e9\u30b9\u56f3\uff08mermaid\uff09","text":"<pre><code>classDiagram\n    class AgentPipeline {\n        +run(input)\n        -_build_generation_prompt()\n        -_build_evaluation_prompt()\n    }\n    class OpenAIResponsesModel\n    class GeminiModel\n    class ClaudeModel\n    class OllamaModel\n\n    AgentPipeline --&gt; OpenAIResponsesModel\n    AgentPipeline --&gt; GeminiModel\n    AgentPipeline --&gt; ClaudeModel\n    AgentPipeline --&gt; OllamaModel</code></pre>"},{"location":"architecture/","title":"\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u8a2d\u8a08\u66f8","text":""},{"location":"architecture/#_2","title":"\u30b7\u30b9\u30c6\u30e0\u69cb\u6210\u30fb\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u306e\u6982\u8981","text":"<p>\u672c\u30b7\u30b9\u30c6\u30e0\u306f\u30ec\u30a4\u30e4\u30fc\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u3092\u63a1\u7528\u3057\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u69cb\u6210\u3068\u306a\u3063\u3066\u3044\u307e\u3059\u3002</p> <ul> <li>UI/\u5229\u7528\u4f8b\u5c64\uff08examples/\uff09</li> <li>\u30e6\u30fc\u30b9\u30b1\u30fc\u30b9\u5c64\uff08AgentPipeline\u30af\u30e9\u30b9\uff09</li> <li>\u6a5f\u80fd\u30af\u30e9\u30b9\u5c64\uff08\u30c4\u30fc\u30eb\u95a2\u6570\u3001\u30ac\u30fc\u30c9\u30ec\u30fc\u30eb\u95a2\u6570\u3001\u8a55\u4fa1\u95a2\u6570\u306a\u3069\uff09</li> <li>\u30c7\u30fc\u30bf\u30af\u30e9\u30b9\u5c64\uff08pydantic\u30e2\u30c7\u30eb\u3001dataclass\u7b49\uff09</li> <li>\u30b2\u30fc\u30c8\u30a6\u30a7\u30a4\u5c64\uff08get_llm\u7b49\u306e\u30e2\u30c7\u30eb\u53d6\u5f97\uff09</li> <li>\u30e6\u30fc\u30c6\u30a3\u30ea\u30c6\u30a3\u5c64\uff08\u8a2d\u5b9a\u3001\u30ed\u30b0\u7b49\uff09</li> </ul>"},{"location":"architecture/#_3","title":"\u4e3b\u8981\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30fc\u30b9","text":"\u30af\u30e9\u30b9\u540d \u5f79\u5272 \u30ec\u30a4\u30e4\u30fc AgentPipeline \u30ef\u30fc\u30af\u30d5\u30ed\u30fc\u7d71\u5408\u30fb\u5b9f\u884c \u30e6\u30fc\u30b9\u30b1\u30fc\u30b9 Agent LLM\u30a8\u30fc\u30b8\u30a7\u30f3\u30c8 \u6a5f\u80fd/\u30e6\u30fc\u30b9\u30b1\u30fc\u30b9 function_tool\u3067\u5b9a\u7fa9\u3057\u305f\u95a2\u6570 \u30c4\u30fc\u30eb\u3068\u3057\u3066\u5229\u7528 \u6a5f\u80fd input_guardrail\u3067\u5b9a\u7fa9\u3057\u305f\u95a2\u6570 \u5165\u529b\u30ac\u30fc\u30c9\u30ec\u30fc\u30eb \u6a5f\u80fd get_llm \u30e2\u30c7\u30eb\u53d6\u5f97 \u30b2\u30fc\u30c8\u30a6\u30a7\u30a4 pydantic.BaseModel \u69cb\u9020\u5316\u51fa\u529b \u30c7\u30fc\u30bf"},{"location":"architecture/#_4","title":"\u4e3b\u8981\u30c7\u30fc\u30bf\uff08\u30c7\u30fc\u30bf\u306e\u7a2e\u985e\u3001\u69cb\u9020\uff09","text":"\u30af\u30e9\u30b9\u540d \u4fdd\u6301\u30c7\u30fc\u30bf EvaluationResult score, comment MathHomeworkOutput is_math_homework, reasoning"},{"location":"architecture/#erplantuml","title":"ER\u56f3\uff08PlantUML\uff09","text":"<pre><code>@startuml\nclass AgentPipeline {\n  - name\n  - generation_instructions\n  - evaluation_instructions\n  - ...\n}\nclass Agent {\n  - name\n  - model\n  - instructions\n  - ...\n}\nclass EvaluationResult {\n  score: int\n  comment: List~str~\n}\nclass MathHomeworkOutput {\n  is_math_homework: bool\n  reasoning: str\n}\nAgentPipeline --&gt; Agent\nAgent --&gt; EvaluationResult\nAgent --&gt; MathHomeworkOutput\n@enduml\n</code></pre>"},{"location":"architecture/#_5","title":"\u53c2\u8003","text":"<ul> <li>\u8a73\u7d30\u306a\u4e8b\u4f8b\u30fb\u4f7f\u3044\u65b9\u306f docs/pipeline_examples.md \u3092\u53c2\u7167\u3002 </li> </ul>"},{"location":"concept/","title":"\u5bfe\u5fdc\u3059\u308b\u8ab2\u984c","text":"<p>OpenAI Agents SDK\u30e9\u30a4\u30d6\u30e9\u30ea\u3067\u306f\u30b5\u30dd\u30fc\u30c8\u3055\u308c\u3066\u3044\u308b\u306e\u306f\u3001OpenAI\u304c\u63d0\u4f9b\u3059\u308bLLM\u306e\u307f\u3067\u3042\u308b\u3002\u305d\u306e\u305f\u3081\u3001Ollama\u3084Anthropic,Google\u306a\u3069\u306e\u6709\u529b\u30e2\u30c7\u30eb\u3092\u4f7f\u7528\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u306a\u3044\u3002</p> <p>\u305d\u3053\u3067\u3001OpenAI Agents\u3067\u52d5\u4f5c\u3059\u308b\u5404LLM\u30d7\u30ed\u30d0\u30a4\u30c0\u30fc\u3092\u3055\u307d\u30fc\u3068\u3067\u304d\u308b\u3088\u3046\u306b\u3059\u308b\u3082\u306e\u3067\u3042\u308b\u3002</p>"},{"location":"concept/#_2","title":"\u5bfe\u5fdc\u30d7\u30ed\u30d0\u30a4\u30c0","text":"<ul> <li>Ollama</li> <li>Gemini</li> <li>Claude</li> </ul>"},{"location":"concept/#_3","title":"\u5bfe\u5fdc\u65b9\u6cd5","text":"<p>OpenAI Agents\u3067\u306f\u5404\u30c1\u30e3\u30c3\u30c8\u30e2\u30c7\u30eb\u306fModel\u30af\u30e9\u30b9\u3068ModelFactory\u30af\u30e9\u30b9\u3092\u63d0\u4f9b\u3057\u3066\u304a\u308a\u3001\u4e0b\u8a18\u3001URL\u306e\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30fc\u30b9\u306b\u306a\u3089\u3063\u305f\u30af\u30e9\u30b9\u3092\u63d0\u4f9b\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\u3002</p> <p>@https://github.com/openai/openai-agents-python/blob/main/src/agents/models/interface.py</p> <p>Model\u30af\u30e9\u30b9\u306f\u7279\u306bOpenAI\u306e\u30e2\u30c7\u30eb\u3068\u5165\u51fa\u529b\u3092\u63c3\u3048\u308b\u5fc5\u8981\u304c\u3042\u308a\u3001\u4e0b\u8a18\u306e\u5165\u51fa\u529b\u306b\u5b8c\u5168\u306b\u5408\u308f\u305b\u308b\u5fc5\u8981\u304c\u3042\u308b\u3002</p> <p>@https://github.com/openai/openai-agents-python/blob/main/src/agents/models/openai_chatcompletions.py</p>"},{"location":"deprecation_plan/","title":"AgentPipeline Deprecation Plan","text":""},{"location":"deprecation_plan/#agentpipeline","title":"AgentPipeline\u306e\u5ec3\u6b62\u4e88\u5b9a\u8a08\u753b","text":""},{"location":"deprecation_plan/#background","title":"\u80cc\u666f (Background)","text":"<p>Flow/Step\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u306e\u5c0e\u5165\u306b\u3088\u308a\u3001\u3088\u308a\u67d4\u8edf\u3067\u62e1\u5f35\u6027\u306e\u9ad8\u3044\u30ef\u30fc\u30af\u30d5\u30ed\u30fc\u7ba1\u7406\u304c\u53ef\u80fd\u306b\u306a\u308a\u307e\u3057\u305f\u3002GenAgent\u30af\u30e9\u30b9\u306b\u3088\u308a\u3001AgentPipeline\u306e\u6a5f\u80fd\u306fFlow\u30ef\u30fc\u30af\u30d5\u30ed\u30fc\u5185\u3067Step\u3068\u3057\u3066\u4f7f\u7528\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u3063\u305f\u305f\u3081\u3001AgentPipeline\u30af\u30e9\u30b9\u3092\u6bb5\u968e\u7684\u306b\u5ec3\u6b62\u3057\u3066\u3044\u304d\u307e\u3059\u3002</p>"},{"location":"deprecation_plan/#migration-benefits","title":"\u79fb\u884c\u306e\u5229\u70b9 (Migration Benefits)","text":""},{"location":"deprecation_plan/#english","title":"English:","text":"<ul> <li>More flexible workflow composition using Flow/Step architecture</li> <li>Better reusability through modular Step design</li> <li>Enhanced error handling and context management</li> <li>Cleaner separation of concerns</li> <li>Future-proof architecture for complex workflows</li> </ul>"},{"location":"deprecation_plan/#_1","title":"\u65e5\u672c\u8a9e:","text":"<ul> <li>Flow/Step\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u306b\u3088\u308b\u3088\u308a\u67d4\u8edf\u306a\u30ef\u30fc\u30af\u30d5\u30ed\u30fc\u69cb\u6210</li> <li>\u30e2\u30b8\u30e5\u30e9\u30fc\u306aStep\u8a2d\u8a08\u306b\u3088\u308b\u518d\u5229\u7528\u6027\u306e\u5411\u4e0a</li> <li>\u30a8\u30e9\u30fc\u30cf\u30f3\u30c9\u30ea\u30f3\u30b0\u3068\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u7ba1\u7406\u306e\u5f37\u5316</li> <li>\u95a2\u5fc3\u306e\u5206\u96e2\u306e\u3088\u308a\u30af\u30ea\u30fc\u30f3\u306a\u5b9f\u73fe</li> <li>\u8907\u96d1\u306a\u30ef\u30fc\u30af\u30d5\u30ed\u30fc\u306b\u5bfe\u5fdc\u3059\u308b\u5c06\u6765\u6027\u306e\u3042\u308b\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3</li> </ul>"},{"location":"deprecation_plan/#deprecation-timeline","title":"\u5ec3\u6b62\u8a08\u753b (Deprecation Timeline)","text":""},{"location":"deprecation_plan/#1-deprecation-warning-v0022","title":"\u30d5\u30a7\u30fc\u30ba 1: Deprecation Warning\u8ffd\u52a0 (v0.0.22) \u2705 \u5b8c\u4e86","text":"<ul> <li>[x] AgentPipeline\u30af\u30e9\u30b9\u306bdeprecation warning\u3092\u8ffd\u52a0</li> <li>[x] \u65b0\u3057\u3044GenAgent\u3078\u306e\u79fb\u884c\u30ac\u30a4\u30c9\u3092\u4f5c\u6210</li> <li>[x] README.md\u3067Flow/Step\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u3092\u63a8\u5968\u3068\u3057\u3066\u8a18\u8f09</li> </ul>"},{"location":"deprecation_plan/#2-examples-v0023","title":"\u30d5\u30a7\u30fc\u30ba 2: Examples\u79fb\u884c (v0.0.23) \u2705 \u5b8c\u4e86","text":"<ul> <li>[x] genagent_simple_generation.py - \u57fa\u672c\u7684\u306a\u751f\u6210\u4f8b\u306e\u79fb\u884c\u7248</li> <li>[x] genagent_with_evaluation.py - \u8a55\u4fa1\u6a5f\u80fd\u4ed8\u304d\u306e\u79fb\u884c\u7248</li> <li>[x] genagent_with_tools.py - \u30c4\u30fc\u30eb\u4f7f\u7528\u4f8b\u306e\u79fb\u884c\u7248</li> <li>[x] genagent_with_guardrails.py - \u30ac\u30fc\u30c9\u30ec\u30fc\u30eb\u4f7f\u7528\u4f8b\u306e\u79fb\u884c\u7248</li> <li>[x] genagent_with_history.py - \u5c65\u6b74\u7ba1\u7406\u4f8b\u306e\u79fb\u884c\u7248</li> <li>[x] genagent_with_retry.py - \u30ea\u30c8\u30e9\u30a4\u6a5f\u80fd\u4f8b\u306e\u79fb\u884c\u7248</li> <li>[x] genagent_with_dynamic_prompt.py - \u52d5\u7684\u30d7\u30ed\u30f3\u30d7\u30c8\u4f8b\u306e\u79fb\u884c\u7248</li> <li>[x] \u65b0\u3057\u3044Flow/Step\u4f7f\u7528\u4f8b\u306e\u5145\u5b9f\uff08\u5404example\u3067\u8907\u6570\u306e\u4f7f\u7528\u4f8b\u3092\u5b9f\u88c5\uff09</li> <li>[x] \u79fb\u884c\u30ac\u30a4\u30c9\u306e\u5b8c\u6210\uff08deprecation_plan.md\u306b\u8a73\u7d30\u306a\u79fb\u884c\u4f8b\u3092\u8a18\u8f09\uff09</li> </ul>"},{"location":"deprecation_plan/#3-v0024","title":"\u30d5\u30a7\u30fc\u30ba 3: \u30c6\u30b9\u30c8\u79fb\u884c (v0.0.24) \u2705 \u5b8c\u4e86","text":"<ul> <li>[x] AgentPipeline\u306e\u30c6\u30b9\u30c8\u3092GenAgent\u30d9\u30fc\u30b9\u306b\u79fb\u884c</li> <li>[x] \u5f8c\u65b9\u4e92\u63db\u6027\u30c6\u30b9\u30c8\u306e\u8ffd\u52a0</li> <li>[x] GenAgent\u306e\u5b8c\u5168\u306a\u30c6\u30b9\u30c8\u30ab\u30d0\u30ec\u30c3\u30b8</li> </ul>"},{"location":"deprecation_plan/#_2","title":"\u79fb\u884c\u3055\u308c\u305f\u30c6\u30b9\u30c8\u30d5\u30a1\u30a4\u30eb:","text":"<ul> <li><code>test_gen_agent_compatibility.py</code> (377\u884c) - \u4e92\u63db\u6027\u3068deprecation\u8b66\u544a\u30c6\u30b9\u30c8</li> <li><code>test_gen_agent_comprehensive.py</code> (306\u884c, 12\u30c6\u30b9\u30c8) - \u5168\u6a5f\u80fd\u30ab\u30d0\u30ec\u30c3\u30b8\u30c6\u30b9\u30c8</li> <li>\u30c6\u30b9\u30c8\u7d50\u679c: \u5168\u3066\u6210\u529f (12 passed, 19 warnings)</li> <li>\u30ab\u30d0\u30ec\u30c3\u30b8: \u30ac\u30fc\u30c9\u30ec\u30fc\u30eb\u3001\u30d7\u30ed\u30f3\u30d7\u30c8\u69cb\u7bc9\u3001\u30e6\u30fc\u30c6\u30a3\u30ea\u30c6\u30a3\u3001\u5c65\u6b74\u7ba1\u7406\u3001\u95be\u5024\u8a2d\u5b9a</li> </ul>"},{"location":"deprecation_plan/#4-v010","title":"\u30d5\u30a7\u30fc\u30ba 4: \u5b8c\u5168\u524a\u9664 (v0.1.0)","text":"<ul> <li>[ ] AgentPipeline\u30af\u30e9\u30b9\u306e\u5b8c\u5168\u524a\u9664</li> <li>[ ]\u95a2\u9023\u3059\u308bimport\u3068export\u306e\u524a\u9664</li> <li>[ ] \u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u306e\u30af\u30ea\u30fc\u30f3\u30a2\u30c3\u30d7</li> </ul>"},{"location":"deprecation_plan/#migration-guide","title":"\u79fb\u884c\u65b9\u6cd5 (Migration Guide)","text":""},{"location":"deprecation_plan/#_3","title":"\u57fa\u672c\u7684\u306a\u79fb\u884c\u30d1\u30bf\u30fc\u30f3","text":""},{"location":"deprecation_plan/#agentpipeline_1","title":"\u65e7: AgentPipeline","text":"<pre><code>from agents_sdk_models import AgentPipeline\n\npipeline = AgentPipeline(\n    name=\"example\",\n    generation_instructions=\"Generate a response\",\n    model=\"gpt-4o-mini\"\n)\n\nresult = pipeline.run(\"User input\")\n</code></pre>"},{"location":"deprecation_plan/#genagent-flowstep","title":"\u65b0: GenAgent (Flow/Step\u5185)","text":"<pre><code>from agents_sdk_models import GenAgent, Flow, create_simple_flow\n\n# Method 1: Direct GenAgent usage\ngen_agent = GenAgent(\n    name=\"example\", \n    generation_instructions=\"Generate a response\",\n    model=\"gpt-4o-mini\",\n    context_key=\"result\"\n)\n\nflow = create_simple_flow(gen_agent)\nresult = await flow.run(input_data=\"User input\")\n\n# Method 2: Utility function\nfrom agents_sdk_models import create_simple_gen_agent\n\ngen_agent = create_simple_gen_agent(\n    name=\"example\",\n    generation_instructions=\"Generate a response\", \n    model=\"gpt-4o-mini\"\n)\n</code></pre>"},{"location":"deprecation_plan/#_4","title":"\u8a55\u4fa1\u6a5f\u80fd\u4ed8\u304d\u306e\u79fb\u884c","text":""},{"location":"deprecation_plan/#agentpipeline-with-evaluation","title":"\u65e7: AgentPipeline with evaluation","text":"<pre><code>pipeline = AgentPipeline(\n    name=\"evaluated_pipeline\",\n    generation_instructions=\"Generate creative content\",\n    evaluation_instructions=\"Evaluate creativity and quality\",\n    model=\"gpt-4o-mini\",\n    evaluation_threshold=0.8\n)\n</code></pre>"},{"location":"deprecation_plan/#genagent-with-evaluation","title":"\u65b0: GenAgent with evaluation","text":"<pre><code>from agents_sdk_models import create_evaluated_gen_agent\n\ngen_agent = create_evaluated_gen_agent(\n    name=\"evaluated_agent\",\n    generation_instructions=\"Generate creative content\",\n    evaluation_instructions=\"Evaluate creativity and quality\",\n    model=\"gpt-4o-mini\",\n    evaluation_threshold=0.8\n)\n</code></pre>"},{"location":"deprecation_plan/#target-files","title":"\u5bfe\u8c61\u30d5\u30a1\u30a4\u30eb (Target Files)","text":""},{"location":"deprecation_plan/#_5","title":"\u30b3\u30a2\u30d5\u30a1\u30a4\u30eb","text":"<ul> <li><code>src/agents_sdk_models/pipeline.py</code> - AgentPipeline\u30af\u30e9\u30b9</li> <li><code>src/agents_sdk_models/__init__.py</code> - export\u524a\u9664</li> </ul>"},{"location":"deprecation_plan/#examples","title":"Examples\u30d5\u30a1\u30a4\u30eb","text":"<ul> <li><code>examples/pipeline_simple_generation.py</code></li> <li><code>examples/pipeline_with_dynamic_prompt.py</code></li> <li><code>examples/pipeline_with_evaluation.py</code></li> <li><code>examples/pipeline_with_guardrails.py</code></li> <li><code>examples/pipeline_with_history.py</code></li> <li><code>examples/pipeline_with_retry.py</code></li> <li><code>examples/pipeline_with_tools.py</code></li> <li><code>examples/simple_llm_query.py</code></li> </ul>"},{"location":"deprecation_plan/#_6","title":"\u30c6\u30b9\u30c8\u30d5\u30a1\u30a4\u30eb","text":"<ul> <li><code>tests/test_pipeline.py</code></li> <li><code>tests/test_pipeline_*.py</code> \u7cfb\u7d71</li> </ul>"},{"location":"deprecation_plan/#backward-compatibility","title":"\u5f8c\u65b9\u4e92\u63db\u6027\u306e\u4fdd\u8a3c (Backward Compatibility)","text":"<p>v0.1.0\u307e\u3067\u5b8c\u5168\u306a\u5f8c\u65b9\u4e92\u63db\u6027\u3092\u4fdd\u8a3c\u3057\u307e\u3059\uff1a - \u65e2\u5b58\u306eAgentPipeline\u30b3\u30fc\u30c9\u306f\u5f15\u304d\u7d9a\u304d\u52d5\u4f5c - Deprecation warning\u306e\u307f\u8868\u793a - \u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u3067\u65b0\u3057\u3044\u65b9\u6cd5\u3092\u63a8\u5968</p>"},{"location":"deprecation_plan/#support","title":"\u30b5\u30dd\u30fc\u30c8 (Support)","text":"<p>\u79fb\u884c\u306b\u95a2\u3059\u308b\u8cea\u554f\u3084\u30b5\u30dd\u30fc\u30c8\u306f\uff1a - GitHub Issues - \u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u306e\u79fb\u884c\u30ac\u30a4\u30c9 - Example \u30b3\u30fc\u30c9\u306e\u53c2\u7167 </p>"},{"location":"example_stdout_tracing/","title":"Example stdout tracing","text":"<pre><code>from agents.tracing import TracingProcessor, add_trace_processor, set_tracing_disabled\nfrom agents.tracing.span_data import GenerationSpanData\n\nclass StdoutTracer(TracingProcessor):\n    # ---- trace \u30ec\u30d9\u30eb ----\n    def on_trace_start(self, trace): ...\n    def on_trace_end(self, trace): ...\n    # ---- span \u30ec\u30d9\u30eb ----\n    def on_span_start(self, span): ...\n    def on_span_end(self, span):\n        data = span.span_data\n        if isinstance(data, GenerationSpanData):\n            # messages \u306f list[dict(role, content)]\n            sys_msg   = next((m[\"content\"] for m in data.input if m[\"role\"]==\"system\"), \"\")\n            user_msg  = next((m[\"content\"] for m in data.input if m[\"role\"]==\"user\"), \"\")\n            assistant = \"\\n\".join(m[\"content\"] for m in data.output or [])\n            print(\"\\n=== Instruction ===\\n\", sys_msg)\n            print(\"=== Prompt ===\\n\", user_msg)\n            print(\"=== Output ===\\n\", assistant, \"\\n\")\n\n    def shutdown(self): ...            # \u5fc5\u8981\u306a\u3089 flush\n    def force_flush(self): ...\n\n# OpenAI \u3078\u306e\u30a8\u30af\u30b9\u30dd\u30fc\u30c8\u3092\u7121\u52b9\u5316\u3059\u308b\u5834\u5408\nset_tracing_disabled(True)             # \u3042\u308b\u3044\u306f set_trace_processors([StdoutTracer()])\n\n# Processor \u3092\u767b\u9332\nadd_trace_processor(StdoutTracer())\n\n# \u3042\u3068\u306f\u666e\u901a\u306b Agent \u3092\u5b9f\u884c\n# result = await Runner.run(agent, \"\u3053\u3093\u306b\u3061\u306f\")\n</code></pre>"},{"location":"flow_context/","title":"\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u7ba1\u7406\u306e\u6bd4\u8f03\u5206\u6790\u3068\u8a2d\u8a08\u63d0\u6848","text":""},{"location":"flow_context/#1","title":"1. \u6982\u8981","text":"<p>Flow/Step \u30d9\u30fc\u30b9\u3078\u79fb\u884c\u3057\u305f agents\u2011sdk\u2011models \u306b\u6700\u9069\u5316\u3057\u305f\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\uff08<code>Context</code>\uff09\u8a2d\u8a08\u3092\u691c\u8a0e\u3059\u308b\u305f\u3081\u3001\u4e0b\u8a18 3 \u70b9\u3092\u6574\u7406\u3059\u308b\u3002</p> <ol> <li>OpenAI Agents SDK \u306b\u304a\u3051\u308b Context \u306e\u4f4d\u7f6e\u3065\u3051</li> <li>LangChain LCEL \u306e\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u7ba1\u7406\u624b\u6cd5</li> <li>\u305d\u308c\u3089\u3092\u8e0f\u307e\u3048\u305f agents\u2011sdk\u2011models \u5411\u3051 Context \u6848</li> </ol>"},{"location":"flow_context/#2-openai-agents-sdk-context","title":"2. OpenAI Agents SDK \u306e Context","text":"\u9805\u76ee \u6982\u8981 \u76ee\u7684 \u4f9d\u5b58\u6027\u6ce8\u5165/\u5c40\u6240\u72b6\u614b\u3092\u683c\u7d0d\u3002LLM \u3078\u306f\u6e21\u3089\u306a\u3044\u3002 \u578b \u4efb\u610f\u306e\u30e6\u30fc\u30b6\u5b9a\u7fa9\u30af\u30e9\u30b9\uff08<code>@dataclass</code> \u3084 Pydantic \u53ef\uff09\u3002 \u4e3b\u306a\u5185\u5bb9 DB \u30cf\u30f3\u30c9\u30e9\u3001\u30ed\u30ac\u30fc\u3001\u5916\u90e8 API \u306a\u3069\u5b9f\u884c\u6642\u4f9d\u5b58\u7269\u3002\u30e6\u30fc\u30b6\u30fc ID \u3084\u6a29\u9650\u306a\u3069\u30bb\u30c3\u30b7\u30e7\u30f3\u30b9\u30b3\u30fc\u30d7\u60c5\u5831\u3002 \u4f1a\u8a71\u5c65\u6b74 Runner \u304c\u5185\u90e8\u7ba1\u7406\u3057 Agent \u3078\u4f9b\u7d66\u3002Context \u306b\u306f\u901a\u5e38\u542b\u3081\u306a\u3044\u3002 \u5206\u5c90\u5236\u5fa1 Handoff\uff0f\u8907\u6570 Agent \u547c\u3073\u51fa\u3057\u30ed\u30b8\u30c3\u30af\u3092\u30b3\u30fc\u30c9\u5074\u3067\u8a18\u8ff0\u3002Context \u306b\u30eb\u30fc\u30c6\u30a3\u30f3\u30b0\u5024\u306f\u4fdd\u6301\u3057\u306a\u3044\u3002 \u30e1\u30ea\u30c3\u30c8 \u5b8c\u5168\u81ea\u7531 \u3067\u65e2\u5b58 DI \u30d1\u30bf\u30fc\u30f3\u306b\u8fd1\u3044\u3002 \u30c7\u30e1\u30ea\u30c3\u30c8 \u30d5\u30a3\u30fc\u30eb\u30c9\u5b9a\u7fa9\u304c\u6563\u5728\u3057\u3084\u3059\u304f\u3001\u5927\u898f\u6a21\u30d5\u30ed\u30fc\u3067\u53ef\u8aad\u6027\u4f4e\u4e0b\u3002"},{"location":"flow_context/#3-langchain-lcel","title":"3. LangChain LCEL \u306e\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u7ba1\u7406","text":"\u9805\u76ee \u6982\u8981 \u5165\u529b/\u51fa\u529b <code>dict[str, Any]</code> \u3092\u30c1\u30a7\u30fc\u30f3\u9593\u3067\u53d7\u6e21\u3057\u3002 \u4f1a\u8a71\u5c65\u6b74 <code>ConversationBufferMemory</code> \u306a\u3069 Memory \u30af\u30e9\u30b9\u304c\u4fdd\u6301\u3057\u3001\u5165\u529b\u8f9e\u66f8\u306b\u5c55\u958b\u3002 \u30eb\u30fc\u30c6\u30a3\u30f3\u30b0 <code>RunnableConditional</code> \u7b49\u3067\u30e9\u30e0\u30c0\u5224\u5b9a\u3057\u30d6\u30e9\u30f3\u30c1\u3002\u8f9e\u66f8\u306b\u76f4\u63a5\u30d5\u30e9\u30b0\u3092\u66f8\u304f\u5834\u5408\u3082\u3002 \u8ffd\u52a0\u30e1\u30bf RunManager <code>metadata/tags</code> \u306b\u4efb\u610f\u30c7\u30fc\u30bf\u3092\u6dfb\u4ed8\u3002v0.2 \u304b\u3089 <code>Context</code>\uff08\u03b2\uff09\u304c\u5c0e\u5165\u3055\u308c\u30b9\u30b3\u30fc\u30d7\u578b get/set \u304c\u53ef\u80fd\u3002 \u30e1\u30ea\u30c3\u30c8 \u5b66\u7fd2\u30b3\u30b9\u30c8\u304c\u4f4e\u304f\u3001\u67d4\u8edf\u6027\u304c\u9ad8\u3044\u3002 \u30c7\u30e1\u30ea\u30c3\u30c8 \u30ad\u30fc\u885d\u7a81\u30fb\u578b\u4e0d\u6574\u5408\u304c\u8d77\u304d\u3084\u3059\u304f\u3001\u53ef\u8aad\u6027\u3082\u4f4e\u4e0b\u3002"},{"location":"flow_context/#4","title":"4. \u4e21\u8005\u6bd4\u8f03\u65e9\u898b\u8868","text":"\u9805\u76ee Agents SDK LangChain LCEL \u578b \u4efb\u610f\u30af\u30e9\u30b9 dict + Memory + metadata \u4f1a\u8a71\u5c65\u6b74 Runner \u5185\u90e8\u4fdd\u6301 Memory \u30af\u30e9\u30b9 \u30eb\u30fc\u30c6\u30a3\u30f3\u30b0 \u30b3\u30fc\u30c9\u5185 handoff RunnableConditional \u4f9d\u5b58\u6027\u6ce8\u5165 Context \u30d5\u30a3\u30fc\u30eb\u30c9 dict / metadata \u578b\u5b89\u5168\u6027 \u9ad8\uff08\u578b\u4ed8\u304d\u30af\u30e9\u30b9\uff09 \u4f4e\uff08\u81ea\u7531\u30ad\u30fc\uff09"},{"location":"flow_context/#5-agentssdkmodels-context","title":"5. agents\u2011sdk\u2011models \u5411\u3051 Context \u8a2d\u8a08\u6848","text":""},{"location":"flow_context/#51","title":"5.1 \u8a2d\u8a08\u65b9\u91dd","text":"<ol> <li>\u578b\u5b89\u5168\u3067\u8aad\u307f\u3084\u3059\u3044 : Pydantic <code>BaseModel</code> \u3092\u63a1\u7528\u3057 IDE \u88dc\u5b8c\u3092\u6d3b\u7528\u3002</li> <li>\u5c65\u6b74\u3082\u4fdd\u6301 : Agents SDK \u4f9d\u5b58\u3092\u907f\u3051\u3001Flow \u5185\u3067\u4e00\u8cab\u7ba1\u7406\u3002</li> <li>\u30eb\u30fc\u30c6\u30a3\u30f3\u30b0\u5185\u5305 : <code>next_label: str | None</code> \u3092\u30d5\u30a3\u30fc\u30eb\u30c9\u5316\u3057 Step \u8fd4\u5374\u3067\u66f4\u65b0\u3002</li> <li>\u8f9e\u66f8\u4e92\u63db : <code>as_dict()/from_dict()</code> \u3067 LCEL \u3068\u306e\u30d6\u30ea\u30c3\u30b8\u3092\u63d0\u4f9b\u3002</li> </ol>"},{"location":"flow_context/#52","title":"5.2 \u30d5\u30a3\u30fc\u30eb\u30c9\u4f8b","text":"<pre><code>class Context(BaseModel):\n    last_user_input: str | None = None      # \u76f4\u8fd1\u30e6\u30fc\u30b6\u30fc\u5165\u529b\n    messages: list[Message] = []            # \u4f1a\u8a71\u5c65\u6b74\n    knowledge: dict[str, Any] = {}          # RAG \u306a\u3069\u5916\u90e8\u77e5\u8b58\n    prev_outputs: dict[str, Any] = {}       # \u524d Step \u751f\u6210\u7269\n    next_label: str | None = None           # \u30eb\u30fc\u30c6\u30a3\u30f3\u30b0\u6307\u793a\n    artifacts: dict[str, Any] = {}          # Flow \u5168\u4f53\u6210\u679c\u7269\n    shared_state: dict[str, Any] = {}       # \u4efb\u610f\u5171\u6709\u5024\n\n    def as_dict(self) -&gt; dict[str, Any]:\n        \"\"\"LCEL \u4e92\u63db\u8f9e\u66f8\u3078\u5909\u63db\"\"\"\n        d = self.dict()\n        d[\"history\"] = d.pop(\"messages\")\n        return d\n\n    @classmethod\n    def from_dict(cls, d: dict[str, Any]) -&gt; \"Context\":\n        d = d.copy()\n        d[\"messages\"] = d.pop(\"history\", [])\n        return cls(**d)\n</code></pre>"},{"location":"flow_context/#53","title":"5.3 \u5229\u7528\u30d1\u30bf\u30fc\u30f3","text":"<pre><code>async def step_example(ctx: Context) -&gt; Context:\n    # 1. \u30e6\u30fc\u30b6\u30fc\u5165\u529b\u3092\u53d6\u5f97\n    user_msg = await ctx.io.ask(\"\u5165\u529b\u3057\u3066\u304f\u3060\u3055\u3044\")\n    ctx.last_user_input = user_msg\n    ctx.messages.append(user_msg)\n\n    # 2. \u30e2\u30c7\u30eb\u547c\u3073\u51fa\u3057\n    answer = await model.invoke(ctx.messages)\n    ctx.messages.append(answer)\n\n    # 3. \u30eb\u30fc\u30c6\u30a3\u30f3\u30b0\u5224\u5b9a\n    ctx.next_label = \"done\" if is_ok(answer) else \"retry\"\n    return ctx\n</code></pre>"},{"location":"flow_context/#54-agents-sdk","title":"5.4 Agents SDK \u9023\u643a\u6848","text":"<ul> <li>BridgeStep: Agents SDK Runner \u3092\u5185\u90e8\u3067\u547c\u3073\u3001Runner.context \u2194\ufe0e Context \u5909\u63db\u3057\u306a\u304c\u3089\u5b9f\u884c\u3002</li> <li>\u5c65\u6b74\u540c\u671f: Runner \u5074\u306e\u30e1\u30c3\u30bb\u30fc\u30b8\u3092\u53d6\u5f97\u3057 <code>ctx.messages</code> \u306b\u30de\u30fc\u30b8\u3002</li> </ul>"},{"location":"flow_context/#55","title":"5.5 \u30e1\u30ea\u30c3\u30c8 &amp; \u30c8\u30ec\u30fc\u30c9\u30aa\u30d5","text":"\u5229\u70b9 \u61f8\u5ff5 \u578b\u5b89\u5168\u30fb\u88dc\u5b8c\u304c\u52b9\u304f Pydantic \u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u30b3\u30b9\u30c8\u5897 LCEL \u3068\u306e\u53cc\u65b9\u5411\u5909\u63db\u3067\u4f7f\u3044\u307e\u308f\u305b\u308b \u4e8c\u91cd\u4fdd\u6301\u3067\u30e1\u30e2\u30ea\u6d88\u8cbb \u30eb\u30fc\u30c6\u30a3\u30f3\u30b0\u304c\u660e\u793a\u7684 Agents SDK \u7684\u306b\u306f\u5c11\u3057\u5197\u9577"},{"location":"flow_context/#56","title":"5.6 \u4eca\u5f8c\u306e\u62e1\u5f35\u30a4\u30e1\u30fc\u30b8","text":"<ol> <li>ContextPlugin API : \u5916\u90e8\u30b7\u30b9\u30c6\u30e0\u3068\u81ea\u52d5\u540c\u671f\u3059\u308b\u62e1\u5f35\u30dd\u30a4\u30f3\u30c8\uff08\u4f8b: DB/Redis\uff09\u3002</li> <li>StreamingMessages : \u5c65\u6b74\u90e8\u306e\u307f\u9045\u5ef6\u30ed\u30fc\u30c9/\u30b9\u30c8\u30ea\u30fc\u30df\u30f3\u30b0\u53ef\u80fd\u306b\u3002</li> <li>TypedArtifacts : \u6210\u679c\u7269\u3092\u30af\u30e9\u30b9\u3054\u3068\u306b\u578b\u4ed8\u3051\u3057\u3066\u5b89\u5168\u6027\u5411\u4e0a\u3002</li> </ol> <p>\u3053\u308c\u306b\u3088\u308a\u3001Agents SDK Models \u306f \u578b\u5b89\u5168\u30fb\u5206\u304b\u308a\u3084\u3059\u3055\u30fb\u4ed6\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u4e92\u63db \u3092\u4e21\u7acb\u3057\u305f\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u7ba1\u7406\u3092\u5b9f\u73fe\u3067\u304d\u308b\u3002</p>"},{"location":"flow_step/","title":"Agents SDK Models: Flow/DAG \u6a5f\u80fd\u8a55\u4fa1\u3068\u62e1\u5f35\u8a2d\u8a08\u00a0(v3)","text":""},{"location":"flow_step/#step-flow-api","title":"Step / Flow API \u30ea\u30d5\u30a1\u30ec\u30f3\u30b9","text":"<p>\u672c\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u3067\u306f agents\u2011sdk\u2011models \u306b\u304a\u3051\u308b <code>Step</code> \u3068 <code>Flow</code> \u304c\u63d0\u4f9b\u3059\u308b\u4e3b\u8981\u30e1\u30bd\u30c3\u30c9\u30fb\u5c5e\u6027\u3092\u4e00\u89a7\u8868\u3067\u6574\u7406\u3059\u308b\u3002CLI \u3067\u3082 GUI \u3067\u3082\u5229\u7528\u3057\u3084\u3059\u3044\u3088\u3046 \u540c\u671f\u30e6\u30fc\u30c6\u30a3\u30ea\u30c6\u30a3 \u3068 \u975e\u540c\u671f\u30bf\u30b9\u30af \u306e\u4e21\u7cfb\u7d71\u3092\u542b\u3081\u308b\u3002</p>"},{"location":"flow_step/#1-step","title":"1. Step \u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30fc\u30b9","text":"\u30e1\u30f3\u30d0\u30fc \u7a2e\u5225 \u30b7\u30b0\u30cd\u30c1\u30e3 / \u578b \u8aac\u660e <code>name</code> \u5c5e\u6027 <code>str</code> \u30b9\u30c6\u30c3\u30d7\u8b58\u5225\u540d\uff08DSL \u3067\u53c2\u7167\uff09 <code>run</code> <code>async def</code> <code>run(user_input: str \\| None, ctx: Context) -&gt; Context</code> \u30b9\u30c6\u30c3\u30d7\u3092 1 \u56de\u5b9f\u884c\u3057\u3001\u65b0\u3057\u3044 <code>Context</code> \u3092\u8fd4\u3059\u3002\u5fc5\u8981\u306b\u5fdc\u3058 <code>ctx.next_label</code> \u3092\u66f4\u65b0\u3059\u308b\u3002 <p>\u5b9f\u88c5\u4f8b: <code>UserInputStep</code>, <code>ConditionStep</code>, <code>AgentPipeline</code> \u306a\u3069\u3002</p>"},{"location":"flow_step/#2-flow","title":"2. Flow \u30af\u30e9\u30b9","text":""},{"location":"flow_step/#_1","title":"\ud83d\ude80 \u65b0\u6a5f\u80fd\uff1a\u62e1\u5f35\u3055\u308c\u305f\u30b3\u30f3\u30b9\u30c8\u30e9\u30af\u30bf","text":"<p>Flow\u30af\u30e9\u30b9\u306f3\u3064\u306e\u65b9\u6cd5\u3067\u4f5c\u6210\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u308a\u307e\u3057\u305f\uff1a</p> <pre><code># 1. \u5358\u4e00\u30b9\u30c6\u30c3\u30d7\uff08\u6700\u3082\u30b7\u30f3\u30d7\u30eb\uff01\uff09\nflow = Flow(steps=gen_agent)\n\n# 2. \u30b7\u30fc\u30b1\u30f3\u30b7\u30e3\u30eb\u30b9\u30c6\u30c3\u30d7\uff08\u81ea\u52d5\u63a5\u7d9a\uff01\uff09\nflow = Flow(steps=[step1, step2, step3])\n\n# 3. \u5f93\u6765\u65b9\u5f0f\uff08\u8907\u96d1\u306a\u30d5\u30ed\u30fc\u7528\uff09\nflow = Flow(start=\"step1\", steps={\"step1\": step1, \"step2\": step2})\n</code></pre> \u30e1\u30bd\u30c3\u30c9 / \u5c5e\u6027 \u540c\u671f / \u975e\u540c\u671f \u30b7\u30b0\u30cd\u30c1\u30e3 \u5f79\u5272\u30fb\u5099\u8003 <code>__init__</code> sync <code>Flow(start=None, steps=Dict[str,Step]|List[Step]|Step)</code> \u62e1\u5f35\uff01 \u8f9e\u66f8\u30fb\u30ea\u30b9\u30c8\u30fb\u5358\u4e00\u30b9\u30c6\u30c3\u30d7\u306b\u5bfe\u5fdc\u3002\u30ea\u30b9\u30c8\u306f\u81ea\u52d5\u63a5\u7d9a\u3001\u5358\u4e00\u306f\u76f4\u63a5\u5b9f\u884c\u3002 <code>context</code> \u5c5e\u6027 <code>Context</code> \u73fe\u5728\u306e\u5171\u6709\u72b6\u614b\u30fb\u5c65\u6b74\u306a\u3069\u3092\u4fdd\u6301\u3002 <code>finished</code> \u5c5e\u6027 <code>bool</code> <code>ctx.next_label is None</code> \u3067 <code>True</code>\u3002 <code>run</code> async `run(initial_input: str None = None) -&gt; Context` <code>run_loop</code> async <code>run_loop() -&gt; None</code> \u975e\u540c\u671f\u30bf\u30b9\u30af\u3068\u3057\u3066\u5e38\u99d0\u3002<code>UserInputStep</code> \u306b\u5f53\u305f\u308b\u3068\u4e00\u6642\u505c\u6b62\u3057\u3001<code>feed()</code> \u5f85\u3061\u3002GUI / WebSocket \u3068\u76f8\u6027\u304c\u826f\u3044\u3002 <code>next_prompt</code> sync `next_prompt() -&gt; str None` <code>feed</code> sync / async <code>feed(user_input: str) -&gt; None</code> \u30e6\u30fc\u30b6\u30fc\u5165\u529b\u3092 <code>ctx.last_user_input</code> \u306b\u683c\u7d0d\u3057\u3001<code>run_loop</code> \u3092\u518d\u958b\u3055\u305b\u308b\u3002 <code>step</code> sync <code>step() -&gt; None</code> \u975e\u540c\u671f\u3092\u4f7f\u308f\u305a 1 \u30b9\u30c6\u30c3\u30d7\u3060\u3051\u540c\u671f\u7684\u306b\u9032\u3081\u308b\u3002LLM \u547c\u3073\u51fa\u3057\u4e2d\u306f\u30d6\u30ed\u30c3\u30af\u3002"},{"location":"flow_step/#_2","title":"\u30e9\u30a4\u30d5\u30b5\u30a4\u30af\u30eb\u56f3\uff08\u6982\u8981\uff09","text":"<ol> <li><code>flow.run_loop()</code> \u3092\u30bf\u30b9\u30af\u8d77\u52d5</li> <li>Flow \u304c <code>UserInputStep</code> \u306b\u5230\u9054 \u21d2 <code>ctx.awaiting_prompt</code> \u306b\u8cea\u554f\u6587\u8a2d\u5b9a &amp; <code>return</code></li> <li>\u30a2\u30d7\u30ea\u5074 \u2192 <code>next_prompt()</code> \u3067\u53d6\u5f97 \u2192 \u30e6\u30fc\u30b6\u30fc\u306b\u63d0\u793a</li> <li><code>feed()</code> \u3067\u56de\u7b54\u6ce8\u5165 \u2192 <code>ctx.waiter.set()</code> \u21d2 <code>run_loop</code> \u518d\u958b</li> <li><code>ctx.next_label is None</code> \u306b\u306a\u3063\u305f\u3089\u30d5\u30ed\u30fc\u7d42\u4e86\u3001<code>flow.finished == True</code>\u3002</li> </ol>"},{"location":"flow_step/#3","title":"3. \ud83c\udfaf \u65b0\u3057\u3044\u8d85\u30b7\u30f3\u30d7\u30eb\u4f7f\u7528\u4f8b","text":"<p>\u65b0\u3057\u3044Flow\u30b3\u30f3\u30b9\u30c8\u30e9\u30af\u30bf\u3092\u4f7f\u3063\u305f\u8d85\u30b7\u30f3\u30d7\u30eb\u306a\u4f8b\uff1a</p> <pre><code>from agents_sdk_models import create_simple_gen_agent, Flow, DebugStep\n\n# 1. \u5358\u4e00\u30b9\u30c6\u30c3\u30d7\uff08\u305f\u3063\u305f1\u884c\uff01\uff09\ngen_agent = create_simple_gen_agent(\"assistant\", \"\u89aa\u5207\u306b\u56de\u7b54\u3057\u307e\u3059\", \"gpt-4o-mini\")\nflow = Flow(steps=gen_agent)\nresult = await flow.run(input_data=\"\u3053\u3093\u306b\u3061\u306f\")\nprint(result.shared_state[\"assistant_result\"])\n\n# 2. \u30b7\u30fc\u30b1\u30f3\u30b7\u30e3\u30eb\u30b9\u30c6\u30c3\u30d7\uff08\u81ea\u52d5\u63a5\u7d9a\uff01\uff09\nreviewer = create_simple_gen_agent(\"reviewer\", \"\u56de\u7b54\u3092\u30ec\u30d3\u30e5\u30fc\u3057\u307e\u3059\", \"gpt-4o\")\nflow = Flow(steps=[gen_agent, reviewer, DebugStep(\"done\", \"\u5b8c\u4e86\")])\nresult = await flow.run(input_data=\"AI\u306b\u3064\u3044\u3066\u6559\u3048\u3066\")\n\n# 3. \u8907\u6570GenAgent\uff08\u30de\u30eb\u30c1\u30a8\u30fc\u30b8\u30a7\u30f3\u30c8\uff09\nidea_gen = create_simple_gen_agent(\"idea\", \"\u30a2\u30a4\u30c7\u30a2\u751f\u6210\", \"gpt-4o-mini\")\nwriter = create_simple_gen_agent(\"writer\", \"\u8a18\u4e8b\u57f7\u7b46\", \"gpt-4o\")\neditor = create_simple_gen_agent(\"editor\", \"\u7de8\u96c6\u30fb\u6821\u6b63\", \"claude-3-5-sonnet-latest\")\n\nflow = Flow(steps=[idea_gen, writer, editor])  # \u81ea\u52d5\u3067\u30a2\u30a4\u30c7\u30a2\u2192\u57f7\u7b46\u2192\u7de8\u96c6\nresult = await flow.run(input_data=\"AI\u6280\u8853\u306b\u3064\u3044\u3066\")\n</code></pre>"},{"location":"flow_step/#4-vs","title":"4. \u540c\u671f vs \u975e\u540c\u671f \u5229\u7528\u4f8b","text":""},{"location":"flow_step/#gui-websocket","title":"\u975e\u540c\u671f GUI / WebSocket","text":"<pre><code>flow = Flow(...)\nasyncio.create_task(flow.run_loop())\n...\nprompt = await flow.context.awaiting_prompt_event.wait()\nawait websocket.send_json({\"prompt\": prompt})\n...\nawait flow.feed(user_input_from_client)\n</code></pre>"},{"location":"flow_step/#cli","title":"\u540c\u671f CLI","text":"<pre><code>flow = Flow(...)\nwhile not flow.finished:\n    if (prompt := flow.next_prompt()):\n        user = input(prompt + \"&gt; \")\n        flow.feed(user)\n    else:\n        flow.step()  # LLM \u547c\u3073\u51fa\u3057\u306a\u3069\nprint(flow.context.artifacts)\n</code></pre> <p>\u3053\u308c\u3067 Step / Flow \u306e API \u4e00\u89a7\u3068\u904b\u7528\u30d1\u30bf\u30fc\u30f3\u304c\u4fef\u77b0\u3067\u304d\u308b\u3002\u8a73\u3057\u3044 <code>Context</code> \u30d5\u30a3\u30fc\u30eb\u30c9\u5b9a\u7fa9\u3084\u578b\u5909\u63db\u30e6\u30fc\u30c6\u30a3\u30ea\u30c6\u30a3\u306f Agents Sdk Context Design \u30ad\u30e3\u30f3\u30d0\u30b9\u3092\u53c2\u7167\u3002</p>"},{"location":"flow_step/#4-flowstep","title":"4. Flow/Step \u6a5f\u80fd\u306e\u8a55\u4fa1","text":""},{"location":"flow_step/#41","title":"4.1\u00a0\u5f37\u307f","text":"<ul> <li>\u5ba3\u8a00\u7684 Step\u306b\u3088\u308bDAG \u5b9a\u7fa9\u00a0\u2014\u00a0\u5b66\u7fd2\u30b3\u30b9\u30c8\u304c\u4f4e\u3044</li> <li>Pipeline \u518d\u5229\u7528\u6027\u00a0\u2014\u00a0\u65e2\u5b58\u8cc7\u7523\u3092\u305d\u306e\u307e\u307e\u30b9\u30c6\u30c3\u30d7\u3068\u3057\u3066\u6d3b\u7528</li> <li>\u6697\u9ed9\u306e END\u00a0\u2014\u00a0\u30b4\u30fc\u30eb\u30b9\u30c6\u30c3\u30d7\u7701\u7565\u3067\u6700\u77ed\u69cb\u6210</li> <li>\u52d5\u7684\u30eb\u30fc\u30c6\u30a3\u30f3\u30b0\u00a0\u2014\u00a0<code>router_fn</code> \u306b\u3088\u308b\u6761\u4ef6\u5206\u5c90\u304c\u5bb9\u6613</li> </ul>"},{"location":"flow_step/#42","title":"4.2\u00a0\u8ab2\u984c","text":"<ul> <li>\u5927\u898f\u6a21\u5316\u3067\u53ef\u8aad\u6027\u4f4e\u4e0b \u2014\u00a0\u8f9e\u66f8\u5b9a\u7fa9\u304c\u80a5\u5927</li> <li>\u5171\u6709\u72b6\u614b\u30ac\u30a4\u30c9\u4e0d\u8db3\u00a0\u2014\u00a0Context \u8a2d\u8a08\u304c\u5fc5\u9808</li> <li>\u30e6\u30fc\u30b6\u30fc\u5165\u529b\u30b9\u30c6\u30c3\u30d7\u672a\u6574\u5099\u00a0\u2014\u00a0\u6a19\u6e96\u578b\u3092\u8ffd\u52a0\u3059\u3079\u304d</li> <li>\u4e26\u5217\u5b9f\u884c\u672a\u5bfe\u5fdc\u00a0\u2014\u00a0Fork/Join\u00a0\u69cb\u6587\u306e\u62e1\u5145\u304c\u5fc5\u8981</li> </ul>"},{"location":"flow_step/#43","title":"4.3\u00a0\u7dcf\u8a55","text":"<p>80%\u00a0\u30e6\u30fc\u30b9\u30b1\u30fc\u30b9\u3092\u6700\u77ed\u30b3\u30fc\u30c9\u3067\u89e3\u6c7a\u3059\u308b\u30e9\u30a4\u30c8\u7d1a\u3060\u304c\u3001\u8ab2\u984c\u514b\u670d\u3067\u5927\u898f\u6a21\u30fb\u5bfe\u8a71\u30ef\u30fc\u30af\u30d5\u30ed\u30fc\u3078\u62e1\u5f35\u53ef\u80fd\u3002</p>"},{"location":"flow_step/#5-flow","title":"5. Flow \u62e1\u5f35\u8a2d\u8a08\u63d0\u6848","text":""},{"location":"flow_step/#51","title":"5.1\u00a0\u8a2d\u8a08\u76ee\u6a19","text":"<ol> <li>\u5ba3\u8a00\u7684 DSL\u00a0\u00d7 \u53ef\u8996\u6027</li> <li>\u30e6\u30fc\u30b6\u30fc\u5165\u529b\u30b9\u30c6\u30c3\u30d7\u00a0\u306e\u6a19\u6e96\u5316</li> <li>\u578b\u5b89\u5168 Context\u00a0\u5171\u6709</li> <li>\u975e\u540c\u671f\u30fb\u4e26\u5217\u00a0\u30b5\u30dd\u30fc\u30c8</li> <li>\u30aa\u30d6\u30b6\u30fc\u30d0\u30d3\u30ea\u30c6\u30a3\u00a0\u7d44\u307f\u8fbc\u307f</li> </ol>"},{"location":"flow_step/#52-step","title":"5.2\u00a0\u5171\u901a\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30fc\u30b9\u00a0<code>Step</code>","text":"<pre><code>from typing import Protocol, runtime_checkable\n\n@runtime_checkable\nclass Step(Protocol):\n    name: str\n    async def run(self, user_input: str | None, ctx: \"Context\") -&gt; \"Context\":\n        ...\n</code></pre> <p><code>AgentPipeline</code> \u3082\u540c\u30b7\u30b0\u30cd\u30c1\u30e3\u3067\u9069\u5408\u3002</p>"},{"location":"flow_step/#53-step","title":"5.3\u00a0\u4ee3\u8868\u7684 Step \u5b9f\u88c5","text":"<p><code>UserInputStep</code>,\u00a0<code>ConditionStep</code>,\u00a0<code>ForkStep</code>,\u00a0<code>JoinStep</code> \u306a\u3069\uff08\u8a73\u7d30\u306f\u524d\u7248\u3068\u540c\u7b49\uff09\u3002</p>"},{"location":"flow_step/#54-dsl","title":"5.4\u00a0DSL \u4f7f\u7528\u4f8b","text":"<pre><code>flow = Flow(\n    start=\"welcome\",\n    steps={\n        \"welcome\": UserInputStep(\"welcome\", prompt=\"\u3054\u7528\u4ef6\u3092\u5165\u529b\u3057\u3066\u304f\u3060\u3055\u3044\"),\n        \"triage\": triage_agent_pipeline,   # Step \u5b9f\u88c5\u6e08\u307f\n        \"need_approval\": ConditionStep(\n            \"need_approval\",\n            cond=lambda ctx: ctx.shared_state.get(\"need_approval\", False),\n            if_true=\"ask_ok\", if_false=\"final\"\n        ),\n        \"ask_ok\": UserInputStep(\"ask_ok\", prompt=\"\u5b9f\u884c\u3057\u3066\u3082\u3088\u308d\u3057\u3044\u3067\u3059\u304b\uff1f(y/n)\"),\n        \"final\": response_agent_pipeline,\n    },\n)\n\n# ---------------- \u975e\u540c\u671f GUI / API \u30b5\u30fc\u30d0 ----------------\nasyncio.create_task(flow.async_run_loop())\n...\nprompt = await flow.context.awaiting_prompt_event.wait()\nawait websocket.send_json({\"prompt\": prompt})\n...\nflow.feed(user_answer)\n\n# ---------------- \u540c\u671f CLI ----------------\nwhile not flow.finished:\n    if (p := flow.next_prompt()):\n        flow.feed(input(p + \"&gt; \"))\n    else:\n        flow.step()\nprint(flow.context.artifacts)\n</code></pre>"},{"location":"flow_step/#55-context","title":"5.5\u00a0Context","text":"<p>\u2192\u00a0\u8a73\u7d30\u306f \u201cAgents\u00a0Sdk\u00a0Context\u00a0Design\u201d \u30ad\u30e3\u30f3\u30d0\u30b9\u3092\u53c2\u7167\u3002</p>"},{"location":"flow_step/#56","title":"5.6\u00a0\u4e26\u5217\u5b9f\u884c\u30b5\u30dd\u30fc\u30c8","text":"\u69cb\u6587 \u8aac\u660e <code>ForkStep(branches: list[str])</code> \u6307\u5b9a\u30b9\u30c6\u30c3\u30d7\u3092 async\u00a0gather \u3067\u4e26\u5217\u8d77\u52d5 `JoinStep(join_type=\"all\" \"any\")` <code>Context</code> \u30de\u30fc\u30b8\u5f8c <code>next_label</code> \u8a2d\u5b9a"},{"location":"flow_step/#57-gui","title":"5.7\u00a0GUI / \u30c1\u30e3\u30c3\u30c8\u7d71\u5408","text":"<ul> <li><code>flow.async_run_loop()</code> \u3092\u30d0\u30c3\u30af\u30b0\u30e9\u30a6\u30f3\u30c9\u30bf\u30b9\u30af\u5316</li> <li><code>ctx.io</code>\u00a0\u62bd\u8c61\u3067 CLI / Web / Bot \u3092\u7d71\u4e00</li> <li>\u30b9\u30c8\u30ea\u30fc\u30df\u30f3\u30b0\u5fdc\u7b54\u306f <code>Step</code> \u5185\u3067\u30c8\u30fc\u30af\u30f3\u9010\u6b21\u9001\u4fe1</li> </ul>"},{"location":"flow_step/#58","title":"5.8\u00a0\u30aa\u30d6\u30b6\u30fc\u30d0\u30d3\u30ea\u30c6\u30a3","text":"<ul> <li><code>before_run</code> / <code>after_run</code> \u30d5\u30c3\u30af \u2192 OpenTelemetry\u00a0Span</li> <li><code>ctx.trace_id</code> \u3067\u5168 Step \u6a2a\u65ad\u306e\u76f8\u95a2 ID</li> </ul>"},{"location":"flow_step/#59","title":"5.9\u00a0\u30ed\u30fc\u30c9\u30de\u30c3\u30d7","text":"\u30d0\u30fc\u30b8\u30e7\u30f3 \u4e3b\u8981\u6a5f\u80fd v0.1 <code>Step</code>, <code>UserInputStep</code>, <code>Context</code>, \u76f4\u5217\u00a0Flow, <code>async_run</code> / <code>async_run_loop</code> v0.2 <code>ConditionStep</code>, <code>ForkStep</code>, <code>JoinStep</code>, \u4e26\u5217\u5b9f\u884c v0.3 GUI/\u30c1\u30e3\u30c3\u30c8 I/O \u30a2\u30c0\u30d7\u30bf\u3001OpenTelemetry \u9023\u643a v0.4 Step \u30c6\u30f3\u30d7\u30ec\u30fc\u30c8\u767b\u9332\u3001AutoDocs \u751f\u6210 v1.0 \u5b89\u5b9a\u7248\u30ea\u30ea\u30fc\u30b9\u3001\u30bb\u30de\u30f3\u30c6\u30a3\u30c3\u30af\u30d0\u30fc\u30b8\u30e7\u30cb\u30f3\u30b0"},{"location":"function_spec/","title":"\u6a5f\u80fd\u4ed5\u69d8\u66f8","text":""},{"location":"function_spec/#1","title":"1. \u30b7\u30f3\u30d7\u30eb\u306a\u751f\u6210","text":"<ul> <li>\u30e6\u30fc\u30b9\u30b1\u30fc\u30b9\u624b\u9806</li> <li>\u30e6\u30fc\u30b6\u30fc\u304c\u5165\u529b\u6587\u3092\u4e0e\u3048\u308b</li> <li>AgentPipeline\u304c\u751f\u6210\u6307\u793a\u3092\u3082\u3068\u306bLLM\u3067\u751f\u6210</li> <li>\u7d50\u679c\u3092\u8fd4\u3059</li> <li>\u30e6\u30fc\u30b9\u30b1\u30fc\u30b9\u30d5\u30ed\u30fc\u56f3 <pre><code>@startuml\nactor User\nparticipant AgentPipeline\nparticipant Agent\nUser -&gt; AgentPipeline: \u5165\u529b\u6587\nAgentPipeline -&gt; Agent: \u751f\u6210\u6307\u793a+\u5165\u529b\nAgent -&gt; AgentPipeline: \u751f\u6210\u7d50\u679c\nAgentPipeline -&gt; User: \u7d50\u679c\u8fd4\u5374\n@enduml\n</code></pre></li> </ul>"},{"location":"function_spec/#2","title":"2. \u751f\u6210\u7269\u306e\u8a55\u4fa1\u4ed8\u304d\u751f\u6210","text":"<ul> <li>\u30e6\u30fc\u30b9\u30b1\u30fc\u30b9\u624b\u9806</li> <li>\u30e6\u30fc\u30b6\u30fc\u304c\u5165\u529b\u6587\u3092\u4e0e\u3048\u308b</li> <li>AgentPipeline\u304c\u751f\u6210\u6307\u793a\u3067\u751f\u6210</li> <li>AgentPipeline\u304c\u8a55\u4fa1\u6307\u793a\u3067\u8a55\u4fa1</li> <li>\u8a55\u4fa1\u30b9\u30b3\u30a2\u304c\u95be\u5024\u4ee5\u4e0a\u306a\u3089\u7d50\u679c\u8fd4\u5374\u3001\u672a\u6e80\u306a\u3089\u30ea\u30c8\u30e9\u30a4or\u5931\u6557</li> <li>\u30e6\u30fc\u30b9\u30b1\u30fc\u30b9\u30d5\u30ed\u30fc\u56f3 <pre><code>@startuml\nactor User\nparticipant AgentPipeline\nparticipant Agent as Generator\nparticipant Agent as Evaluator\nUser -&gt; AgentPipeline: \u5165\u529b\u6587\nAgentPipeline -&gt; Generator: \u751f\u6210\u6307\u793a+\u5165\u529b\nGenerator -&gt; AgentPipeline: \u751f\u6210\u7d50\u679c\nAgentPipeline -&gt; Evaluator: \u8a55\u4fa1\u6307\u793a+\u751f\u6210\u7d50\u679c\nEvaluator -&gt; AgentPipeline: \u8a55\u4fa1\u30b9\u30b3\u30a2\nalt \u30b9\u30b3\u30a2&gt;=\u95be\u5024\n  AgentPipeline -&gt; User: \u7d50\u679c\u8fd4\u5374\nelse \u30b9\u30b3\u30a2&lt;\u95be\u5024\n  AgentPipeline -&gt; User: \u5931\u6557\u901a\u77e5\nend\n@enduml\n</code></pre></li> </ul>"},{"location":"function_spec/#3","title":"3. \u30c4\u30fc\u30eb\u9023\u643a","text":"<ul> <li>\u30e6\u30fc\u30b9\u30b1\u30fc\u30b9\u624b\u9806</li> <li>\u30e6\u30fc\u30b6\u30fc\u304c\u5165\u529b\u6587\u3092\u4e0e\u3048\u308b</li> <li>AgentPipeline\u304c\u30c4\u30fc\u30eb\u4ed8\u304d\u3067\u751f\u6210</li> <li>\u5fc5\u8981\u306b\u5fdc\u3058\u3066\u30c4\u30fc\u30eb\u95a2\u6570\u304c\u547c\u3070\u308c\u308b</li> <li>\u7d50\u679c\u3092\u8fd4\u3059</li> <li>\u30e6\u30fc\u30b9\u30b1\u30fc\u30b9\u30d5\u30ed\u30fc\u56f3 <pre><code>@startuml\nactor User\nparticipant AgentPipeline\nparticipant Agent\nparticipant Tool\nUser -&gt; AgentPipeline: \u5165\u529b\u6587\nAgentPipeline -&gt; Agent: \u751f\u6210\u6307\u793a+\u5165\u529b+\u30c4\u30fc\u30eb\nAgent -&gt; Tool: \u30c4\u30fc\u30eb\u547c\u3073\u51fa\u3057\nTool -&gt; Agent: \u30c4\u30fc\u30eb\u7d50\u679c\nAgent -&gt; AgentPipeline: \u751f\u6210\u7d50\u679c\nAgentPipeline -&gt; User: \u7d50\u679c\u8fd4\u5374\n@enduml\n</code></pre></li> </ul>"},{"location":"function_spec/#4","title":"4. \u30ac\u30fc\u30c9\u30ec\u30fc\u30eb\uff08\u5165\u529b\u30ac\u30fc\u30c9\u30ec\u30fc\u30eb\uff09","text":"<ul> <li>\u30e6\u30fc\u30b9\u30b1\u30fc\u30b9\u624b\u9806</li> <li>\u30e6\u30fc\u30b6\u30fc\u304c\u5165\u529b\u6587\u3092\u4e0e\u3048\u308b</li> <li>AgentPipeline\u304c\u30ac\u30fc\u30c9\u30ec\u30fc\u30eb\u95a2\u6570\u3067\u5165\u529b\u691c\u67fb</li> <li>\u554f\u984c\u306a\u3051\u308c\u3070\u751f\u6210\u3001\u554f\u984c\u3042\u308c\u3070\u30d6\u30ed\u30c3\u30af</li> <li>\u30e6\u30fc\u30b9\u30b1\u30fc\u30b9\u30d5\u30ed\u30fc\u56f3 <pre><code>@startuml\nactor User\nparticipant AgentPipeline\nparticipant Guardrail\nparticipant Agent\nUser -&gt; AgentPipeline: \u5165\u529b\u6587\nAgentPipeline -&gt; Guardrail: \u5165\u529b\u691c\u67fb\nalt \u554f\u984c\u306a\u3057\n  AgentPipeline -&gt; Agent: \u751f\u6210\n  Agent -&gt; AgentPipeline: \u751f\u6210\u7d50\u679c\n  AgentPipeline -&gt; User: \u7d50\u679c\u8fd4\u5374\nelse \u554f\u984c\u3042\u308a\n  AgentPipeline -&gt; User: \u30d6\u30ed\u30c3\u30af\u901a\u77e5\nend\n@enduml\n</code></pre></li> </ul>"},{"location":"function_spec/#_2","title":"\u53c2\u8003","text":"<ul> <li>\u8a73\u7d30\u306a\u30b3\u30fc\u30c9\u4f8b\u306f docs/pipeline_examples.md \u3092\u53c2\u7167\u3002 </li> </ul>"},{"location":"new_flow_features/","title":"\ud83d\ude80 \u65b0\u3057\u3044Flow\u6a5f\u80fd\u5b8c\u5168\u30ac\u30a4\u30c9","text":"<p>\u672c\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u3067\u306f\u3001agents-sdk-models v0.0.8\u4ee5\u964d\u3067\u8ffd\u52a0\u3055\u308c\u305f\u65b0\u3057\u3044Flow\u4f5c\u6210\u6a5f\u80fd\u306b\u3064\u3044\u3066\u8a73\u3057\u304f\u8aac\u660e\u3057\u307e\u3059\u3002</p>"},{"location":"new_flow_features/#_1","title":"\u6982\u8981","text":"<p>\u5f93\u6765\u306eFlow\u306f\u8f9e\u66f8\u5f62\u5f0f\u3067\u306e\u30b9\u30c6\u30c3\u30d7\u5b9a\u7fa9\u304c\u5fc5\u8981\u3067\u3057\u305f\u304c\u3001\u65b0\u3057\u3044Flow\u30b3\u30f3\u30b9\u30c8\u30e9\u30af\u30bf\u306f3\u3064\u306e\u65b9\u6cd5\u3067\u30ef\u30fc\u30af\u30d5\u30ed\u30fc\u3092\u4f5c\u6210\u3067\u304d\u307e\u3059\uff1a</p> <ol> <li>\u5358\u4e00\u30b9\u30c6\u30c3\u30d7 - <code>Flow(steps=single_step)</code></li> <li>\u30b7\u30fc\u30b1\u30f3\u30b7\u30e3\u30eb\u30b9\u30c6\u30c3\u30d7 - <code>Flow(steps=[step1, step2, step3])</code> </li> <li>\u5f93\u6765\u65b9\u5f0f - <code>Flow(start=\"step1\", steps={\"step1\": step1, ...})</code></li> </ol>"},{"location":"new_flow_features/#flow_1","title":"\ud83c\udfaf \u6700\u3082\u30b7\u30f3\u30d7\u30eb\uff1a\u5358\u4e00\u30b9\u30c6\u30c3\u30d7Flow","text":"<pre><code>from agents_sdk_models import create_simple_gen_agent, Flow\n\n# GenAgent\u3092\u4f5c\u6210\ngen_agent = create_simple_gen_agent(\n    name=\"assistant\",\n    instructions=\"\u3042\u306a\u305f\u306f\u89aa\u5207\u306a\u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u3067\u3059\u3002\",\n    model=\"gpt-4o-mini\"\n)\n\n# Flow\u3092\u4f5c\u6210\uff08\u305f\u3063\u305f1\u884c\uff01\uff09\nflow = Flow(steps=gen_agent)\n\n# \u5b9f\u884c\nresult = await flow.run(input_data=\"\u3053\u3093\u306b\u3061\u306f\")\nprint(result.shared_state[\"assistant_result\"])\n</code></pre>"},{"location":"new_flow_features/#flow_2","title":"\ud83d\udd17 \u81ea\u52d5\u63a5\u7d9a\uff1a\u30b7\u30fc\u30b1\u30f3\u30b7\u30e3\u30ebFlow","text":"<pre><code>from agents_sdk_models import create_simple_gen_agent, Flow, DebugStep\n\n# \u8907\u6570\u306e\u30b9\u30c6\u30c3\u30d7\u3092\u5b9a\u7fa9\nidea_gen = create_simple_gen_agent(\"idea\", \"\u30a2\u30a4\u30c7\u30a2\u3092\u751f\u6210\", \"gpt-4o-mini\")\nwriter = create_simple_gen_agent(\"writer\", \"\u8a18\u4e8b\u3092\u57f7\u7b46\", \"gpt-4o\")\nreviewer = create_simple_gen_agent(\"reviewer\", \"\u8a18\u4e8b\u3092\u30ec\u30d3\u30e5\u30fc\", \"claude-3-5-sonnet-latest\")\ndebug = DebugStep(\"debug\", \"\u30ef\u30fc\u30af\u30d5\u30ed\u30fc\u5b8c\u4e86\")\n\n# \u30b7\u30fc\u30b1\u30f3\u30b7\u30e3\u30ebFlow\uff08\u81ea\u52d5\u63a5\u7d9a\uff01\uff09\nflow = Flow(steps=[idea_gen, writer, reviewer, debug])\n\n# \u5b9f\u884c\uff08idea_gen \u2192 writer \u2192 reviewer \u2192 debug \u306e\u9806\u3067\u81ea\u52d5\u5b9f\u884c\uff09\nresult = await flow.run(input_data=\"AI\u6280\u8853\u306b\u3064\u3044\u3066\")\n\n# \u5404\u30b9\u30c6\u30c3\u30d7\u306e\u7d50\u679c\u3092\u78ba\u8a8d\nprint(\"\u30a2\u30a4\u30c7\u30a2:\", result.shared_state[\"idea_result\"])\nprint(\"\u8a18\u4e8b:\", result.shared_state[\"writer_result\"])  \nprint(\"\u30ec\u30d3\u30e5\u30fc:\", result.shared_state[\"reviewer_result\"])\n</code></pre>"},{"location":"new_flow_features/#genagent","title":"\u2699\ufe0f \u9ad8\u5ea6\u306a\u4f8b\uff1a\u8a55\u4fa1\u4ed8\u304dGenAgent","text":"<pre><code>from agents_sdk_models import create_evaluated_gen_agent, Flow\n\n# \u8a55\u4fa1\u6a5f\u80fd\u4ed8\u304dGenAgent\nsmart_agent = create_evaluated_gen_agent(\n    name=\"smart_writer\",\n    generation_instructions=\"\u6280\u8853\u8a18\u4e8b\u3092\u57f7\u7b46\u3057\u3066\u304f\u3060\u3055\u3044\",\n    evaluation_instructions=\"\u8a18\u4e8b\u306e\u8cea\u3092100\u70b9\u6e80\u70b9\u3067\u8a55\u4fa1\u3057\u3001\u6539\u5584\u70b9\u3092\u6307\u6458\u3057\u3066\u304f\u3060\u3055\u3044\",\n    model=\"gpt-4o\",\n    threshold=80,  # 80\u70b9\u672a\u6e80\u306a\u3089\u81ea\u52d5\u30ea\u30c8\u30e9\u30a4\n    retries=2\n)\n\n# \u30b7\u30f3\u30d7\u30eb\u306aFlow\nflow = Flow(steps=smart_agent)\nresult = await flow.run(input_data=\"\u6a5f\u68b0\u5b66\u7fd2\u306e\u57fa\u790e\u306b\u3064\u3044\u3066\")\n\n# \u8a55\u4fa1\u7d50\u679c\u3082\u542b\u3081\u3066\u8868\u793a\nevaluation = result.shared_state.get(\"smart_writer_evaluation\")\nif evaluation:\n    print(f\"\u8a55\u4fa1\u70b9\u6570: {evaluation.get('score', 'N/A')}\")\n    print(f\"\u30b3\u30e1\u30f3\u30c8: {evaluation.get('comment', 'N/A')}\")\n</code></pre>"},{"location":"new_flow_features/#_2","title":"\ud83d\udd27 \u30c4\u30fc\u30eb\u9023\u643a","text":"<pre><code>from agents import function_tool\nfrom agents_sdk_models import create_simple_gen_agent, Flow\n\n@function_tool\ndef get_weather(location: str) -&gt; str:\n    \"\"\"\u6307\u5b9a\u5730\u57df\u306e\u5929\u6c17\u60c5\u5831\u3092\u53d6\u5f97\"\"\"\n    return f\"Weather in {location}: Sunny, 25\u00b0C\"\n\n@function_tool  \ndef get_news(topic: str) -&gt; str:\n    \"\"\"\u6307\u5b9a\u30c8\u30d4\u30c3\u30af\u306e\u30cb\u30e5\u30fc\u30b9\u3092\u53d6\u5f97\"\"\"\n    return f\"Latest news about {topic}: AI breakthrough announced\"\n\n# \u30c4\u30fc\u30eb\u4ed8\u304dGenAgent\nweather_agent = create_simple_gen_agent(\n    name=\"weather_bot\",\n    instructions=\"\u5929\u6c17\u3084\u30cb\u30e5\u30fc\u30b9\u306e\u8cea\u554f\u306b\u7b54\u3048\u3066\u304f\u3060\u3055\u3044\u3002\u5fc5\u8981\u306b\u5fdc\u3058\u3066\u30c4\u30fc\u30eb\u3092\u4f7f\u7528\u3057\u3066\u304f\u3060\u3055\u3044\u3002\",\n    model=\"gpt-4o-mini\",\n    generation_tools=[get_weather, get_news]\n)\n\nflow = Flow(steps=weather_agent)\nresult = await flow.run(input_data=\"\u6771\u4eac\u306e\u5929\u6c17\u3068AI\u306e\u30cb\u30e5\u30fc\u30b9\u3092\u6559\u3048\u3066\")\n</code></pre>"},{"location":"new_flow_features/#_3","title":"\ud83c\udf1f \u30de\u30eb\u30c1\u30a8\u30fc\u30b8\u30a7\u30f3\u30c8\u5354\u8abf","text":"<pre><code>from agents_sdk_models import create_simple_gen_agent, Flow\n\n# \u5c02\u9580\u5206\u91ce\u306e\u7570\u306a\u308b\u30a8\u30fc\u30b8\u30a7\u30f3\u30c8\nresearcher = create_simple_gen_agent(\n    name=\"researcher\", \n    instructions=\"\u6280\u8853\u8abf\u67fb\u3092\u884c\u3044\u3001\u6b63\u78ba\u306a\u60c5\u5831\u3092\u63d0\u4f9b\u3057\u307e\u3059\",\n    model=\"gpt-4o\"\n)\n\ntranslator = create_simple_gen_agent(\n    name=\"translator\",\n    instructions=\"\u6280\u8853\u6587\u66f8\u3092\u5206\u304b\u308a\u3084\u3059\u3044\u65e5\u672c\u8a9e\u306b\u7ffb\u8a33\u3057\u307e\u3059\", \n    model=\"gpt-4o\"\n)\n\nsummarizer = create_simple_gen_agent(\n    name=\"summarizer\",\n    instructions=\"\u9577\u3044\u6587\u7ae0\u3092\u8981\u70b9\u3092\u62bc\u3055\u3048\u3066\u8981\u7d04\u3057\u307e\u3059\",\n    model=\"claude-3-5-sonnet-latest\"\n)\n\n# \u30de\u30eb\u30c1\u30a8\u30fc\u30b8\u30a7\u30f3\u30c8\u5354\u8abfFlow\nflow = Flow(steps=[researcher, translator, summarizer])\nresult = await flow.run(input_data=\"\u91cf\u5b50\u30b3\u30f3\u30d4\u30e5\u30fc\u30c6\u30a3\u30f3\u30b0\u306e\u6700\u65b0\u52d5\u5411\")\n\nprint(\"\u8abf\u67fb\u7d50\u679c:\", result.shared_state[\"researcher_result\"])\nprint(\"\u7ffb\u8a33\u7d50\u679c:\", result.shared_state[\"translator_result\"]) \nprint(\"\u8981\u7d04\u7d50\u679c:\", result.shared_state[\"summarizer_result\"])\n</code></pre>"},{"location":"new_flow_features/#_4","title":"\ud83d\udd00 \u6761\u4ef6\u5206\u5c90\uff08\u5f93\u6765\u65b9\u5f0f\uff09","text":"<p>\u8907\u96d1\u306a\u6761\u4ef6\u5206\u5c90\u304c\u5fc5\u8981\u306a\u5834\u5408\u306f\u5f93\u6765\u306e\u8f9e\u66f8\u65b9\u5f0f\u3092\u4f7f\u7528\uff1a</p> <pre><code>from agents_sdk_models import Flow, ConditionStep, create_simple_gen_agent\n\ndef check_urgency(ctx):\n    user_input = ctx.last_user_input or \"\"\n    return \"\u7dca\u6025\" in user_input or \"\u6025\u304e\" in user_input\n\nurgent_agent = create_simple_gen_agent(\"urgent\", \"\u7dca\u6025\u5bfe\u5fdc\u3057\u307e\u3059\", \"gpt-4o\")\nnormal_agent = create_simple_gen_agent(\"normal\", \"\u901a\u5e38\u5bfe\u5fdc\u3057\u307e\u3059\", \"gpt-4o-mini\")\n\n# \u6761\u4ef6\u5206\u5c90Flow\nflow = Flow(\n    start=\"check\",\n    steps={\n        \"check\": ConditionStep(\"check\", check_urgency, \"urgent\", \"normal\"),\n        \"urgent\": urgent_agent,\n        \"normal\": normal_agent\n    }\n)\n\nresult = await flow.run(input_data=\"\u7dca\u6025\u306b\u30ec\u30dd\u30fc\u30c8\u3092\u4f5c\u6210\u3057\u3066\u304f\u3060\u3055\u3044\")\n</code></pre>"},{"location":"new_flow_features/#_5","title":"\ud83d\udca1 \u30d9\u30b9\u30c8\u30d7\u30e9\u30af\u30c6\u30a3\u30b9","text":""},{"location":"new_flow_features/#1","title":"1. \u30b9\u30c6\u30c3\u30d7\u547d\u540d\u898f\u5247","text":"<pre><code># Good: \u5206\u304b\u308a\u3084\u3059\u3044\u540d\u524d\ngen_agent = create_simple_gen_agent(\"content_writer\", \"\u8a18\u4e8b\u57f7\u7b46\", \"gpt-4o\")\n\n# Bad: \u610f\u5473\u4e0d\u660e\u306a\u540d\u524d  \ngen_agent = create_simple_gen_agent(\"step1\", \"\u8a18\u4e8b\u57f7\u7b46\", \"gpt-4o\")\n</code></pre>"},{"location":"new_flow_features/#2","title":"2. \u30e2\u30c7\u30eb\u306e\u4f7f\u3044\u5206\u3051","text":"<pre><code># \u8907\u96d1\u306a\u30bf\u30b9\u30af: \u9ad8\u6027\u80fd\u30e2\u30c7\u30eb\ncomplex_agent = create_simple_gen_agent(\"analyzer\", \"\u8907\u96d1\u306a\u5206\u6790\", \"gpt-4o\")\n\n# \u30b7\u30f3\u30d7\u30eb\u306a\u30bf\u30b9\u30af: \u8efd\u91cf\u30e2\u30c7\u30eb\nsimple_agent = create_simple_gen_agent(\"formatter\", \"\u30c6\u30ad\u30b9\u30c8\u6574\u5f62\", \"gpt-4o-mini\")\n</code></pre>"},{"location":"new_flow_features/#3","title":"3. \u30a8\u30e9\u30fc\u30cf\u30f3\u30c9\u30ea\u30f3\u30b0","text":"<pre><code>try:\n    result = await flow.run(input_data=\"\u5165\u529b\u30c7\u30fc\u30bf\")\n    if \"error\" in result.shared_state:\n        print(\"\u30a8\u30e9\u30fc\u304c\u767a\u751f\u3057\u307e\u3057\u305f:\", result.shared_state[\"error\"])\nexcept Exception as e:\n    print(\"\u5b9f\u884c\u30a8\u30e9\u30fc:\", str(e))\n</code></pre>"},{"location":"new_flow_features/#_6","title":"\ud83d\udcca \u30d1\u30d5\u30a9\u30fc\u30de\u30f3\u30b9\u6bd4\u8f03","text":"\u65b9\u5f0f \u30b3\u30fc\u30c9\u884c\u6570 \u8a2d\u5b9a\u306e\u8907\u96d1\u3055 \u5b66\u7fd2\u30b3\u30b9\u30c8 \u65e7AgentPipeline 10-15\u884c \u4e2d \u4e2d \u65b0Flow(\u5358\u4e00) 3\u884c \u4f4e \u4f4e \u65b0Flow(\u30b7\u30fc\u30b1\u30f3\u30b7\u30e3\u30eb) 5-8\u884c \u4f4e \u4f4e \u65b0Flow(\u5f93\u6765) 15-20\u884c \u9ad8 \u9ad8"},{"location":"new_flow_features/#_7","title":"\ud83d\ude80 \u79fb\u884c\u30ac\u30a4\u30c9","text":""},{"location":"new_flow_features/#agentpipeline","title":"AgentPipeline\u304b\u3089\u306e\u79fb\u884c","text":"<pre><code># \u65e7: AgentPipeline\npipeline = AgentPipeline(\n    name=\"example\",\n    generation_instructions=\"\u6587\u7ae0\u3092\u751f\u6210\",\n    evaluation_instructions=\"\u8a55\u4fa1\u3057\u307e\u3059\", \n    model=\"gpt-4o-mini\",\n    threshold=70\n)\nresult = pipeline.run(\"\u5165\u529b\")\n\n# \u65b0: GenAgent + Flow\ngen_agent = create_evaluated_gen_agent(\n    name=\"example\",\n    generation_instructions=\"\u6587\u7ae0\u3092\u751f\u6210\",\n    evaluation_instructions=\"\u8a55\u4fa1\u3057\u307e\u3059\",\n    model=\"gpt-4o-mini\", \n    threshold=70\n)\nflow = Flow(steps=gen_agent)\nresult = await flow.run(input_data=\"\u5165\u529b\")\n</code></pre>"},{"location":"new_flow_features/#_8","title":"\ud83d\udd17 \u95a2\u9023\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8","text":"<ul> <li>\u30af\u30a4\u30c3\u30af\u30b9\u30bf\u30fc\u30c8</li> <li>\u5fdc\u7528\u4f8b </li> <li>Flow/Step API \u30ea\u30d5\u30a1\u30ec\u30f3\u30b9</li> <li>API \u30ea\u30d5\u30a1\u30ec\u30f3\u30b9 </li> </ul>"},{"location":"pipeline_examples/","title":"AgentPipeline\u6d3b\u7528\u4e8b\u4f8b\u96c6","text":"<p>\u3053\u306e\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u3067\u306f\u3001<code>AgentPipeline</code>\u30af\u30e9\u30b9\u3092\u6d3b\u7528\u3057\u305f\u5404\u7a2e\u4e8b\u4f8b\u3092\u7d39\u4ecb\u3057\u307e\u3059\u3002</p>"},{"location":"pipeline_examples/#1","title":"1. \u30b7\u30f3\u30d7\u30eb\u306a\u751f\u6210\uff08\u8a55\u4fa1\u306a\u3057\uff09","text":"<ul> <li>\u30d5\u30a1\u30a4\u30eb: <code>examples/pipeline_simple_generation.py</code></li> <li>\u6982\u8981: \u30e6\u30fc\u30b6\u30fc\u5165\u529b\u306b\u57fa\u3065\u304d\u3001\u8a55\u4fa1\u306a\u3057\u3067\u76f4\u63a5\u751f\u6210\u7d50\u679c\u3092\u8fd4\u3059\u6700\u5c0f\u69cb\u6210\u306e\u4f8b\u3002</li> </ul>"},{"location":"pipeline_examples/#_1","title":"\u30b3\u30fc\u30c9\u4f8b","text":"<pre><code>pipeline = AgentPipeline(\n    name=\"simple_generator\",\n    generation_instructions=\"\"\"\n    You are a helpful assistant that generates creative stories.\n    \u3042\u306a\u305f\u306f\u5275\u9020\u7684\u306a\u7269\u8a9e\u3092\u751f\u6210\u3059\u308b\u5f79\u7acb\u3064\u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u3067\u3059\u3002\n    Please generate a short story based on the user's input.\n    \u30e6\u30fc\u30b6\u30fc\u306e\u5165\u529b\u306b\u57fa\u3065\u3044\u3066\u77ed\u3044\u7269\u8a9e\u3092\u751f\u6210\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n    \"\"\",\n    evaluation_instructions=None,\n    model=\"gpt-3.5-turbo\"\n)\nresult = pipeline.run(\"A story about a robot learning to paint\")\n</code></pre>"},{"location":"pipeline_examples/#2","title":"2. \u751f\u6210\u7269\u306e\u8a55\u4fa1\u4ed8\u304d\u751f\u6210","text":"<ul> <li>\u30d5\u30a1\u30a4\u30eb: <code>examples/pipeline_with_evaluation.py</code></li> <li>\u6982\u8981: \u751f\u6210\u7269\u306b\u5bfe\u3057\u3066\u81ea\u52d5\u8a55\u4fa1\u3092\u884c\u3044\u3001\u95be\u5024\u3092\u6e80\u305f\u3057\u305f\u5834\u5408\u306e\u307f\u7d50\u679c\u3092\u8fd4\u3059\u4f8b\u3002</li> </ul>"},{"location":"pipeline_examples/#_2","title":"\u30b3\u30fc\u30c9\u4f8b","text":"<pre><code>pipeline = AgentPipeline(\n    name=\"evaluated_generator\",\n    generation_instructions=\"...\",\n    evaluation_instructions=\"...\",\n    model=\"gpt-3.5-turbo\",\n    threshold=70\n)\nresult = pipeline.run(\"A story about a robot learning to paint\")\n</code></pre>"},{"location":"pipeline_examples/#3","title":"3. \u30c4\u30fc\u30eb\u9023\u643a\u306b\u3088\u308b\u751f\u6210","text":"<ul> <li>\u30d5\u30a1\u30a4\u30eb: <code>examples/pipeline_with_tools.py</code></li> <li>\u6982\u8981: <code>@function_tool</code>\u3067\u5b9a\u7fa9\u3057\u305fPython\u95a2\u6570\u3092\u30c4\u30fc\u30eb\u3068\u3057\u3066\u7d44\u307f\u8fbc\u307f\u3001\u5916\u90e8\u60c5\u5831\u53d6\u5f97\u3084\u8a08\u7b97\u306a\u3069\u3092\u7d44\u307f\u5408\u308f\u305b\u305f\u751f\u6210\u3092\u884c\u3046\u4f8b\u3002</li> </ul>"},{"location":"pipeline_examples/#_3","title":"\u30b3\u30fc\u30c9\u4f8b","text":"<pre><code>from agents import function_tool\n\n@function_tool\ndef search_web(query: str) -&gt; str:\n    ...\n\n@function_tool\ndef get_weather(location: str) -&gt; str:\n    ...\n\npipeline = AgentPipeline(\n    name=\"tooled_generator\",\n    generation_instructions=\"...\",\n    evaluation_instructions=None,\n    model=\"gpt-3.5-turbo\",\n    generation_tools=[search_web, get_weather]\n)\nresult = pipeline.run(\"What's the weather like in Tokyo?\")\n</code></pre>"},{"location":"pipeline_examples/#4","title":"4. \u30ac\u30fc\u30c9\u30ec\u30fc\u30eb\uff08\u5165\u529b\u30ac\u30fc\u30c9\u30ec\u30fc\u30eb\uff09\u306b\u3088\u308b\u5165\u529b\u5236\u5fa1","text":"<ul> <li>\u30d5\u30a1\u30a4\u30eb: <code>examples/pipeline_with_guardrails.py</code></li> <li>\u6982\u8981: \u5165\u529b\u5185\u5bb9\u306b\u5bfe\u3057\u3066\u30ac\u30fc\u30c9\u30ec\u30fc\u30eb\uff08\u4f8b: \u6570\u5b66\u306e\u5bbf\u984c\u4f9d\u983c\u306e\u691c\u51fa\uff09\u3092\u8a2d\u3051\u3001\u4e0d\u9069\u5207\u306a\u30ea\u30af\u30a8\u30b9\u30c8\u3092\u30d6\u30ed\u30c3\u30af\u3059\u308b\u4f8b\u3002</li> </ul>"},{"location":"pipeline_examples/#_4","title":"\u30b3\u30fc\u30c9\u4f8b","text":"<pre><code>from agents import input_guardrail, GuardrailFunctionOutput, InputGuardrailTripwireTriggered\n\n@input_guardrail\nasync def math_guardrail(ctx, agent, input):\n    ...\n\npipeline = AgentPipeline(\n    name=\"guardrail_pipeline\",\n    generation_instructions=\"...\",\n    evaluation_instructions=None,\n    model=\"gpt-4o\",\n    input_guardrails=[math_guardrail]\n)\n\ntry:\n    result = pipeline.run(\"Can you help me solve for x: 2x + 3 = 11?\")\nexcept InputGuardrailTripwireTriggered:\n    print(\"[Guardrail Triggered] Math homework detected. Request blocked.\")\n</code></pre>"},{"location":"pipeline_examples/#5","title":"5. \u30ea\u30c8\u30e9\u30a4\u6642\u306e\u30b3\u30e1\u30f3\u30c8\u30d5\u30a3\u30fc\u30c9\u30d0\u30c3\u30af","text":"<ul> <li>\u6a5f\u80fd: \u524d\u56de\u306e\u8a55\u4fa1\u30b3\u30e1\u30f3\u30c8\u3092\u6307\u5b9a\u3057\u305f\u91cd\u5927\u5ea6\u30ec\u30d9\u30eb\u3067\u751f\u6210\u30d7\u30ed\u30f3\u30d7\u30c8\u306b\u4ed8\u4e0e\u3057\u3001\u6539\u5584\u3092\u4fc3\u3059</li> <li>\u30d1\u30e9\u30e1\u30fc\u30bf:</li> <li><code>retry_comment_importance</code>: <code>serious</code>, <code>normal</code>, <code>minor</code> \u306e\u3044\u305a\u308c\u304b\u3092\u6307\u5b9a\u53ef\u80fd</li> </ul>"},{"location":"pipeline_examples/#_5","title":"\u30b3\u30fc\u30c9\u4f8b","text":"<pre><code>from agents_sdk_models.pipeline import AgentPipeline\n\npipeline = AgentPipeline(\n    name=\"comment_retry\",\n    generation_instructions=\"\u751f\u6210\u30d7\u30ed\u30f3\u30d7\u30c8\",\n    evaluation_instructions=\"\u8a55\u4fa1\u30d7\u30ed\u30f3\u30d7\u30c8\",\n    model=\"gpt-4o-mini\",\n    threshold=80,\n    retries=2,\n    retry_comment_importance=[\"serious\", \"normal\"]\n)\nresult = pipeline.run(\"\u8a55\u4fa1\u5bfe\u8c61\u306e\u30c6\u30ad\u30b9\u30c8\")\nprint(result)\n</code></pre>"},{"location":"pipeline_examples/#_6","title":"\u53c2\u8003","text":"<ul> <li>\u5404\u30b5\u30f3\u30d7\u30eb\u306f <code>examples/</code> \u30d5\u30a9\u30eb\u30c0\u306b\u683c\u7d0d\u3055\u308c\u3066\u3044\u307e\u3059\u3002</li> <li>\u8a73\u7d30\u306a\u4f7f\u3044\u65b9\u3084\u5fdc\u7528\u4f8b\u306fREADME\u3082\u53c2\u7167\u3057\u3066\u304f\u3060\u3055\u3044\u3002 </li> </ul>"},{"location":"pipleine_clerify/","title":"ClearifyPipeline\u306b\u3064\u3044\u3066","text":"<p>ClearifyPipeline\u306f\u8981\u4ef6\u3092\u660e\u78ba\u5316\u3059\u308b\u306e\u306b\u5fc5\u8981\u306aAgentPipeline\u306e\u30b5\u30d6\u30af\u30e9\u30b9\u3067\u3059\u3002\u6b21\u306e\u30b9\u30c6\u30c3\u30d7\u3078\u8981\u4ef6\u304c\u6e80\u305f\u3055\u308c\u3066\u3044\u308b\u304b\u3092\u78ba\u8a8d\u3055\u308c\u308b\u307e\u3067\u3001\u7e70\u308a\u8fd4\u3057\u8cea\u554f\u3092\u884c\u3044\u3001\u30e6\u30fc\u30b6\u30fc\u306b\u78ba\u8a8d\u3092\u884c\u3044\u307e\u3059\u3002</p> <pre><code>class ReportRequirements(BaseModel):\n    event: str  # \u30a4\u30d9\u30f3\u30c8\u540d\n    date: str   #\u3000\u65e5\u4ed8\n    place: str  # \u5834\u6240\n    topics: List[str] # \u30c8\u30d4\u30c3\u30af\n    interested: str # \u5370\u8c61\u306b\u6b8b\u308b\u3053\u3068\n    expression: str # \u611f\u60f3\n\npipeline = ClearifyPipeline(\n    name=\"clearify_report_requrements\",\n    generation_instructions=\"\"\"\n    \u3042\u306a\u305f\u306f\u30ec\u30dd\u30fc\u30c8\u3092\u4f5c\u6210\u3059\u308b\u6e96\u5099\u3092\u884c\u3044\u307e\u3059\u3002\n    \u30ec\u30dd\u30fc\u30c8\u306b\u8a18\u8f09\u3059\u308b\u8981\u4ef6\u3092\u6574\u7406\u3057\u3001\u9b45\u529b\u7684\u306a\u30ec\u30dd\u30fc\u30c8\u3068\u306a\u308b\u3088\u3046\u805e\u304d\u624b\u3068\u3057\u3066\u3001\u30e6\u30fc\u30b6\u30fc\u3068\u5bfe\u8a71\u3057\u8981\u4ef6\u3092\u5f15\u304d\u51fa\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n    \u8981\u4ef6\u304c\u660e\u78ba\u3067\u306a\u304b\u3063\u305f\u308a\u3001\u9b45\u529b\u7684\u51fa\u306a\u3044\u5834\u5408\u306f\u3001\u3055\u3089\u306b\u8cea\u554f\u3092\u304f\u308a\u304b\u3048\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n    \u5fc5\u8981\u306a\u9805\u76ee\u3068\u3001\u305d\u308c\u3092\u9b45\u529b\u7684\u306b\u3059\u308b\u30dd\u30a4\u30f3\u30c8\u3092\u4f1d\u3048\u305f\u308a\u3001\u30b5\u30f3\u30d7\u30eb\u3092\u63d0\u793a\u3057\u3066\u3001\u30e6\u30fc\u30b6\u30fc\u306e\u4f53\u9a13\u304b\u3089\u30ec\u30dd\u30fc\u30c8\u3092\u4f5c\u6210\u3059\u308b\u305f\u3081\u306e\u3001\u51fa\u6765\u308b\u3060\u3051\u8a73\u7d30\u306a\u6750\u6599\u3092\u96c6\u3081\u3066\u304f\u3060\u3055\u3044\u3002\n    \"\"\",\n    output_data = ReportRequirements,\n    clerify_max_turns = 20,\n    evaluation_instructions=None,\n    model=\"gpt-4o\"\n)\nresult = pipeline.run(\"I would like to make a xxxx\")\n</code></pre> <p>ClearifyPipeline\u306f\u30e6\u30fc\u30b6\u30fc\u304b\u3089\u306e\u8981\u671b\u3055\u308c\u305f\u8981\u4ef6\u306b\u52a0\u3048\u3066\u3001pydantic basemodel\u3067\u51fa\u529b\u578b\u304c\u5b9a\u7fa9\u3055\u308c\u3066\u3044\u308b\u5834\u5408\u306f\u3001\u30e6\u30fc\u30b6\u30fc\u306e\u51fa\u529b\u578b\u3092\u30e9\u30c3\u30d7\u3057\u3001\u6b21\u306e\u3088\u3046\u306a\u30af\u30e9\u30b9\u3068\u3057\u3066LLM\u306b\u51fa\u529b\u3055\u305b\u308b\u3088\u3046\u306b\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> <pre><code>class Clearify[T](BaseModel):\n    clearity: bool  # True\u306a\u3089\u8981\u4ef6\u304c\u78ba\u5b9a\n    user_requirement: T # Option True\u6642\u306b\u767a\u751f\n</code></pre> <p>\u30e6\u30fc\u30b6\u30fc\u304b\u3089\u578b\u304c\u6307\u5b9a\u3055\u308c\u3066\u3044\u306a\u3044\u306f\u6587\u5b57\u5217\u3068\u3057\u3066\u8981\u4ef6\u3092\u53d6\u5f97\u3057\u3001\u8981\u4ef6\u3092\u8fd4\u5374\u3059\u308b\u3088\u3046\u306b\u3057\u3066\u304f\u3060\u3055\u3044\u3002</p> <pre><code>class Clearify(BaseModel):\n    clearity: bool  # True\u306a\u3089\u8981\u4ef6\u304c\u78ba\u5b9a\n    user_requirement: str # Option True\u6642\u306b\u767a\u751f\n</code></pre>"},{"location":"requirements/","title":"\u8981\u4ef6\u5b9a\u7fa9\u66f8","text":""},{"location":"requirements/#_2","title":"\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306e\u76ee\u7684\u30fb\u80cc\u666f","text":"<p>OpenAI Agents SDK\u3092\u6d3b\u7528\u3057\u3001\u751f\u6210\u30fb\u8a55\u4fa1\u30fb\u30c4\u30fc\u30eb\u9023\u643a\u30fb\u30ac\u30fc\u30c9\u30ec\u30fc\u30eb\u306a\u3069\u306eAI\u30ef\u30fc\u30af\u30d5\u30ed\u30fc\u3092\u7c21\u5358\u306b\u69cb\u7bc9\u30fb\u62e1\u5f35\u3067\u304d\u308bPython\u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u63d0\u4f9b\u3059\u308b\u3002</p>"},{"location":"requirements/#_3","title":"\u5229\u7528\u8005\u3068\u56f0\u308a\u3054\u3068","text":"\u5229\u7528\u8005\u306e\u7a2e\u985e \u30b4\u30fc\u30eb \u5236\u7d04 \u56f0\u308a\u3054\u3068 AI\u958b\u767a\u8005 LLM\u3092\u6d3b\u7528\u3057\u305f\u591a\u69d8\u306a\u30ef\u30fc\u30af\u30d5\u30ed\u30fc\u3092\u7d20\u65e9\u304f\u69cb\u7bc9 Python, SDK\u4f9d\u5b58 \u30b5\u30f3\u30d7\u30eb\u3084\u8a2d\u8a08\u4f8b\u304c\u5c11\u306a\u3044\u3001\u30ac\u30fc\u30c9\u30ec\u30fc\u30eb\u3084\u30c4\u30fc\u30eb\u9023\u643a\u306e\u5b9f\u88c5\u304c\u7169\u96d1 \u7814\u7a76\u8005 \u8a55\u4fa1\u3084\u5b89\u5168\u6027\u3092\u62c5\u4fdd\u3057\u305fAI\u5b9f\u9a13 \u67d4\u8edf\u306a\u30ab\u30b9\u30bf\u30de\u30a4\u30ba \u8a55\u4fa1\u30fb\u5b89\u5168\u6027\u306e\u4ed5\u7d44\u307f\u304c\u4e0d\u8db3"},{"location":"requirements/#_4","title":"\u63a1\u7528\u3059\u308b\u6280\u8853\u30b9\u30bf\u30c3\u30af","text":"\u6280\u8853 \u30d0\u30fc\u30b8\u30e7\u30f3/\u5099\u8003 Python 3.10\u4ee5\u4e0a OpenAI Agents SDK \u6700\u65b0 pydantic 2.x"},{"location":"requirements/#_5","title":"\u6a5f\u80fd\uff08\u30e6\u30fc\u30b9\u30b1\u30fc\u30b9\uff09\u4e00\u89a7","text":"\u56f0\u308a\u3054\u3068 \u6a5f\u80fd\uff08\u30e6\u30fc\u30b9\u30b1\u30fc\u30b9\uff09 \u6a5f\u80fd\u306e\u30a4\u30e1\u30fc\u30b8 \u30b5\u30f3\u30d7\u30eb\u3084\u8a2d\u8a08\u4f8b\u304c\u5c11\u306a\u3044 AgentPipeline\u306b\u3088\u308b\u751f\u6210\u30fb\u8a55\u4fa1\u30fb\u30c4\u30fc\u30eb\u9023\u643a\u30fb\u30ac\u30fc\u30c9\u30ec\u30fc\u30eb\u306e\u7d71\u5408 \u30b3\u30fc\u30c9\u4f8b\u30fbexamples/pipeline_*.py \u8a55\u4fa1\u30fb\u5b89\u5168\u6027\u306e\u4ed5\u7d44\u307f\u304c\u4e0d\u8db3 \u5165\u529b\u30ac\u30fc\u30c9\u30ec\u30fc\u30eb\u30fb\u81ea\u52d5\u8a55\u4fa1 \u30ac\u30fc\u30c9\u30ec\u30fc\u30eb/\u8a55\u4fa1\u30c6\u30f3\u30d7\u30ec\u30fc\u30c8\u306e\u6d3b\u7528"},{"location":"requirements/#_6","title":"\u53c2\u8003","text":"<ul> <li>\u8a73\u7d30\u306a\u4e8b\u4f8b\u306f docs/pipeline_examples.md \u3092\u53c2\u7167\u3002 </li> </ul>"},{"location":"tracing/","title":"\u30c8\u30ec\u30fc\u30b7\u30f3\u30b0","text":""},{"location":"tracing/#openai-agents-sdk","title":"OpenAI Agents SDK \u306e\u30c8\u30ec\u30fc\u30b7\u30f3\u30b0\u6a5f\u80fd","text":"<p>OpenAI Agents SDK \u306f\u30c7\u30d5\u30a9\u30eb\u30c8\u3067 OpenAI \u306e\u30c8\u30ec\u30fc\u30b7\u30f3\u30b0\u30b5\u30fc\u30d3\u30b9\u3092\u5229\u7528\u3057\u3001\u5b9f\u884c\u4e2d\u306e\u30b9\u30d1\u30f3\u60c5\u5831\u3092\u81ea\u52d5\u7684\u306b\u53ce\u96c6\u30fb\u8a18\u9332\u3057\u307e\u3059\u3002 - \u30d7\u30ed\u30d0\u30a4\u30c0\u30fc\u306b\u95a2\u308f\u3089\u305a\u3001\u5185\u90e8\u3067\u306f OpenAI \u306e Trace API \u3092\u4f7f\u7528\u3057\u307e\u3059\u3002 - <code>OPENAI_API_KEY</code> \u306e\u8a2d\u5b9a\u304c\u5fc5\u8981\u3067\u3059\u3002</p>"},{"location":"tracing/#_2","title":"\u672c\u30e9\u30a4\u30d6\u30e9\u30ea\u3067\u306e\u30a2\u30d7\u30ed\u30fc\u30c1","text":"<ul> <li>\u30c7\u30d5\u30a9\u30eb\u30c8\u3067\u30b3\u30f3\u30bd\u30fc\u30eb\u5411\u3051\u306e <code>ConsoleTracingProcessor</code> \u3092\u63d0\u4f9b\u3057\u307e\u3059\u3002</li> <li>\u6709\u52b9\u5316: <code>enable_console_tracing()</code> \u3092\u547c\u3073\u51fa\u3059\u3068\u3001\u30b3\u30f3\u30bd\u30fc\u30eb\u306b\u30ab\u30e9\u30fc\u4ed8\u304d\u3067\u30b9\u30d1\u30f3\u60c5\u5831\u3092\u8868\u793a\u3057\u307e\u3059\u3002</li> <li>\u30c8\u30ec\u30fc\u30b7\u30f3\u30b0\u3092\u30ab\u30b9\u30bf\u30de\u30a4\u30ba\u3057\u305f\u3044\u5834\u5408\u306f\u3001\u30b5\u30fc\u30c9\u30d1\u30fc\u30c6\u30a3\u30fc\u304c\u63d0\u4f9b\u3059\u308b\u72ec\u81ea\u306e Processor \u3092\u4f7f\u7528\u3057\u3066\u304f\u3060\u3055\u3044\u3002</li> <li>\u30c8\u30ec\u30fc\u30b7\u30f3\u30b0\u3092\u505c\u6b62\u3057\u305f\u3044\u5834\u5408\u306f\u3001<code>disable_tracing()</code> \u3092\u547c\u3073\u51fa\u3057\u3066\u5168\u3066\u306e\u30c8\u30ec\u30fc\u30b7\u30f3\u30b0\u6a5f\u80fd\u3092\u7121\u52b9\u5316\u3057\u307e\u3059\u3002</li> </ul> <p>\u4ee5\u4e0a\u306e\u4ed5\u7d44\u307f\u306b\u3088\u308a\u3001\u30c7\u30d0\u30c3\u30b0\u6642\u3084\u30ed\u30fc\u30ab\u30eb\u958b\u767a\u74b0\u5883\u3067\u3082\u624b\u8efd\u306b\u30c8\u30ec\u30fc\u30b7\u30f3\u30b0\u60c5\u5831\u3092\u78ba\u8a8d\u3067\u304d\u3001\u5fc5\u8981\u306b\u5fdc\u3058\u3066\u30b5\u30fc\u30d3\u30b9\u9023\u643a\u3084\u30ed\u30b0\u51fa\u529b\u5148\u3092\u5207\u308a\u66ff\u3048\u3089\u308c\u307e\u3059\u3002 </p>"},{"location":"workflow/","title":"Workflow \u306b\u3064\u3044\u3066","text":"<ul> <li>Workflow\u3068\u306fAgent/Interaction/Tool\u306e\u9023\u9396\u3092\u884c\u3046\u6a5f\u80fd\u3067\u3059\u3002</li> <li>\u30eb\u30fc\u30c6\u30a3\u30f3\u30b0\u60c5\u5831\u306b\u3082\u3068\u3065\u304d\u3001\u6b21\u306e\u8981\u7d20\u3092\u5b9a\u3081\u307e\u3059\u3002</li> <li></li> <li> <p>\u6b21\u306e\u3088\u3046\u306a\u30b1\u30fc\u30b9\u304c\u3042\u308a\u307e\u3059\u3002 \u3000\u30fb\u5bfe\u8a71\u578b\u30ef\u30fc\u30af\u30d5\u30ed\u30fc \u3000\u30fb\u81ea\u5f8b\u578b\u30ef\u30fc\u30af\u30d5\u30ed\u30fc</p> </li> </ul>"},{"location":"tutorials/advanced/","title":"\u5fdc\u7528\u4f8b","text":"<p>\u3053\u306e\u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb\u3067\u306f\u3001Agents SDK Models \u306e\u5fdc\u7528\u7684\u306a\u4f7f\u3044\u65b9\u3092\u7d39\u4ecb\u3057\u307e\u3059\u3002</p>"},{"location":"tutorials/advanced/#0-flow","title":"0. \u65b0\u3057\u3044Flow\u4f5c\u6210\u65b9\u6cd5\uff08\u8d85\u91cd\u8981\uff01\uff09","text":"<p>\u65b0\u3057\u3044 Flow \u306f3\u3064\u306e\u65b9\u6cd5\u3067\u4f5c\u6210\u3067\u304d\u307e\u3059\uff1a</p> <pre><code>from agents_sdk_models import create_simple_gen_agent, Flow, DebugStep\n\n# 1. \u5358\u4e00\u30b9\u30c6\u30c3\u30d7\uff08\u6700\u3082\u30b7\u30f3\u30d7\u30eb\uff01\uff09\ngen_agent = create_simple_gen_agent(\"writer\", \"\u6587\u7ae0\u3092\u66f8\u3044\u3066\u304f\u3060\u3055\u3044\", \"gpt-4o-mini\")\nflow = Flow(steps=gen_agent)\n\n# 2. \u30b7\u30fc\u30b1\u30f3\u30b7\u30e3\u30eb\u30b9\u30c6\u30c3\u30d7\uff08\u81ea\u52d5\u63a5\u7d9a\uff01\uff09\ndebug_step = DebugStep(\"debug\", \"\u51e6\u7406\u5b8c\u4e86\")\nflow = Flow(steps=[gen_agent, debug_step])  # gen_agent \u2192 debug_step\n\n# 3. \u5f93\u6765\u65b9\u5f0f\uff08\u8907\u96d1\u306a\u30d5\u30ed\u30fc\u7528\uff09\nflow = Flow(\n    start=\"writer\",\n    steps={\n        \"writer\": gen_agent,\n        \"debug\": debug_step\n    }\n)\n\n# \u5b9f\u884c\u306f\u5168\u3066\u540c\u3058\nresult = await flow.run(input_data=\"AI\u306b\u3064\u3044\u3066\u66f8\u3044\u3066\")\nprint(result.shared_state[\"writer_result\"])\n</code></pre>"},{"location":"tutorials/advanced/#1","title":"1. \u30c4\u30fc\u30eb\u9023\u643a\uff08\u65b0\u63a8\u5968\u65b9\u6cd5\uff09","text":"<pre><code>from agents import function_tool\nfrom agents_sdk_models import create_simple_gen_agent, Flow\n\n@function_tool\ndef get_weather(location: str) -&gt; str:\n    return f\"Weather in {location}: Sunny, 25\u00b0C\"\n\n# GenAgent + Flow\u65b9\u5f0f\uff08\u63a8\u5968\uff09\nweather_agent = create_simple_gen_agent(\n    name=\"weather_bot\",\n    instructions=\"\"\"\n    \u3042\u306a\u305f\u306f\u5929\u6c17\u60c5\u5831\u3092\u63d0\u4f9b\u3059\u308b\u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u3067\u3059\u3002\u5fc5\u8981\u306b\u5fdc\u3058\u3066get_weather\u30c4\u30fc\u30eb\u3092\u4f7f\u3063\u3066\u304f\u3060\u3055\u3044\u3002\n    \"\"\",\n    model=\"gpt-4o-mini\",\n    generation_tools=[get_weather]\n)\n\nflow = Flow(steps=weather_agent)  # \u8d85\u30b7\u30f3\u30d7\u30eb\uff01\nresult = await flow.run(input_data=\"\u6771\u4eac\u306e\u5929\u6c17\u306f\uff1f\")\nprint(result.shared_state[\"weather_bot_result\"])\n</code></pre>"},{"location":"tutorials/advanced/#_2","title":"\u65e7\u65b9\u5f0f\uff08\u975e\u63a8\u5968\uff09","text":"<pre><code># AgentPipeline\uff08v0.1.0\u3067\u524a\u9664\u4e88\u5b9a\uff09\nfrom agents_sdk_models import AgentPipeline\npipeline = AgentPipeline(\n    name=\"tool_example\",\n    generation_instructions=\"\u5929\u6c17\u60c5\u5831\u3092\u63d0\u4f9b\u3057\u3066\u304f\u3060\u3055\u3044\u3002\",\n    model=\"gpt-4o-mini\",\n    generation_tools=[get_weather]\n)\nresult = pipeline.run(\"\u6771\u4eac\u306e\u5929\u6c17\u306f\uff1f\")\nprint(result)\n</code></pre>"},{"location":"tutorials/advanced/#2","title":"2. \u30ac\u30fc\u30c9\u30ec\u30fc\u30eb\uff08\u5165\u529b\u5236\u5fa1\uff09","text":"<pre><code>from agents import input_guardrail, GuardrailFunctionOutput, Runner, RunContextWrapper, Agent\nfrom agents_sdk_models import AgentPipeline\nfrom pydantic import BaseModel\n\nclass MathCheck(BaseModel):\n    is_math: bool\n    reason: str\n\nguardrail_agent = Agent(\n    name=\"math_check\",\n    instructions=\"\u6570\u5b66\u306e\u5bbf\u984c\u304b\u5224\u5b9a\u3057\u3066\u304f\u3060\u3055\u3044\u3002\",\n    output_type=MathCheck\n)\n\n@input_guardrail\nasync def math_guardrail(ctx: RunContextWrapper, agent: Agent, input: str):\n    result = await Runner.run(guardrail_agent, input, context=ctx.context)\n    return GuardrailFunctionOutput(\n        output_info=result.final_output,\n        tripwire_triggered=result.final_output.is_math,\n    )\n\npipeline = AgentPipeline(\n    name=\"guardrail_example\",\n    generation_instructions=\"\u8cea\u554f\u306b\u7b54\u3048\u3066\u304f\u3060\u3055\u3044\u3002\",\n    model=\"gpt-4o-mini\",\n    input_guardrails=[math_guardrail]\n)\ntry:\n    result = pipeline.run(\"2x+3=11\u3092\u89e3\u3044\u3066\")\n    print(result)\nexcept Exception:\n    print(\"\u30ac\u30fc\u30c9\u30ec\u30fc\u30eb\u767a\u52d5: \u6570\u5b66\u306e\u5bbf\u984c\u4f9d\u983c\u3092\u691c\u51fa\")\n</code></pre>"},{"location":"tutorials/advanced/#3","title":"3. \u30c0\u30a4\u30ca\u30df\u30c3\u30af\u30d7\u30ed\u30f3\u30d7\u30c8","text":"<pre><code>def dynamic_prompt(user_input: str) -&gt; str:\n    return f\"[DYNAMIC] {user_input.upper()}\"\n\npipeline = AgentPipeline(\n    name=\"dynamic_example\",\n    generation_instructions=\"\u30ea\u30af\u30a8\u30b9\u30c8\u306b\u7b54\u3048\u3066\u304f\u3060\u3055\u3044\u3002\",\n    model=\"gpt-4o-mini\",\n    dynamic_prompt=dynamic_prompt\n)\nresult = pipeline.run(\"\u9762\u767d\u3044\u8a71\u3092\u3057\u3066\")\nprint(result)\n</code></pre>"},{"location":"tutorials/advanced/#4","title":"4. \u30ea\u30c8\u30e9\u30a4\uff06\u81ea\u5df1\u6539\u5584\uff08\u65b0\u63a8\u5968\u65b9\u6cd5\uff09","text":"<pre><code>from agents_sdk_models import create_evaluated_gen_agent, Flow\n\n# \u8a55\u4fa1\u4ed8\u304dGenAgent + Flow\u65b9\u5f0f\uff08\u63a8\u5968\uff09\nsmart_agent = create_evaluated_gen_agent(\n    name=\"smart_writer\",\n    generation_instructions=\"\u6587\u7ae0\u3092\u751f\u6210\u3057\u3066\u304f\u3060\u3055\u3044\u3002\",\n    evaluation_instructions=\"\u5206\u304b\u308a\u3084\u3059\u3055\u3067\u8a55\u4fa1\u3057\u3001\u30b3\u30e1\u30f3\u30c8\u3082\u8fd4\u3057\u3066\u304f\u3060\u3055\u3044\u3002\",\n    model=\"gpt-4o-mini\",\n    threshold=80,\n    retries=2\n)\n\nflow = Flow(steps=smart_agent)  # \u81ea\u52d5\u3067\u30ea\u30c8\u30e9\u30a4\uff06\u81ea\u5df1\u6539\u5584\uff01\nresult = await flow.run(input_data=\"AI\u306e\u6b74\u53f2\u3092\u6559\u3048\u3066\")\nprint(result.shared_state[\"smart_writer_result\"])\n</code></pre>"},{"location":"tutorials/advanced/#_3","title":"\u65e7\u65b9\u5f0f\uff08\u975e\u63a8\u5968\uff09","text":"<pre><code># AgentPipeline\uff08v0.1.0\u3067\u524a\u9664\u4e88\u5b9a\uff09\npipeline = AgentPipeline(\n    name=\"retry_example\",\n    generation_instructions=\"\u6587\u7ae0\u3092\u751f\u6210\u3057\u3066\u304f\u3060\u3055\u3044\u3002\",\n    evaluation_instructions=\"\u5206\u304b\u308a\u3084\u3059\u3055\u3067\u8a55\u4fa1\u3057\u3001\u30b3\u30e1\u30f3\u30c8\u3082\u8fd4\u3057\u3066\u304f\u3060\u3055\u3044\u3002\",\n    model=\"gpt-4o-mini\",\n    threshold=80,\n    retries=2\n)\nresult = pipeline.run(\"AI\u306e\u6b74\u53f2\u3092\u6559\u3048\u3066\")\nprint(result)\n</code></pre>"},{"location":"tutorials/advanced/#5","title":"5. \u8907\u96d1\u306a\u30ef\u30fc\u30af\u30d5\u30ed\u30fc\uff08\u30de\u30eb\u30c1\u30b9\u30c6\u30c3\u30d7\uff09","text":"<pre><code>from agents_sdk_models import (\n    create_simple_gen_agent, create_evaluated_gen_agent, \n    Flow, DebugStep, UserInputStep, ConditionStep\n)\n\n# \u8907\u6570\u306eGenAgent\u3092\u7d44\u307f\u5408\u308f\u305b\nidea_generator = create_simple_gen_agent(\n    name=\"idea_gen\", \n    instructions=\"\u5275\u9020\u7684\u306a\u30a2\u30a4\u30c7\u30a2\u3092\u751f\u6210\u3057\u3066\u304f\u3060\u3055\u3044\", \n    model=\"gpt-4o-mini\"\n)\n\ncontent_writer = create_evaluated_gen_agent(\n    name=\"writer\",\n    generation_instructions=\"\u63d0\u4f9b\u3055\u308c\u305f\u30a2\u30a4\u30c7\u30a2\u3092\u57fa\u306b\u8a73\u7d30\u306a\u8a18\u4e8b\u3092\u66f8\u3044\u3066\u304f\u3060\u3055\u3044\",\n    evaluation_instructions=\"\u8a18\u4e8b\u306e\u8cea\u3092\u8a55\u4fa1\u3057\u3066\u304f\u3060\u3055\u3044\",\n    model=\"gpt-4o\",\n    threshold=75\n)\n\nreviewer = create_simple_gen_agent(\n    name=\"reviewer\",\n    instructions=\"\u8a18\u4e8b\u3092\u30ec\u30d3\u30e5\u30fc\u3057\u3066\u6539\u5584\u63d0\u6848\u3092\u3057\u3066\u304f\u3060\u3055\u3044\",\n    model=\"claude-3-5-sonnet-latest\"\n)\n\n# \u30b7\u30fc\u30b1\u30f3\u30b7\u30e3\u30eb\u30ef\u30fc\u30af\u30d5\u30ed\u30fc\uff08\u81ea\u52d5\u63a5\u7d9a\uff01\uff09\nflow = Flow(steps=[\n    idea_generator,      # \u30a2\u30a4\u30c7\u30a2\u751f\u6210\n    content_writer,      # \u8a18\u4e8b\u57f7\u7b46\uff08\u8a55\u4fa1\u4ed8\u304d\uff09\n    reviewer,           # \u30ec\u30d3\u30e5\u30fc\n    DebugStep(\"done\", \"\u30ef\u30fc\u30af\u30d5\u30ed\u30fc\u5b8c\u4e86\")\n])\n\nresult = await flow.run(input_data=\"AI\u6280\u8853\u306b\u3064\u3044\u3066\")\nprint(\"\u30a2\u30a4\u30c7\u30a2:\", result.shared_state[\"idea_gen_result\"])\nprint(\"\u8a18\u4e8b:\", result.shared_state[\"writer_result\"])\nprint(\"\u30ec\u30d3\u30e5\u30fc:\", result.shared_state[\"reviewer_result\"])\n</code></pre>"},{"location":"tutorials/advanced/#6","title":"6. \u6761\u4ef6\u5206\u5c90\u4ed8\u304d\u30ef\u30fc\u30af\u30d5\u30ed\u30fc","text":"<pre><code># \u6761\u4ef6\u306b\u5fdc\u3058\u3066\u7570\u306a\u308b\u51e6\u7406\u30d1\u30b9\u3092\u5b9f\u884c\ndef check_content_type(ctx):\n    user_input = ctx.last_user_input or \"\"\n    return \"\u6280\u8853\" in user_input\n\ntech_writer = create_simple_gen_agent(\"tech\", \"\u6280\u8853\u8a18\u4e8b\u3092\u66f8\u304f\", \"gpt-4o\")\ngeneral_writer = create_simple_gen_agent(\"general\", \"\u4e00\u822c\u8a18\u4e8b\u3092\u66f8\u304f\", \"gpt-4o-mini\")\n\n# \u5f93\u6765\u65b9\u5f0f\uff08\u8907\u96d1\u306a\u30d5\u30ed\u30fc\u7528\uff09\ncomplex_flow = Flow(\n    start=\"check_type\",\n    steps={\n        \"check_type\": ConditionStep(\n            \"check_type\", \n            check_content_type, \n            \"tech_writer\", \n            \"general_writer\"\n        ),\n        \"tech_writer\": tech_writer,\n        \"general_writer\": general_writer,\n        \"review\": create_simple_gen_agent(\"reviewer\", \"\u8a18\u4e8b\u3092\u30ec\u30d3\u30e5\u30fc\", \"claude-3-5-sonnet-latest\")\n    }\n)\n\nresult = await complex_flow.run(input_data=\"\u6280\u8853\u7684\u306a\u5185\u5bb9\u306b\u3064\u3044\u3066\u66f8\u3044\u3066\")\n</code></pre>"},{"location":"tutorials/advanced/#_4","title":"\u30dd\u30a4\u30f3\u30c8","text":"<ul> <li>\u65b0\u6a5f\u80fd\uff1a <code>Flow(steps=[step1, step2])</code> \u3067\u81ea\u52d5\u30b7\u30fc\u30b1\u30f3\u30b7\u30e3\u30eb\u63a5\u7d9a</li> <li>\u65b0\u6a5f\u80fd\uff1a <code>Flow(steps=single_step)</code> \u3067\u5358\u4e00\u30b9\u30c6\u30c3\u30d7\u3082\u8d85\u30b7\u30f3\u30d7\u30eb</li> <li>\u30c4\u30fc\u30eb\u3084\u30ac\u30fc\u30c9\u30ec\u30fc\u30eb\u3001\u52d5\u7684\u30d7\u30ed\u30f3\u30d7\u30c8\u3001\u81ea\u5df1\u6539\u5584\u3092\u67d4\u8edf\u306b\u7d44\u307f\u5408\u308f\u305b\u53ef\u80fd</li> <li>\u65e7 <code>AgentPipeline</code> \u304b\u3089 <code>GenAgent + Flow</code> \u3078\u306e\u79fb\u884c\u306f\u7c21\u5358</li> <li>\u8907\u96d1\u306a\u30ef\u30fc\u30af\u30d5\u30ed\u30fc\u3082\u6570\u884c\u3067\u69cb\u7bc9\u53ef\u80fd</li> </ul>"},{"location":"tutorials/quickstart/","title":"\u30af\u30a4\u30c3\u30af\u30b9\u30bf\u30fc\u30c8","text":"<p>\u3053\u306e\u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb\u3067\u306f\u3001Agents SDK Models \u3092\u4f7f\u3063\u305f\u6700\u5c0f\u9650\u306eLLM\u6d3b\u7528\u4f8b\u3092\u7d39\u4ecb\u3057\u307e\u3059\u3002</p>"},{"location":"tutorials/quickstart/#1","title":"1. \u30e2\u30c7\u30eb\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u306e\u53d6\u5f97","text":"<pre><code>from agents_sdk_models import get_llm\nllm = get_llm(\"gpt-4o-mini\")\n</code></pre>"},{"location":"tutorials/quickstart/#2-agent","title":"2. Agent \u3067\u30b7\u30f3\u30d7\u30eb\u306a\u5bfe\u8a71","text":"<pre><code>from agents import Agent, Runner\nagent = Agent(\n    name=\"Assistant\",\n    model=llm,\n    instructions=\"\u3042\u306a\u305f\u306f\u89aa\u5207\u306a\u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u3067\u3059\u3002\"\n)\nresult = Runner.run_sync(agent, \"\u3053\u3093\u306b\u3061\u306f\uff01\")\nprint(result.final_output)\n</code></pre>"},{"location":"tutorials/quickstart/#3-genagent-flow","title":"3. GenAgent + Flow \u3067\u751f\u6210\uff0b\u8a55\u4fa1\uff08\u63a8\u5968\uff09","text":"<pre><code>from agents_sdk_models import create_simple_gen_agent, Flow\n\n# GenAgent\u3092\u4f5c\u6210\ngen_agent = create_simple_gen_agent(\n    name=\"ai_expert\",\n    instructions=\"\"\"\n    \u3042\u306a\u305f\u306f\u5f79\u7acb\u3064\u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u3067\u3059\u3002\u30e6\u30fc\u30b6\u30fc\u306e\u8981\u671b\u306b\u5fdc\u3058\u3066\u6587\u7ae0\u3092\u751f\u6210\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n    \"\"\",\n    evaluation_instructions=\"\"\"\n    \u751f\u6210\u3055\u308c\u305f\u6587\u7ae0\u3092\u5206\u304b\u308a\u3084\u3059\u3055\u3067100\u70b9\u6e80\u70b9\u8a55\u4fa1\u3057\u3001\u30b3\u30e1\u30f3\u30c8\u3082\u4ed8\u3051\u3066\u304f\u3060\u3055\u3044\u3002\n    \"\"\",\n    model=\"gpt-4o-mini\",\n    threshold=70\n)\n\n# Flow\u3092\u4f5c\u6210\uff08\u8d85\u30b7\u30f3\u30d7\u30eb\uff01\uff09\nflow = Flow(steps=gen_agent)\nresult = await flow.run(input_data=\"AI\u306e\u6d3b\u7528\u4e8b\u4f8b\u3092\u6559\u3048\u3066\")\nprint(result.shared_state[\"ai_expert_result\"])\n</code></pre>"},{"location":"tutorials/quickstart/#4-agentpipeline","title":"4. \u65e7AgentPipeline\uff08\u975e\u63a8\u5968\uff09","text":"<pre><code>from agents_sdk_models import AgentPipeline\n# \u6ce8\u610f\uff1aAgentPipeline\u306fv0.1.0\u3067\u524a\u9664\u4e88\u5b9a\u3067\u3059\npipeline = AgentPipeline(\n    name=\"eval_example\",\n    generation_instructions=\"\u3042\u306a\u305f\u306f\u5f79\u7acb\u3064\u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u3067\u3059\u3002\",\n    evaluation_instructions=\"\u751f\u6210\u3055\u308c\u305f\u6587\u7ae0\u3092\u5206\u304b\u308a\u3084\u3059\u3055\u3067100\u70b9\u6e80\u70b9\u8a55\u4fa1\u3057\u3066\u304f\u3060\u3055\u3044\u3002\",\n    model=\"gpt-4o-mini\",\n    threshold=70\n)\nresult = pipeline.run(\"AI\u306e\u6d3b\u7528\u4e8b\u4f8b\u3092\u6559\u3048\u3066\")\nprint(result)\n</code></pre>"},{"location":"tutorials/quickstart/#_2","title":"\u30dd\u30a4\u30f3\u30c8","text":"<ul> <li><code>get_llm</code> \u3067\u4e3b\u8981\u306aLLM\u3092\u7c21\u5358\u53d6\u5f97</li> <li><code>Agent</code> \u3067\u30b7\u30f3\u30d7\u30eb\u306a\u5bfe\u8a71</li> <li>\u65b0\u63a8\u5968\uff1a <code>GenAgent + Flow</code> \u3067\u751f\u6210\u30fb\u8a55\u4fa1\u30fb\u81ea\u5df1\u6539\u5584\u307e\u3067\u4e00\u6c17\u901a\u8cab</li> <li><code>Flow(steps=gen_agent)</code> \u3060\u3051\u3067\u8907\u96d1\u306a\u30ef\u30fc\u30af\u30d5\u30ed\u30fc\u3082\u8d85\u30b7\u30f3\u30d7\u30eb\u306b\u5b9f\u73fe</li> <li>\u65e7 <code>AgentPipeline</code> \u306f v0.1.0 \u3067\u524a\u9664\u4e88\u5b9a\uff08\u79fb\u884c\u306f\u7c21\u5358\u3067\u3059\uff09</li> </ul>"}]}